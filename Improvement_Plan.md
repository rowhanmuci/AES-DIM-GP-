# ASE FOCoS ç†±é˜»é æ¸¬ - æ”¹é€²è¨ˆåŠƒ

æ ¹æ“šæ•™æˆå›é¥‹å’Œå¯¦é©—çµæœåˆ¶å®šçš„å®Œæ•´æ”¹é€²æ–¹æ¡ˆ

---

## ğŸ“‹ æ•™æˆçš„æ ¸å¿ƒéœ€æ±‚

### 1. **é—œéµæŒ‡æ¨™**: ç›¸å°èª¤å·® (MAPE)
- âœ… **ç›®æ¨™**: æ‰€æœ‰é æ¸¬èª¤å·® < 20%
- âš ï¸ **ç¾æ³**: 17ç­†è³‡æ–™èª¤å·® > 20%
- ğŸ¯ **é‡è¦æ€§**: å°å…¬å¸è€Œè¨€ï¼ŒTheta_JC 0.01 vs 0.02 ç‰©ç†æ€§è³ªå·®å¾ˆå¤§

### 2. **è¶…åƒæ•¸æœå°‹**
- ğŸ”§ å…¬å¸å»ºè­°çµåˆè¶…åƒæ•¸æœå°‹å·¥å…·
- ğŸ“Š å…¬å¸ç¶“é©—: æ¨¡å‹è¶…åƒæ•¸å½±éŸ¿çµæœé —å¤§
- ğŸ¯ éœ€è¦ç³»çµ±åŒ–çš„è¶…åƒæ•¸å„ªåŒ–

### 3. **TIM_TYPEç‰¹å¾µè™•ç†**
- âš ï¸ ç›®å‰åªç”¨One-hot encoding
- ğŸ’¡ æ‡‰è©²æœ‰æ›´å¥½çš„é¡åˆ¥ç‰¹å¾µè™•ç†æ–¹å¼
- ğŸ” éœ€è¦æ¢ç´¢æ›´å…ˆé€²çš„embeddingæ–¹æ³•

### 4. **è³‡æ–™å“è³ª**
- âš ï¸ Testè³‡æ–™ä¸­æœ‰è »å¤šé‡è¤‡
- ğŸ§¹ éœ€è¦è³‡æ–™æ¸…ç†

---

## ğŸ¯ æ”¹é€²ç­–ç•¥ç¸½è¦½

### å„ªå…ˆé †åº (Priority)

| å„ªå…ˆç´š | ä»»å‹™ | é æœŸæ•ˆæœ | å·¥ä½œé‡ |
|--------|------|----------|--------|
| ğŸ”´ **P0** | è¶…åƒæ•¸æœå°‹ | å¤§å¹…æå‡æº–ç¢ºåº¦ | ä¸­ |
| ğŸ”´ **P0** | ç•°å¸¸é»åˆ†æ | å®šä½å•é¡Œæ ¹æº | å° |
| ğŸŸ¡ **P1** | TIM_TYPEç‰¹å¾µå·¥ç¨‹ | æå‡ç‰¹å¾µè¡¨é” | ä¸­ |
| ğŸŸ¡ **P1** | è³‡æ–™æ¸…ç† | é¿å…éæ“¬åˆ | å° |
| ğŸŸ¢ **P2** | Ensembleå„ªåŒ– | é€²ä¸€æ­¥æå‡ | å¤§ |
| ğŸŸ¢ **P2** | æå¤±å‡½æ•¸èª¿æ•´ | é‡å°ç•°å¸¸é» | ä¸­ |

---

## ğŸ“Š æ–¹æ¡ˆ1: è¶…åƒæ•¸æœå°‹ (P0)

### ç›®æ¨™
ç³»çµ±åŒ–æœå°‹æœ€ä½³è¶…åƒæ•¸çµ„åˆï¼Œé™ä½æœ€å¤§èª¤å·®åˆ°20%ä»¥ä¸‹

### éœ€è¦æœå°‹çš„è¶…åƒæ•¸

#### DKLæ¶æ§‹åƒæ•¸
```python
hyperparameters = {
    # ç‰¹å¾µæå–å™¨
    'hidden_dims': [
        [64, 32, 16],      # æ·ºå±¤
        [128, 64, 32],     # ä¸­å±¤
        [256, 128, 64],    # æ·±å±¤ï¼ˆçµ„å“¡ç”¨é€™å€‹ï¼‰
        [128, 64, 32, 16], # æ›´æ·±
    ],
    'feature_dim': [4, 6, 8, 12],  # æ½›åœ¨ç©ºé–“ç¶­åº¦
    'dropout_rate': [0.0, 0.1, 0.2],
    
    # GP kernel
    'kernel_type': [
        'RBF',                    # å–®ä¸€kernel
        'RBF+Linear',            # çµ„åˆ
        'RBF+Matern',           # çµ„åˆ
        'Complex',              # çµ„å“¡çš„è¤‡é›œçµ„åˆ
    ],
    
    # è¨“ç·´åƒæ•¸
    'learning_rate': [0.001, 0.005, 0.01, 0.02],
    'weight_decay': [1e-5, 1e-4, 1e-3],
    'batch_norm': [True, False],
    
    # æ­£å‰‡åŒ–
    'noise_constraint': [1e-4, 1e-3, 1e-2],
}
```

### æœå°‹æ–¹æ³•

#### æ–¹æ³•1: Optuna (æ¨è–¦)
```python
import optuna

def objective(trial):
    # å®šç¾©è¶…åƒæ•¸ç©ºé–“
    hidden_dims = trial.suggest_categorical('hidden_dims', 
        [[64,32,16], [128,64,32], [256,128,64]])
    feature_dim = trial.suggest_int('feature_dim', 4, 12)
    lr = trial.suggest_float('lr', 1e-3, 1e-1, log=True)
    
    # è¨“ç·´æ¨¡å‹
    model = train_dkl(hidden_dims, feature_dim, lr, ...)
    
    # è©•ä¼°æŒ‡æ¨™: æœ€å¤§ç›¸å°èª¤å·®
    max_relative_error = evaluate_max_error(model, test_data)
    
    return max_relative_error

# åŸ·è¡Œæœå°‹
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=100)
```

**å„ªé»**:
- âœ… è‡ªå‹•åŒ–æœå°‹
- âœ… æ”¯æ´early stopping
- âœ… è¦–è¦ºåŒ–çµæœ
- âœ… å¯ä»¥resumeä¸­æ–·çš„æœå°‹

#### æ–¹æ³•2: Ray Tune
```python
from ray import tune
from ray.tune.schedulers import ASHAScheduler

config = {
    'hidden_dims': tune.choice([[64,32,16], [128,64,32], [256,128,64]]),
    'feature_dim': tune.randint(4, 12),
    'lr': tune.loguniform(1e-3, 1e-1),
}

scheduler = ASHAScheduler(metric='max_error', mode='min')
analysis = tune.run(train_dkl, config=config, num_samples=100)
```

**å„ªé»**:
- âœ… ä¸¦è¡Œæœå°‹
- âœ… æ—©åœæ©Ÿåˆ¶
- âœ… è³‡æºåˆ†é…å„ªåŒ–

### è©•ä¼°æŒ‡æ¨™è¨­è¨ˆ

**é‡é»**: ä¸åªçœ‹MAPEï¼Œè¦çœ‹æœ€å¤§èª¤å·®ï¼

```python
def evaluate_comprehensive(model, X_test, y_test):
    y_pred = model.predict(X_test)
    
    # ç›¸å°èª¤å·®
    relative_errors = np.abs((y_test - y_pred) / y_test) * 100
    
    metrics = {
        'mape': np.mean(relative_errors),
        'max_error': np.max(relative_errors),           # æœ€é‡è¦ï¼
        'outlier_20': np.sum(relative_errors > 20),     # >20%çš„æ¨£æœ¬æ•¸
        'outlier_15': np.sum(relative_errors > 15),
        'outlier_10': np.sum(relative_errors > 10),
        'p95': np.percentile(relative_errors, 95),      # 95åˆ†ä½æ•¸
        'p99': np.percentile(relative_errors, 99),
    }
    
    return metrics
```

**å„ªåŒ–ç›®æ¨™**:
```python
# è¤‡åˆç›®æ¨™å‡½æ•¸
def combined_objective(metrics):
    # ä¸»è¦ç›®æ¨™: é™ä½æœ€å¤§èª¤å·®
    primary = metrics['max_error']
    
    # æ¬¡è¦ç›®æ¨™: é™ä½ç•°å¸¸é»æ•¸é‡
    secondary = metrics['outlier_20'] * 5  # æ‡²ç½°ç•°å¸¸é»
    
    # ç¬¬ä¸‰ç›®æ¨™: æ•´é«”MAPE
    tertiary = metrics['mape']
    
    return primary + secondary + tertiary * 0.5
```

---

## ğŸ” æ–¹æ¡ˆ2: ç•°å¸¸é»æ·±åº¦åˆ†æ (P0)

### ç›®æ¨™
æ‰¾å‡ºé‚£17ç­†èª¤å·®>20%çš„æ¨£æœ¬ï¼Œåˆ†æå…±åŒç‰¹å¾µ

### åˆ†ææ­¥é©Ÿ

#### Step 1: å®šä½ç•°å¸¸é»
```python
def analyze_outliers(model, X_test, y_test, test_df, threshold=20):
    y_pred = model.predict(X_test)
    relative_errors = np.abs((y_test - y_pred) / y_test) * 100
    
    outlier_mask = relative_errors > threshold
    outlier_df = test_df[outlier_mask].copy()
    outlier_df['Pred'] = y_pred[outlier_mask]
    outlier_df['True'] = y_test[outlier_mask]
    outlier_df['Error%'] = relative_errors[outlier_mask]
    
    return outlier_df
```

#### Step 2: ç‰¹å¾µåˆ†å¸ƒåˆ†æ
```python
def outlier_feature_analysis(outlier_df, normal_df):
    """æ¯”è¼ƒç•°å¸¸é»å’Œæ­£å¸¸é»çš„ç‰¹å¾µåˆ†å¸ƒ"""
    
    features = ['TIM_TYPE', 'TIM_THICKNESS', 'TIM_COVERAGE']
    
    analysis = {}
    for feat in features:
        analysis[feat] = {
            'outlier_mean': outlier_df[feat].mean(),
            'normal_mean': normal_df[feat].mean(),
            'outlier_std': outlier_df[feat].std(),
            'normal_std': normal_df[feat].std(),
            'outlier_range': (outlier_df[feat].min(), outlier_df[feat].max()),
            'normal_range': (normal_df[feat].min(), normal_df[feat].max()),
        }
    
    return analysis
```

#### Step 3: ç•°å¸¸é»æ¨¡å¼è­˜åˆ¥
```python
# æª¢æŸ¥ç•°å¸¸é»æ˜¯å¦é›†ä¸­åœ¨æŸäº›å€åŸŸ
def check_outlier_patterns(outlier_df):
    patterns = {
        'TIM_TYPEåˆ†å¸ƒ': outlier_df['TIM_TYPE'].value_counts(),
        'THICKNESSç¯„åœ': {
            'low (<0.1)': len(outlier_df[outlier_df['TIM_THICKNESS'] < 0.1]),
            'mid (0.1-0.2)': len(outlier_df[(outlier_df['TIM_THICKNESS'] >= 0.1) & 
                                            (outlier_df['TIM_THICKNESS'] < 0.2)]),
            'high (>0.2)': len(outlier_df[outlier_df['TIM_THICKNESS'] >= 0.2]),
        },
        'COVERAGEç¯„åœ': {
            'low (<30%)': len(outlier_df[outlier_df['TIM_COVERAGE'] < 30]),
            'mid (30-70%)': len(outlier_df[(outlier_df['TIM_COVERAGE'] >= 30) & 
                                           (outlier_df['TIM_COVERAGE'] < 70)]),
            'high (>70%)': len(outlier_df[outlier_df['TIM_COVERAGE'] >= 70]),
        }
    }
    return patterns
```

### å¯èƒ½çš„ç•°å¸¸é»åŸå› 

1. **é‚Šç•Œæ¨£æœ¬**: 
   - æ¥µç«¯çš„THICKNESSæˆ–COVERAGEå€¼
   - è¨“ç·´é›†ä¸­å¾ˆå°‘è¦‹çš„çµ„åˆ

2. **ç‰¹å®šTIM_TYPE**:
   - æŸäº›é¡å‹çš„æ¨£æœ¬æ•¸å¤ªå°‘
   - ç‰©ç†ç‰¹æ€§å·®ç•°å¤§

3. **è³‡æ–™å“è³ª**:
   - é‡æ¸¬èª¤å·®
   - é‡è¤‡è³‡æ–™
   - æ¨™ç±¤éŒ¯èª¤

### é‡å°æ€§æ”¹é€²

```python
# ç­–ç•¥1: ç•°å¸¸é»åŠ æ¬Š
def weighted_loss(y_pred, y_true, sample_weights):
    """å°ç•°å¸¸æ¨£æœ¬å€åŸŸåŠ å¤§æ¬Šé‡"""
    mse = (y_pred - y_true) ** 2
    weighted_mse = mse * sample_weights
    return weighted_mse.mean()

# ç­–ç•¥2: ç•°å¸¸é»å¢å¼·
def augment_outlier_regions(X_train, y_train, outlier_indices):
    """å°ç•°å¸¸å€åŸŸçš„æ¨£æœ¬åšè³‡æ–™å¢å¼·"""
    # åœ¨ç•°å¸¸é»é™„è¿‘æ·»åŠ å™ªè²æ¨£æœ¬
    augmented_X = []
    augmented_y = []
    
    for idx in outlier_indices:
        for _ in range(5):  # æ¯å€‹ç•°å¸¸é»ç”Ÿæˆ5å€‹è®Šç¨®
            noise = np.random.normal(0, 0.01, X_train[idx].shape)
            augmented_X.append(X_train[idx] + noise)
            augmented_y.append(y_train[idx])
    
    return np.vstack([X_train, augmented_X]), np.hstack([y_train, augmented_y])
```

---

## ğŸ§¬ æ–¹æ¡ˆ3: TIM_TYPEç‰¹å¾µå·¥ç¨‹ (P1)

### ç›®å‰å•é¡Œ
- âŒ One-hot encoding: å‡è¨­é¡åˆ¥é–“å®Œå…¨ç¨ç«‹
- âŒ ç„¡æ³•æ•æ‰TIM_TYPEçš„ç‰©ç†ç›¸ä¼¼æ€§
- âŒ é«˜ç¶­ç¨€ç–è¡¨ç¤º

### æ”¹é€²æ–¹æ¡ˆ

#### æ–¹æ¡ˆ3.1: Entity Embedding (æ¨è–¦)
```python
class TIMTypeEmbedding(nn.Module):
    def __init__(self, n_types, embedding_dim=4):
        super().__init__()
        self.embedding = nn.Embedding(n_types, embedding_dim)
        
    def forward(self, tim_type_indices):
        # tim_type_indices: [batch_size]
        # output: [batch_size, embedding_dim]
        return self.embedding(tim_type_indices)

# ä½¿ç”¨åœ¨DKLä¸­
class DKLWithEmbedding(nn.Module):
    def __init__(self, n_types, continuous_dim, embedding_dim=4):
        super().__init__()
        self.type_embedding = TIMTypeEmbedding(n_types, embedding_dim)
        
        # DNNæ¥å—embedding + é€£çºŒç‰¹å¾µ
        self.dnn = DnnFeatureExtractor(
            input_dim=embedding_dim + continuous_dim,
            output_dim=6
        )
```

**å„ªé»**:
- âœ… è‡ªå‹•å­¸ç¿’TIM_TYPEçš„æ½›åœ¨è¡¨ç¤º
- âœ… èƒ½æ•æ‰é¡å‹é–“çš„ç›¸ä¼¼æ€§
- âœ… é™ç¶­ï¼ˆ4-8ç¶­ vs One-hotçš„Nç¶­ï¼‰

#### æ–¹æ¡ˆ3.2: Target Encoding
```python
def target_encode_tim_type(train_df, test_df, target_col='Theta.JC'):
    """ç”¨ç›®æ¨™è®Šé‡çš„å¹³å‡å€¼ä¾†ç·¨ç¢¼é¡åˆ¥"""
    
    # è¨ˆç®—æ¯å€‹TIM_TYPEçš„å¹³å‡Theta.JC
    type_means = train_df.groupby('TIM_TYPE')[target_col].mean()
    
    # åŠ å…¥å…¨å±€å¹³å‡ä½œç‚ºå¹³æ»‘
    global_mean = train_df[target_col].mean()
    smoothing = 10  # å¹³æ»‘åƒæ•¸
    
    type_counts = train_df.groupby('TIM_TYPE').size()
    
    # å¹³æ»‘å¾Œçš„ç·¨ç¢¼
    smooth_means = (type_means * type_counts + global_mean * smoothing) / (type_counts + smoothing)
    
    # æ‡‰ç”¨åˆ°è¨“ç·´å’Œæ¸¬è©¦é›†
    train_df['TIM_TYPE_encoded'] = train_df['TIM_TYPE'].map(smooth_means)
    test_df['TIM_TYPE_encoded'] = test_df['TIM_TYPE'].map(smooth_means).fillna(global_mean)
    
    return train_df, test_df
```

**å„ªé»**:
- âœ… ç›´æ¥åæ˜ TIM_TYPEå°ç›®æ¨™çš„å½±éŸ¿
- âœ… å–®ä¸€ç¶­åº¦ï¼Œç°¡å–®é«˜æ•ˆ
- âš ï¸ æ³¨æ„: éœ€è¦é¿å…target leakage (ç”¨CV)

#### æ–¹æ¡ˆ3.3: ç‰©ç†å±¬æ€§Encoding
```python
def physics_based_encoding(tim_type):
    """æ ¹æ“šTIMææ–™çš„ç‰©ç†å±¬æ€§ç·¨ç¢¼"""
    
    # å‡è¨­æˆ‘å€‘çŸ¥é“æ¯ç¨®TIMçš„ç‰©ç†å±¬æ€§
    physics_properties = {
        1: {'thermal_conductivity': 5.0, 'density': 2.5, 'viscosity': 100},
        2: {'thermal_conductivity': 8.0, 'density': 3.0, 'viscosity': 150},
        3: {'thermal_conductivity': 3.5, 'density': 2.0, 'viscosity': 80},
        # ... å…¶ä»–é¡å‹
    }
    
    # ç”¨ç‰©ç†å±¬æ€§ä½œç‚ºç‰¹å¾µ
    if tim_type in physics_properties:
        return np.array([
            physics_properties[tim_type]['thermal_conductivity'],
            physics_properties[tim_type]['density'],
            physics_properties[tim_type]['viscosity']
        ])
    else:
        return np.zeros(3)  # æœªçŸ¥é¡å‹ç”¨0å¡«å……
```

**å„ªé»**:
- âœ… èå…¥é ˜åŸŸçŸ¥è­˜
- âœ… ç‰©ç†æ„ç¾©æ˜ç¢º
- âš ï¸ éœ€è¦: å–å¾—TIMææ–™çš„å¯¦éš›ç‰©ç†åƒæ•¸

---

## ğŸ§¹ æ–¹æ¡ˆ4: è³‡æ–™æ¸…ç† (P1)

### å•é¡Œ
æ•™æˆæåˆ°testè³‡æ–™ä¸­æœ‰è »å¤šé‡è¤‡

### æ¸…ç†æ­¥é©Ÿ

#### Step 1: æª¢æ¸¬é‡è¤‡
```python
def check_duplicates(df, features=['TIM_TYPE', 'TIM_THICKNESS', 'TIM_COVERAGE']):
    """æª¢æŸ¥å®Œå…¨é‡è¤‡çš„æ¨£æœ¬"""
    
    # å®Œå…¨é‡è¤‡
    full_duplicates = df.duplicated(subset=features + ['Theta.JC'])
    print(f"å®Œå…¨é‡è¤‡: {full_duplicates.sum()} ç­†")
    
    # ç‰¹å¾µé‡è¤‡ä½†ç›®æ¨™ä¸åŒ (å¯èƒ½æ˜¯é‡æ¸¬èª¤å·®)
    feature_duplicates = df.duplicated(subset=features, keep=False)
    ambiguous = df[feature_duplicates & ~full_duplicates]
    
    if len(ambiguous) > 0:
        print(f"ç‰¹å¾µç›¸åŒä½†ç›®æ¨™ä¸åŒ: {len(ambiguous)} ç­†")
        print(ambiguous.groupby(features)['Theta.JC'].agg(['mean', 'std', 'count']))
    
    return full_duplicates, ambiguous
```

#### Step 2: è™•ç†ç­–ç•¥

```python
def clean_duplicates(df, strategy='average'):
    """
    strategy:
    - 'drop': åˆªé™¤é‡è¤‡
    - 'average': ç‰¹å¾µç›¸åŒæ™‚å–ç›®æ¨™å¹³å‡
    - 'keep_first': ä¿ç•™ç¬¬ä¸€å€‹
    """
    
    features = ['TIM_TYPE', 'TIM_THICKNESS', 'TIM_COVERAGE']
    
    if strategy == 'drop':
        return df.drop_duplicates(subset=features + ['Theta.JC'])
    
    elif strategy == 'average':
        # å°ç›¸åŒç‰¹å¾µçš„æ¨£æœ¬ï¼Œå–ç›®æ¨™å¹³å‡å€¼
        df_clean = df.groupby(features, as_index=False).agg({
            'Theta.JC': 'mean',
            # å…¶ä»–æ¬„ä½ä¿ç•™ç¬¬ä¸€å€‹
            **{col: 'first' for col in df.columns if col not in features + ['Theta.JC']}
        })
        return df_clean
    
    elif strategy == 'keep_first':
        return df.drop_duplicates(subset=features, keep='first')
```

#### Step 3: ç•°å¸¸å€¼è™•ç†
```python
def remove_outliers(df, target_col='Theta.JC', method='iqr'):
    """ç§»é™¤ç›®æ¨™è®Šé‡çš„ç•°å¸¸å€¼"""
    
    if method == 'iqr':
        Q1 = df[target_col].quantile(0.25)
        Q3 = df[target_col].quantile(0.75)
        IQR = Q3 - Q1
        
        lower = Q1 - 1.5 * IQR
        upper = Q3 + 1.5 * IQR
        
        mask = (df[target_col] >= lower) & (df[target_col] <= upper)
        
    elif method == 'zscore':
        z_scores = np.abs((df[target_col] - df[target_col].mean()) / df[target_col].std())
        mask = z_scores < 3
    
    outliers = df[~mask]
    print(f"ç§»é™¤ {len(outliers)} å€‹ç•°å¸¸å€¼")
    
    return df[mask], outliers
```

---

## ğŸ¯ æ–¹æ¡ˆ5: æå¤±å‡½æ•¸å„ªåŒ– (P2)

### ç›®æ¨™
é‡å°ç›¸å°èª¤å·®å„ªåŒ–ï¼Œè€Œéçµ•å°èª¤å·®

### ç•¶å‰å•é¡Œ
MSE/MAEå„ªåŒ–çš„æ˜¯çµ•å°èª¤å·®ï¼Œä½†å…¬å¸è¦æ±‚çš„æ˜¯ç›¸å°èª¤å·®<20%

### è§£æ±ºæ–¹æ¡ˆ

#### æ–¹æ¡ˆ5.1: MAPE Loss
```python
def mape_loss(y_pred, y_true):
    """Mean Absolute Percentage Error Loss"""
    epsilon = 1e-8  # é¿å…é™¤ä»¥0
    return torch.mean(torch.abs((y_true - y_pred) / (y_true + epsilon))) * 100

# åœ¨DKLè¨“ç·´ä¸­ä½¿ç”¨
for epoch in range(epochs):
    optimizer.zero_grad()
    output = model(train_x)
    
    # GP likelihood loss
    gp_loss = -mll(output, train_y)
    
    # MAPE loss
    pred_mean = output.mean
    mape = mape_loss(pred_mean, train_y)
    
    # çµ„åˆloss
    total_loss = gp_loss + 0.1 * mape  # æ¬Šé‡å¯èª¿
    
    total_loss.backward()
    optimizer.step()
```

#### æ–¹æ¡ˆ5.2: Huber Loss (å°ç•°å¸¸é»ç©©å¥)
```python
def huber_loss(y_pred, y_true, delta=1.0):
    """å°å¤§èª¤å·®è¼ƒä¸æ•æ„Ÿ"""
    error = y_pred - y_true
    is_small = torch.abs(error) <= delta
    
    small_error = 0.5 * error ** 2
    large_error = delta * (torch.abs(error) - 0.5 * delta)
    
    return torch.where(is_small, small_error, large_error).mean()
```

#### æ–¹æ¡ˆ5.3: Weighted MSE (å°ç•°å¸¸å€åŸŸåŠ æ¬Š)
```python
def weighted_mse_loss(y_pred, y_true, sample_weights):
    """å°é æ¸¬å›°é›£çš„å€åŸŸåŠ å¤§æ¬Šé‡"""
    mse = (y_pred - y_true) ** 2
    weighted_mse = mse * sample_weights
    return weighted_mse.mean()

# å‹•æ…‹è¨ˆç®—æ¬Šé‡
def compute_sample_weights(X, outlier_regions):
    """é è¿‘ç•°å¸¸é»å€åŸŸçš„æ¨£æœ¬æ¬Šé‡æ›´é«˜"""
    weights = torch.ones(len(X))
    
    for region in outlier_regions:
        # è¨ˆç®—åˆ°ç•°å¸¸å€åŸŸçš„è·é›¢
        dist = torch.norm(X - region['center'], dim=1)
        
        # è·é›¢è¶Šè¿‘æ¬Šé‡è¶Šé«˜
        region_weights = torch.exp(-dist / region['radius'])
        weights += region_weights
    
    return weights / weights.sum() * len(X)
```

---

## ğŸ“¦ å®Œæ•´å¯¦ä½œè¨ˆåŠƒ

### Phase 1: åŸºç¤æ”¹é€² (æœŸæœ«å¾Œç¬¬1é€±)

**ä»»å‹™**:
1. âœ… è³‡æ–™æ¸…ç† (é‡è¤‡æ¨£æœ¬è™•ç†)
2. âœ… ç•°å¸¸é»æ·±åº¦åˆ†æ (æ‰¾å‡º17ç­†çš„å…±åŒç‰¹å¾µ)
3. âœ… å¯¦ä½œMAPE loss

**é æœŸæˆæœ**:
- ç•°å¸¸é»å ±å‘Š
- æ¸…ç†å¾Œçš„è³‡æ–™é›†
- åŸºæº–MAPEæ”¹å–„

---

### Phase 2: è¶…åƒæ•¸å„ªåŒ– (æœŸæœ«å¾Œç¬¬2é€±)

**ä»»å‹™**:
1. âœ… æ•´åˆOptuna
2. âœ… å®šç¾©æœå°‹ç©ºé–“
3. âœ… åŸ·è¡Œ100æ¬¡è©¦é©—
4. âœ… åˆ†ææœ€ä½³é…ç½®

**é æœŸæˆæœ**:
- æœ€ä½³è¶…åƒæ•¸çµ„åˆ
- è¶…åƒæ•¸é‡è¦æ€§åˆ†æ
- Max error < 20%

---

### Phase 3: ç‰¹å¾µå·¥ç¨‹ (æœŸæœ«å¾Œç¬¬3é€±)

**ä»»å‹™**:
1. âœ… TIM_TYPE Entity Embedding
2. âœ… Target Encoding (with CV)
3. âœ… å¦‚æœèƒ½æ‹¿åˆ°ç‰©ç†åƒæ•¸ï¼ŒåŠ å…¥Physics-based encoding

**é æœŸæˆæœ**:
- TIM_TYPEæ›´å¥½çš„è¡¨ç¤º
- æ¨¡å‹æº–ç¢ºåº¦æå‡

---

### Phase 4: æ¨¡å‹å„ªåŒ– (æœŸæœ«å¾Œç¬¬4é€±)

**ä»»å‹™**:
1. âœ… çµ„å“¡çš„è¤‡é›œkernel vs ç°¡å–®kernelå°æ¯”
2. âœ… Ensembleå¤šå€‹æœ€ä½³é…ç½®
3. âœ… æœ€çµ‚æ¨¡å‹é¸æ“‡

**é æœŸæˆæœ**:
- ç”Ÿç”¢ç´šæ¨¡å‹
- å®Œæ•´æ–‡æª”

---

## ğŸ”§ æŠ€è¡“ç´°ç¯€å°æ¯”

### ä½ çš„DKL vs çµ„å“¡çš„DKL

| ç‰¹æ€§ | ä½ çš„ç‰ˆæœ¬ | çµ„å“¡çš„ç‰ˆæœ¬ | å»ºè­° |
|------|----------|-----------|------|
| **ç¶²è·¯æ·±åº¦** | [64, 32, 16, 8] | [256, 128, 64] | è¶…åƒæ•¸æœå°‹æ±ºå®š |
| **Kernel** | RBF | RBF+Linear+Matern+RQ | å…ˆè©¦ç°¡å–®ï¼Œå†è©¦è¤‡é›œ |
| **è¨“ç·´ç­–ç•¥** | ä¸‰éšæ®µ | å–®éšæ®µ+æ—©åœ | çµ„å“¡çš„æ›´ç°¡æ½” |
| **Loss** | GP MLL | GP MLL | éƒ½åŠ å…¥MAPE |
| **Scheduler** | ç„¡ | CosineAnnealing | çµ„å“¡çš„æ›´å¥½ |
| **è¨˜æ†¶é«”ç®¡ç†** | ç„¡ | æœ‰gcæ¸…ç† | æ¡ç”¨çµ„å“¡çš„ |

---

## ğŸ’¡ å¿«é€Ÿé–‹å§‹å»ºè­°

### ç«‹å³è¡Œå‹•é …ç›® (æœŸæœ«å‰å¯åš)

1. **ç•°å¸¸é»åˆ†æè…³æœ¬** (30åˆ†é˜):
```python
# å¿«é€Ÿåˆ†æé‚£17ç­†
outlier_df = analyze_outliers(model, X_test, y_test, test_df, threshold=20)
print(outlier_df[['TIM_TYPE', 'TIM_THICKNESS', 'TIM_COVERAGE', 'Error%']].to_string())
```

2. **è³‡æ–™é‡è¤‡æª¢æŸ¥** (15åˆ†é˜):
```python
# æª¢æŸ¥é‡è¤‡
full_dup, ambiguous = check_duplicates(test_df)
print(f"å®Œå…¨é‡è¤‡: {full_dup.sum()} ç­†")
```

3. **æº–å‚™Optunaæ¡†æ¶** (1å°æ™‚):
```python
# å»ºç«‹åŸºæœ¬æ¡†æ¶ï¼ŒæœŸæœ«å¾Œç›´æ¥åŸ·è¡Œ
def objective(trial):
    # è¶…åƒæ•¸å®šç¾©
    ...
    return max_error
```

---

## ğŸ“Š é æœŸæ”¹é€²æ•ˆæœ

### ä¿å®ˆä¼°è¨ˆ

| æ”¹é€²é …ç›® | é æœŸæ•ˆæœ |
|----------|----------|
| è³‡æ–™æ¸…ç† | -2% MAPE |
| è¶…åƒæ•¸å„ªåŒ– | -5~10% max error |
| TIM_TYPE embedding | -3% MAPE |
| MAPE loss | -5% max error |
| **ç¸½è¨ˆ** | **ç•°å¸¸é»<17ç­†ï¼Œmax erroræ¥è¿‘20%** |

### æ¨‚è§€ä¼°è¨ˆ

å¦‚æœè¶…åƒæ•¸æœå°‹é †åˆ© + embeddingæ•ˆæœå¥½:
- âœ… ç•°å¸¸é»é™åˆ° 5-10ç­†
- âœ… Max error < 15%
- âœ… MAPE < 3%

---

## âœ… æª¢æŸ¥æ¸…å–®

æœŸæœ«å¾Œé–‹å§‹å‰ï¼Œæº–å‚™å¥½ï¼š

- [ ] çµ„å“¡ç¨‹å¼ç¢¼æ•´åˆæ¸¬è©¦
- [ ] Optunaå®‰è£å’Œæ¸¬è©¦
- [ ] è³‡æ–™æ¸…ç†è…³æœ¬æº–å‚™
- [ ] ç•°å¸¸é»åˆ†æè…³æœ¬æº–å‚™
- [ ] å¯¦é©—è¨˜éŒ„è¡¨æ ¼è¨­è¨ˆ
- [ ] èˆ‡çµ„å“¡åˆ†å·¥è¨è«–

ç¥æœŸæœ«é †åˆ©ï¼ğŸ‰
