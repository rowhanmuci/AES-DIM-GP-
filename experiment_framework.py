"""
å®Œæ•´å¯¦é©—æ¡†æ¶ - çµ±ä¸€è©•ä¼°æ‰€æœ‰DIM-GPè®Šé«”
"""

import numpy as np
import pandas as pd
import time
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import warnings
warnings.filterwarnings('ignore')

from dimgp_complete_models import (
    MLPModel, XGBoostModel, StandardGP,
    DeepKernelLearning, DeepMixtureGPExperts, EnsembleModel,
    get_model
)


class ExperimentFramework:
    """çµ±ä¸€çš„å¯¦é©—æ¡†æ¶"""
    
    def __init__(self, dataset_name='Above'):
        self.dataset_name = dataset_name
        self.results = {}
        self.models = {}
        
    def load_data(self, X_train, y_train, X_test, y_test):
        """è¼‰å…¥è³‡æ–™"""
        self.X_train = X_train
        self.y_train = y_train
        self.X_test = X_test
        self.y_test = y_test
        
        print(f"\n{'='*60}")
        print(f"Dataset: {self.dataset_name}")
        print(f"Train size: {len(X_train)}, Test size: {len(X_test)}")
        print(f"Features: {X_train.shape[1]}")
        print(f"Target range: [{y_train.min():.4f}, {y_train.max():.4f}]")
        print(f"{'='*60}\n")
    
    def run_model(self, model_name, model_params=None):
        """è¨“ç·´å’Œè©•ä¼°å–®å€‹æ¨¡å‹"""
        print(f"\n{'='*60}")
        print(f"Running: {model_name}")
        print(f"{'='*60}")
        
        if model_params is None:
            model_params = {}
        
        # å»ºç«‹æ¨¡å‹
        try:
            model = get_model(model_name, **model_params)
        except Exception as e:
            print(f"Error creating model: {e}")
            return None
        
        # è¨“ç·´
        start_time = time.time()
        try:
            model.fit(self.X_train, self.y_train)
            train_time = time.time() - start_time
            print(f"âœ“ Training completed in {train_time:.2f}s")
        except Exception as e:
            print(f"âœ— Training failed: {e}")
            return None
        
        # é æ¸¬
        try:
            start_time = time.time()
            
            # æª¢æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æ´ä¸ç¢ºå®šæ€§ä¼°è¨ˆ
            if hasattr(model, 'predict'):
                import inspect
                sig = inspect.signature(model.predict)
                if 'return_std' in sig.parameters:
                    y_pred, y_std = model.predict(self.X_test, return_std=True)
                    has_uncertainty = True
                else:
                    y_pred = model.predict(self.X_test)
                    y_std = np.zeros_like(y_pred)
                    has_uncertainty = False
            else:
                y_pred = model.predict(self.X_test)
                y_std = np.zeros_like(y_pred)
                has_uncertainty = False
            
            pred_time = time.time() - start_time
            print(f"âœ“ Prediction completed in {pred_time:.2f}s")
            
        except Exception as e:
            print(f"âœ— Prediction failed: {e}")
            return None
        
        # è©•ä¼°æŒ‡æ¨™
        metrics = self._compute_metrics(y_pred, y_std, has_uncertainty)
        
        # å„²å­˜çµæœ
        self.results[model_name] = {
            'metrics': metrics,
            'predictions': y_pred,
            'std': y_std,
            'train_time': train_time,
            'pred_time': pred_time,
            'has_uncertainty': has_uncertainty
        }
        
        self.models[model_name] = model
        
        # é¡¯ç¤ºçµæœ
        self._print_metrics(model_name, metrics, train_time)
        
        return metrics
    
    def _compute_metrics(self, y_pred, y_std, has_uncertainty):
        """è¨ˆç®—è©•ä¼°æŒ‡æ¨™"""
        metrics = {}
        
        # åŸºæœ¬æŒ‡æ¨™
        metrics['RMSE'] = np.sqrt(mean_squared_error(self.y_test, y_pred))
        metrics['MAE'] = mean_absolute_error(self.y_test, y_pred)
        metrics['R2'] = r2_score(self.y_test, y_pred)
        metrics['MAPE'] = np.mean(np.abs((self.y_test - y_pred) / self.y_test)) * 100
        
        # ä¸ç¢ºå®šæ€§æŒ‡æ¨™
        if has_uncertainty and np.any(y_std > 0):
            # 95% ä¿¡è³´å€é–“
            ci_lower = y_pred - 1.96 * y_std
            ci_upper = y_pred + 1.96 * y_std
            
            # è¦†è“‹ç‡ (Coverage)
            coverage = np.mean((self.y_test >= ci_lower) & (self.y_test <= ci_upper))
            metrics['CI_Coverage'] = coverage * 100
            
            # å¹³å‡CIå¯¬åº¦
            metrics['CI_Width'] = np.mean(ci_upper - ci_lower)
            
            # Calibration: æª¢æŸ¥é æ¸¬èª¤å·®æ˜¯å¦èˆ‡ä¸ç¢ºå®šæ€§ç›¸ç¬¦
            errors = np.abs(self.y_test - y_pred)
            metrics['Mean_Error'] = np.mean(errors)
            metrics['Mean_Std'] = np.mean(y_std)
            
            # Negative Log Predictive Density (NLPD)
            # å‡è¨­Gaussianåˆ†ä½ˆ
            nlpd = 0.5 * np.log(2 * np.pi * y_std**2) + 0.5 * ((self.y_test - y_pred)**2 / y_std**2)
            metrics['NLPD'] = np.mean(nlpd)
        else:
            metrics['CI_Coverage'] = None
            metrics['CI_Width'] = None
            metrics['Mean_Error'] = None
            metrics['Mean_Std'] = None
            metrics['NLPD'] = None
        
        return metrics
    
    def _print_metrics(self, model_name, metrics, train_time):
        """é¡¯ç¤ºæŒ‡æ¨™"""
        print(f"\n{'â”€'*60}")
        print(f"Results for {model_name}:")
        print(f"{'â”€'*60}")
        print(f"  RMSE:      {metrics['RMSE']:.6f}")
        print(f"  MAE:       {metrics['MAE']:.6f}")
        print(f"  RÂ²:        {metrics['R2']:.6f}")
        print(f"  MAPE:      {metrics['MAPE']:.2f}%")
        print(f"  Time:      {train_time:.2f}s")
        
        if metrics['CI_Coverage'] is not None:
            print(f"\n  Uncertainty Quantification:")
            print(f"  CI Coverage:  {metrics['CI_Coverage']:.2f}%")
            print(f"  CI Width:     {metrics['CI_Width']:.6f}")
            print(f"  Mean Error:   {metrics['Mean_Error']:.6f}")
            print(f"  Mean Std:     {metrics['Mean_Std']:.6f}")
            print(f"  NLPD:         {metrics['NLPD']:.4f}")
        
        print(f"{'â”€'*60}\n")
    
    def run_all_models(self):
        """åŸ·è¡Œæ‰€æœ‰æ¨¡å‹"""
        print(f"\n{'#'*60}")
        print(f"# Running Complete Experiment: {self.dataset_name}")
        print(f"{'#'*60}\n")
        
        # æ¨¡å‹é…ç½®
        model_configs = {
            'MLP': {},
            'XGBoost': {},
            'GP': {'subsample': 1000},
            'DKL': {
                'input_dim': self.X_train.shape[1],
                'hidden_dims': [64, 32, 16],
                'feature_dim': 8,
                'epochs': 100
            },
            'MoE': {
                'n_experts': 3,
                'n_inducing': 100,
                'hidden_dims': (32, 16)
            },
            'Ensemble': {
                'mlp_weight': 0.5,
                'xgb_weight': 0.5
            }
        }
        
        # åŸ·è¡Œæ‰€æœ‰æ¨¡å‹
        for model_name, params in model_configs.items():
            try:
                self.run_model(model_name, params)
            except Exception as e:
                print(f"âœ— {model_name} failed: {e}")
                continue
        
        print(f"\n{'#'*60}")
        print(f"# All Models Completed!")
        print(f"{'#'*60}\n")
    
    def get_summary_table(self):
        """ç”Ÿæˆçµæœæ‘˜è¦è¡¨"""
        data = []
        
        for model_name, result in self.results.items():
            metrics = result['metrics']
            row = {
                'Model': model_name,
                'RMSE': metrics['RMSE'],
                'MAE': metrics['MAE'],
                'RÂ²': metrics['R2'],
                'MAPE (%)': metrics['MAPE'],
                'Train Time (s)': result['train_time'],
                'Has UQ': 'âœ“' if result['has_uncertainty'] else 'âœ—'
            }
            
            if metrics['CI_Coverage'] is not None:
                row['CI Coverage (%)'] = metrics['CI_Coverage']
                row['CI Width'] = metrics['CI_Width']
                row['NLPD'] = metrics['NLPD']
            
            data.append(row)
        
        df = pd.DataFrame(data)
        
        # æ’åºï¼šå…ˆæŒ‰RÂ²é™åºï¼Œå†æŒ‰RMSEå‡åº
        if len(df) > 0:
            df = df.sort_values(['RÂ²', 'RMSE'], ascending=[False, True])
        
        return df
    
    def print_summary(self):
        """é¡¯ç¤ºæ‘˜è¦"""
        df = self.get_summary_table()
        
        print(f"\n{'='*80}")
        print(f"SUMMARY: {self.dataset_name} Dataset")
        print(f"{'='*80}\n")
        print(df.to_string(index=False))
        print(f"\n{'='*80}\n")
        
        # æœ€ä½³æ¨¡å‹
        if len(df) > 0:
            best_acc = df.iloc[0]['Model']
            print(f"ğŸ† Best Accuracy: {best_acc} (RÂ²={df.iloc[0]['RÂ²']:.6f})")
            
            if 'CI Coverage (%)' in df.columns:
                uq_models = df[df['Has UQ'] == 'âœ“']
                if len(uq_models) > 0:
                    best_uq = uq_models.iloc[0]['Model']
                    print(f"ğŸ¯ Best with UQ: {best_uq}")
        
        return df
    
    def save_results(self, filename):
        """å„²å­˜çµæœ"""
        df = self.get_summary_table()
        df.to_csv(filename, index=False)
        print(f"âœ“ Results saved to {filename}")


def run_complete_experiment(X_train, y_train, X_test, y_test, dataset_name='Dataset'):
    """åŸ·è¡Œå®Œæ•´å¯¦é©—çš„ä¾¿æ·å‡½æ•¸"""
    
    exp = ExperimentFramework(dataset_name=dataset_name)
    exp.load_data(X_train, y_train, X_test, y_test)
    exp.run_all_models()
    summary = exp.print_summary()
    
    return exp, summary


if __name__ == "__main__":
    print("Experiment Framework Ready!")
    print("Use: run_complete_experiment(X_train, y_train, X_test, y_test)")
