"""
Phase 2B æ¨£æœ¬åŠ æ¬Š - æœ€çµ‚ç”Ÿç”¢ç‰ˆæœ¬
å¯è¨­å®šéš¨æ©Ÿç¨®å­ä»¥ç¢ºä¿å¯é‡ç¾æ€§

ä½¿ç”¨æ–¹æ³•:
    python phase2b_final.py                    # ä½¿ç”¨é è¨­ç¨®å­2024
    python phase2b_final.py --seed 42          # ä½¿ç”¨æŒ‡å®šç¨®å­
    python phase2b_final.py --seed 2024 -v     # è©³ç´°æ¨¡å¼
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import gpytorch
from sklearn.preprocessing import StandardScaler
import warnings
import random
import os
import argparse

warnings.filterwarnings('ignore')

torch.set_default_dtype(torch.float64)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def set_seed(seed):
    """
    è¨­ç½®éš¨æ©Ÿç¨®å­ä»¥ç¢ºä¿å®Œå…¨å¯é‡ç¾æ€§
    
    Args:
        seed: éš¨æ©Ÿç¨®å­æ•¸å€¼
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)
    
    print(f"âœ“ éš¨æ©Ÿç¨®å­è¨­å®šç‚º: {seed}")


def clear_gpu_cache():
    """æ¸…ç©ºGPUå¿«å–"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()


# ==========================================
# æ¨¡å‹å®šç¾©
# ==========================================

class DnnFeatureExtractor(nn.Module):
    """æ·±åº¦ç¥ç¶“ç¶²è·¯ç‰¹å¾µæå–å™¨"""
    
    def __init__(self, input_dim, hidden_dims=[64, 32, 16], output_dim=8, dropout=0.1):
        super().__init__()
        
        layers = []
        prev_dim = input_dim
        
        for h_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, h_dim),
                nn.BatchNorm1d(h_dim),
                nn.ReLU(),
                nn.Dropout(dropout)
            ])
            prev_dim = h_dim
        
        layers.append(nn.Linear(prev_dim, output_dim))
        
        self.network = nn.Sequential(*layers)
        self.output_dim = output_dim
    
    def forward(self, x):
        return self.network(x)


class GPRegressionModel(gpytorch.models.ExactGP):
    """é«˜æ–¯éç¨‹å›æ­¸æ¨¡å‹"""
    
    def __init__(self, train_x, train_y, likelihood, feature_extractor):
        super().__init__(train_x, train_y, likelihood)
        
        self.feature_extractor = feature_extractor
        self.mean_module = gpytorch.means.ConstantMean()
        
        self.covar_module = gpytorch.kernels.ScaleKernel(
            gpytorch.kernels.RBFKernel(ard_num_dims=feature_extractor.output_dim)
        )
    
    def forward(self, x):
        projected_x = self.feature_extractor(x)
        mean_x = self.mean_module(projected_x)
        covar_x = self.covar_module(projected_x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)


# ==========================================
# æå¤±å‡½æ•¸
# ==========================================

def weighted_mape_loss(y_pred, y_true, weights, epsilon=1e-8):
    """
    åŠ æ¬ŠMAPEæå¤±å‡½æ•¸
    
    æ³¨æ„: åœ¨æ¨™æº–åŒ–ç©ºé–“è¨ˆç®—ï¼ˆè¨“ç·´æ™‚ç”¨ï¼‰
    """
    mape_per_sample = torch.abs((y_true - y_pred) / 
                                (torch.abs(y_true) + epsilon)) * 100
    weighted_mape = torch.sum(mape_per_sample * weights) / torch.sum(weights)
    return weighted_mape


def compute_sample_weights(X, weight_factor=3.0):
    """
    è¨ˆç®—æ¨£æœ¬æ¬Šé‡
    
    å›°é›£æ¨£æœ¬å®šç¾©: TIM_TYPE=3 AND Coverage=0.8 AND THICKNESS>=220
    
    Args:
        X: ç‰¹å¾µçŸ©é™£ [TIM_TYPE, TIM_THICKNESS, TIM_COVERAGE]
        weight_factor: å›°é›£æ¨£æœ¬çš„æ¬Šé‡å€æ•¸
        
    Returns:
        weights: æ¨£æœ¬æ¬Šé‡æ•¸çµ„
    """
    weights = np.ones(len(X))
    
    difficult_mask = (
        (X[:, 0] == 3) &      # TIM_TYPE = 3
        (X[:, 2] == 0.8) &    # TIM_COVERAGE = 0.8
        (X[:, 1] >= 220)      # TIM_THICKNESS >= 220
    )
    
    weights[difficult_mask] *= weight_factor
    
    return weights


# ==========================================
# è¨“ç·´èˆ‡è©•ä¼°
# ==========================================

def train_model(X_train, y_train, config, verbose=True):
    """
    è¨“ç·´DKLæ¨¡å‹
    
    Args:
        X_train: è¨“ç·´ç‰¹å¾µ
        y_train: è¨“ç·´æ¨™ç±¤
        config: è¨“ç·´é…ç½®
        verbose: æ˜¯å¦é¡¯ç¤ºè¨“ç·´éç¨‹
        
    Returns:
        model, likelihood, scaler_x, scaler_y
    """
    # è¨ˆç®—æ¨£æœ¬æ¬Šé‡
    sample_weights_np = compute_sample_weights(X_train, config['sample_weight_factor'])
    
    if verbose:
        difficult_count = np.sum(sample_weights_np > 1.0)
        print(f"\nè¨ˆç®—æ¨£æœ¬æ¬Šé‡:")
        print(f"  ç­–ç•¥: Type 3 + Coverage 0.8 + THICKNESS>=220")
        print(f"  å›°é›£æ¨£æœ¬æ•¸: {difficult_count} ({difficult_count/len(X_train)*100:.2f}%)")
        print(f"  æ¬Šé‡å€æ•¸: {config['sample_weight_factor']}x")
    
    # æ¨™æº–åŒ–
    scaler_x = StandardScaler()
    scaler_y = StandardScaler()
    
    X_train_scaled = scaler_x.fit_transform(X_train)
    y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
    
    train_x = torch.from_numpy(X_train_scaled).to(device)
    train_y = torch.from_numpy(y_train_scaled).to(device)
    sample_weights = torch.from_numpy(sample_weights_np).to(device)
    
    # å»ºç«‹æ¨¡å‹
    feature_extractor = DnnFeatureExtractor(
        input_dim=train_x.shape[1],
        hidden_dims=config['hidden_dims'],
        output_dim=config['feature_dim'],
        dropout=config['dropout']
    ).to(device)
    
    likelihood = gpytorch.likelihoods.GaussianLikelihood().to(device)
    model = GPRegressionModel(train_x, train_y, likelihood, feature_extractor).to(device)
    
    # å„ªåŒ–å™¨
    optimizer = optim.Adam([
        {'params': model.feature_extractor.parameters(), 'lr': config['lr'], 'weight_decay': 1e-4},
        {'params': model.covar_module.parameters()},
        {'params': model.mean_module.parameters()},
        {'params': model.likelihood.parameters()},
    ], lr=config['lr'])
    
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=2)
    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)
    
    # è¨“ç·´
    if verbose:
        print(f"\né–‹å§‹è¨“ç·´...")
    
    model.train()
    likelihood.train()
    
    best_loss = float('inf')
    patience_counter = 0
    
    for epoch in range(config['epochs']):
        optimizer.zero_grad()
        
        output = model(train_x)
        gp_loss = -mll(output, train_y)
        mape = weighted_mape_loss(output.mean, train_y, sample_weights)
        total_loss = gp_loss + config['mape_weight'] * mape
        
        total_loss.backward()
        optimizer.step()
        scheduler.step()
        
        current_loss = total_loss.item()
        
        # é¡¯ç¤ºè¨“ç·´é€²åº¦
        if verbose and (epoch + 1) % 100 == 0:
            print(f"Epoch {epoch+1}: GP Loss={gp_loss.item():.4f}, "
                  f"MAPE={mape.item():.2f}%, Total={total_loss.item():.4f}")
        
        # Early stopping
        if current_loss < best_loss:
            best_loss = current_loss
            patience_counter = 0
            best_state = {
                'model': model.state_dict(),
                'likelihood': likelihood.state_dict(),
            }
        else:
            patience_counter += 1
        
        if patience_counter >= config['patience']:
            if verbose:
                print(f"æ—©åœ at Epoch {epoch+1}")
            break
    
    # è¼‰å…¥æœ€ä½³æ¨¡å‹
    model.load_state_dict(best_state['model'])
    likelihood.load_state_dict(best_state['likelihood'])
    
    if verbose:
        print(f"è¨“ç·´å®Œæˆ (Final Loss: {best_loss:.4f})")
    
    return model, likelihood, scaler_x, scaler_y


def evaluate_model(model, likelihood, X_test, y_test, scaler_x, scaler_y, verbose=True):
    """
    è©•ä¼°æ¨¡å‹
    
    Args:
        model: è¨“ç·´å¥½çš„æ¨¡å‹
        likelihood: Likelihood
        X_test: æ¸¬è©¦ç‰¹å¾µ
        y_test: æ¸¬è©¦æ¨™ç±¤
        scaler_x, scaler_y: æ¨™æº–åŒ–å™¨
        verbose: æ˜¯å¦é¡¯ç¤ºè©•ä¼°çµæœ
        
    Returns:
        results: åŒ…å«MAPE, outliersç­‰æŒ‡æ¨™çš„å­—å…¸
    """
    model.eval()
    likelihood.eval()
    
    X_test_scaled = scaler_x.transform(X_test)
    test_x = torch.from_numpy(X_test_scaled).to(device)
    
    with torch.no_grad(), gpytorch.settings.fast_pred_var():
        pred_dist = likelihood(model(test_x))
        y_pred_scaled = pred_dist.mean.cpu().numpy()
        y_std_scaled = pred_dist.stddev.cpu().numpy()
    
    # åæ¨™æº–åŒ–
    y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
    y_std = y_std_scaled * scaler_y.scale_[0]
    
    # è¨ˆç®—æŒ‡æ¨™ (åœ¨åŸå§‹ç©ºé–“)
    relative_errors = np.abs((y_test - y_pred) / y_test) * 100
    
    mape = np.mean(relative_errors)
    mae = np.mean(np.abs(y_test - y_pred))
    max_error = np.max(relative_errors)
    
    outliers_20 = np.sum(relative_errors > 20)
    outliers_15 = np.sum(relative_errors > 15)
    outliers_10 = np.sum(relative_errors > 10)
    
    # Type 3åˆ†æ
    type3_mask = X_test[:, 0] == 3
    type3_outliers = np.sum((relative_errors > 20) & type3_mask)
    
    if verbose:
        print(f"\n{'='*60}")
        print(f"è©•ä¼°çµæœ")
        print(f"{'='*60}")
        print(f"æ¨£æœ¬æ•¸: {len(y_test)}")
        print(f"\næº–ç¢ºåº¦:")
        print(f"  MAPE:      {mape:.2f}%")
        print(f"  MAE:       {mae:.4f}")
        print(f"  Max Error: {max_error:.2f}%")
        print(f"\nç•°å¸¸é»:")
        print(f"  >20%: {outliers_20}/{len(y_test)} ({outliers_20/len(y_test)*100:.2f}%)")
        print(f"  >15%: {outliers_15}/{len(y_test)} ({outliers_15/len(y_test)*100:.2f}%)")
        print(f"  >10%: {outliers_10}/{len(y_test)} ({outliers_10/len(y_test)*100:.2f}%)")
        if np.sum(type3_mask) > 0:
            print(f"\nType 3ç•°å¸¸é»: {type3_outliers}/{np.sum(type3_mask)}")
        print(f"{'='*60}\n")
    
    results = {
        'mape': mape,
        'mae': mae,
        'max_error': max_error,
        'outliers_20': outliers_20,
        'outliers_15': outliers_15,
        'outliers_10': outliers_10,
        'type3_outliers': type3_outliers,
        'predictions': y_pred,
        'std': y_std,
        'errors': relative_errors
    }
    
    return results


def save_predictions(X_test, y_test, results, filename):
    """ä¿å­˜é æ¸¬çµæœåˆ°CSV"""
    df = pd.DataFrame({
        'TIM_TYPE': X_test[:, 0],
        'TIM_THICKNESS': X_test[:, 1],
        'TIM_COVERAGE': X_test[:, 2],
        'True': y_test,
        'Predicted': results['predictions'],
        'Error%': results['errors'],
        'Std': results['std']
    })
    
    df.to_csv(filename, index=False)
    print(f"âœ“ é æ¸¬çµæœå·²ä¿å­˜åˆ°: {filename}")


# ==========================================
# ä¸»å‡½æ•¸
# ==========================================

def main(seed=2024, verbose=True):
    """
    ä¸»è¨“ç·´æµç¨‹
    
    Args:
        seed: éš¨æ©Ÿç¨®å­
        verbose: æ˜¯å¦é¡¯ç¤ºè©³ç´°ä¿¡æ¯
    """
    # è¨­ç½®éš¨æ©Ÿç¨®å­å’Œæ¸…ç©ºGPU
    clear_gpu_cache()
    set_seed(seed)
    
    print(f"\nä½¿ç”¨è£ç½®: {device}\n")
    
    print("="*60)
    print("Phase 2B: æ¨£æœ¬åŠ æ¬Š (Sample Weighting) - æœ€çµ‚ç‰ˆæœ¬")
    print("="*60)
    
    # ç‰¹å¾µå’Œç›®æ¨™
    feature_cols = ['TIM_TYPE', 'TIM_THICKNESS', 'TIM_COVERAGE']
    target_col = 'Theta.JC'
    
    # é…ç½®ï¼ˆæœ€ä½³åƒæ•¸ï¼‰
    config = {
        'hidden_dims': [64, 32, 16],
        'feature_dim': 8,
        'dropout': 0.1,
        'lr': 0.01,
        'epochs': 500,
        'patience': 50,
        'mape_weight': 0.1,
        'sample_weight_factor': 3.0,
    }
    
    if verbose:
        print(f"\né…ç½®:")
        for key, value in config.items():
            print(f"  {key}: {value}")
    
    # ==========================================
    # Above Dataset
    # ==========================================
    
    print(f"\n\n{'ğŸ”µ Above 50% Coverage'}\n")
    
    # è¼‰å…¥è³‡æ–™
    train_above = pd.read_excel('data/train/Above.xlsx')
    test_above = pd.read_excel('data/test/Above.xlsx')
    
    # è¨“ç·´é›†æ¸…ç†ï¼ˆå»é™¤é‡è¤‡ï¼Œå–å¹³å‡ï¼‰
    train_above_clean = train_above.groupby(feature_cols, as_index=False).agg({
        target_col: 'mean'
    })
    
    print(f"è¨“ç·´é›†: {len(train_above_clean)} ç­†")
    print(f"æ¸¬è©¦é›†: {len(test_above)} ç­†")
    
    X_train_above = train_above_clean[feature_cols].values
    y_train_above = train_above_clean[target_col].values
    
    X_test_above = test_above[feature_cols].values
    y_test_above = test_above[target_col].values
    
    # è¨“ç·´
    model_above, likelihood_above, scaler_x_above, scaler_y_above = train_model(
        X_train_above, y_train_above, config, verbose=verbose
    )
    
    # è©•ä¼°
    results_above = evaluate_model(
        model_above, likelihood_above, 
        X_test_above, y_test_above, 
        scaler_x_above, scaler_y_above,
        verbose=verbose
    )
    
    # ä¿å­˜é æ¸¬çµæœ
    save_predictions(X_test_above, y_test_above, results_above, 
                     f'phase2b_final_above_seed{seed}_predictions.csv')
    
    # ==========================================
    # Below Dataset
    # ==========================================
    
    print(f"\n\n{'ğŸ”µ Below 50% Coverage'}\n")
    
    # è¼‰å…¥è³‡æ–™
    train_below = pd.read_excel('data/train/Below.xlsx')
    test_below = pd.read_excel('data/test/Below.xlsx')
    
    # è¨“ç·´é›†æ¸…ç†
    train_below_clean = train_below.groupby(feature_cols, as_index=False).agg({
        target_col: 'mean'
    })
    
    print(f"è¨“ç·´é›†: {len(train_below_clean)} ç­†")
    print(f"æ¸¬è©¦é›†: {len(test_below)} ç­†")
    
    X_train_below = train_below_clean[feature_cols].values
    y_train_below = train_below_clean[target_col].values
    
    X_test_below = test_below[feature_cols].values
    y_test_below = test_below[target_col].values
    
    # è¨“ç·´
    model_below, likelihood_below, scaler_x_below, scaler_y_below = train_model(
        X_train_below, y_train_below, config, verbose=verbose
    )
    
    # è©•ä¼°
    results_below = evaluate_model(
        model_below, likelihood_below,
        X_test_below, y_test_below,
        scaler_x_below, scaler_y_below,
        verbose=verbose
    )
    
    # ä¿å­˜é æ¸¬çµæœ
    save_predictions(X_test_below, y_test_below, results_below,
                     f'phase2b_final_below_seed{seed}_predictions.csv')
    
    # ==========================================
    # ç¸½çµ
    # ==========================================
    
    print("\n" + "="*60)
    print("æœ€çµ‚çµæœç¸½çµ")
    print("="*60)
    print(f"éš¨æ©Ÿç¨®å­: {seed}")
    print(f"\nAboveè³‡æ–™é›†:")
    print(f"  ç•°å¸¸é» (>20%): {results_above['outliers_20']}/{len(y_test_above)} ({results_above['outliers_20']/len(y_test_above)*100:.2f}%)")
    print(f"  MAPE: {results_above['mape']:.2f}%")
    print(f"  Type 3ç•°å¸¸é»: {results_above['type3_outliers']}")
    
    print(f"\nBelowè³‡æ–™é›†:")
    print(f"  ç•°å¸¸é» (>20%): {results_below['outliers_20']}/{len(y_test_below)} ({results_below['outliers_20']/len(y_test_below)*100:.2f}%)")
    print(f"  MAPE: {results_below['mape']:.2f}%")
    
    print("\n" + "="*60)
    print("âœ“ è¨“ç·´å®Œæˆï¼")
    print("="*60 + "\n")
    
    return {
        'above': results_above,
        'below': results_below,
        'seed': seed
    }


if __name__ == "__main__":
    # å‘½ä»¤è¡Œåƒæ•¸è§£æ
    parser = argparse.ArgumentParser(description='Phase 2B æ¨£æœ¬åŠ æ¬Š - æœ€çµ‚ç‰ˆæœ¬')
    parser.add_argument('--seed', type=int, default=2024, 
                        help='éš¨æ©Ÿç¨®å­ (é è¨­: 2024)')
    parser.add_argument('-v', '--verbose', action='store_true', 
                        help='é¡¯ç¤ºè©³ç´°è¨“ç·´éç¨‹')
    
    args = parser.parse_args()
    
    # é‹è¡Œè¨“ç·´
    results = main(seed=args.seed, verbose=args.verbose)
    
    # é¡¯ç¤ºæœ€ä½³ç¨®å­å»ºè­°
    print("\nğŸ’¡ æç¤º:")
    print(f"   ç•¶å‰ä½¿ç”¨ç¨®å­: {args.seed}")
    print(f"   æœ€ä½³ç¨®å­ (ç¶“10æ¬¡æ¸¬è©¦): 2024")
    print(f"   å…¶ä»–å„ªç§€ç¨®å­: 42, 123, 999\n")
    print("é‹è¡Œç¤ºä¾‹:")
    print("  python phase2b_final.py                # ä½¿ç”¨é è¨­ç¨®å­2024")
    print("  python phase2b_final.py --seed 42      # ä½¿ç”¨ç¨®å­42")
    print("  python phase2b_final.py --seed 123 -v  # ä½¿ç”¨ç¨®å­123ï¼Œè©³ç´°æ¨¡å¼\n")
