"""
Phase 2B: æ¨£æœ¬åŠ æ¬Š (Sample Weighting)
æ ¸å¿ƒç­–ç•¥: å°å›°é›£æ¨£æœ¬ï¼ˆType 3 + Coverage 0.8 + å¤§THICKNESSï¼‰åŠ å¤§æ¬Šé‡
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import gpytorch
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, max_error
import warnings
warnings.filterwarnings('ignore')

torch.set_default_dtype(torch.float64)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ä½¿ç”¨è£ç½®: {device}\n")


# ==========================================
# æ¨¡å‹å®šç¾© (èˆ‡Phase 1ç›¸åŒ)
# ==========================================

class DnnFeatureExtractor(nn.Module):
    def __init__(self, input_dim, hidden_dims=[64, 32, 16], output_dim=8, dropout=0.1):
        super().__init__()
        
        layers = []
        prev_dim = input_dim
        
        for h_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, h_dim),
                nn.BatchNorm1d(h_dim),
                nn.ReLU(),
                nn.Dropout(dropout)
            ])
            prev_dim = h_dim
        
        layers.append(nn.Linear(prev_dim, output_dim))
        
        self.network = nn.Sequential(*layers)
        self.output_dim = output_dim
    
    def forward(self, x):
        return self.network(x)


class GPRegressionModel(gpytorch.models.ExactGP):
    def __init__(self, train_x, train_y, likelihood, feature_extractor):
        super().__init__(train_x, train_y, likelihood)
        
        self.feature_extractor = feature_extractor
        self.mean_module = gpytorch.means.ConstantMean()
        
        self.covar_module = gpytorch.kernels.ScaleKernel(
            gpytorch.kernels.RBFKernel(ard_num_dims=feature_extractor.output_dim)
        )
    
    def forward(self, x):
        projected_x = self.feature_extractor(x)
        mean_x = self.mean_module(projected_x)
        covar_x = self.covar_module(projected_x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)


def mape_loss(y_pred, y_true, epsilon=1e-8):
    """MAPE Loss"""
    return torch.mean(torch.abs((y_true - y_pred) / (torch.abs(y_true) + epsilon))) * 100


def weighted_mape_loss(y_pred, y_true, weights, epsilon=1e-8):
    """åŠ æ¬ŠMAPE Loss"""
    mape_per_sample = torch.abs((y_true - y_pred) / (torch.abs(y_true) + epsilon)) * 100
    weighted_mape = torch.sum(mape_per_sample * weights) / torch.sum(weights)
    return weighted_mape


# ==========================================
# æ¨£æœ¬æ¬Šé‡è¨ˆç®— (æ ¸å¿ƒæ”¹é€²!)
# ==========================================

def compute_sample_weights(X, y, strategy='difficult_samples', weight_factor=3.0):
    """
    è¨ˆç®—æ¨£æœ¬æ¬Šé‡
    
    Parameters:
    -----------
    X : numpy array [N, 3]
        ç‰¹å¾µçŸ©é™£ [TIM_TYPE, THICKNESS, COVERAGE]
    y : numpy array [N]
        ç›®æ¨™è®Šé‡ (ç”¨æ–¼è­˜åˆ¥å°å€¼æ¨£æœ¬)
    strategy : str
        æ¬Šé‡ç­–ç•¥:
        - 'difficult_samples': å°Type 3 + Coverage 0.8 + å¤§THICKNESSåŠ æ¬Š
        - 'type3_only': åªå°Type 3åŠ æ¬Š
        - 'small_values': å°å°Theta.JCå€¼åŠ æ¬Š
        - 'combined': çµ„åˆç­–ç•¥
    weight_factor : float
        æ¬Šé‡å€æ•¸ (é è¨­3å€)
    
    Returns:
    --------
    weights : numpy array [N]
        æ¨£æœ¬æ¬Šé‡
    """
    
    weights = np.ones(len(X))
    
    if strategy == 'difficult_samples':
        # ç­–ç•¥1: Type 3 + Coverage 0.8 + å¤§THICKNESS
        difficult_mask = (
            (X[:, 0] == 3) &              # Type 3
            (X[:, 2] == 0.8) &            # Coverage 0.8
            (X[:, 1] >= 220)              # THICKNESS >= 220
        )
        weights[difficult_mask] *= weight_factor
        
        n_difficult = difficult_mask.sum()
        print(f"  ç­–ç•¥: Type 3 + Coverage 0.8 + THICKNESS>=220")
        print(f"  å›°é›£æ¨£æœ¬æ•¸: {n_difficult} ({n_difficult/len(X)*100:.2f}%)")
        print(f"  æ¬Šé‡å€æ•¸: {weight_factor}x\n")
    
    elif strategy == 'type3_only':
        # ç­–ç•¥2: åªå°Type 3åŠ æ¬Š
        type3_mask = (X[:, 0] == 3)
        weights[type3_mask] *= weight_factor
        
        n_type3 = type3_mask.sum()
        print(f"  ç­–ç•¥: Type 3å…¨éƒ¨æ¨£æœ¬")
        print(f"  Type 3æ¨£æœ¬æ•¸: {n_type3} ({n_type3/len(X)*100:.2f}%)")
        print(f"  æ¬Šé‡å€æ•¸: {weight_factor}x\n")
    
    elif strategy == 'small_values':
        # ç­–ç•¥3: å°å°Theta.JCå€¼åŠ æ¬Š (ç›¸å°èª¤å·®å®¹æ˜“å¤§)
        small_value_mask = (y < np.percentile(y, 25))  # æœ€å°çš„25%
        weights[small_value_mask] *= weight_factor
        
        n_small = small_value_mask.sum()
        print(f"  ç­–ç•¥: Theta.JC < {np.percentile(y, 25):.4f} (æœ€å°25%)")
        print(f"  å°å€¼æ¨£æœ¬æ•¸: {n_small} ({n_small/len(X)*100:.2f}%)")
        print(f"  æ¬Šé‡å€æ•¸: {weight_factor}x\n")
    
    elif strategy == 'combined':
        # ç­–ç•¥4: çµ„åˆ (Type 3å›°é›£æ¨£æœ¬ + å°å€¼)
        difficult_mask = (
            (X[:, 0] == 3) & 
            (X[:, 2] == 0.8) & 
            (X[:, 1] >= 220)
        )
        small_value_mask = (y < np.percentile(y, 25))
        
        # Type 3å›°é›£æ¨£æœ¬: 3å€æ¬Šé‡
        weights[difficult_mask] *= weight_factor
        
        # å°å€¼æ¨£æœ¬: é¡å¤–1.5å€ (å¦‚æœä¸æ˜¯å›°é›£æ¨£æœ¬)
        weights[small_value_mask & ~difficult_mask] *= 1.5
        
        # å¦‚æœæ—¢æ˜¯å›°é›£æ¨£æœ¬åˆæ˜¯å°å€¼: ç¸½å…±4.5å€
        weights[difficult_mask & small_value_mask] *= 1.5
        
        print(f"  ç­–ç•¥: çµ„åˆ (Type 3å›°é›£æ¨£æœ¬ + å°å€¼)")
        print(f"  Type 3å›°é›£: {difficult_mask.sum()} ({difficult_mask.sum()/len(X)*100:.2f}%)")
        print(f"  å°å€¼: {small_value_mask.sum()} ({small_value_mask.sum()/len(X)*100:.2f}%)")
        print(f"  é‡ç–Š: {(difficult_mask & small_value_mask).sum()}")
        print(f"  æ¬Šé‡å€æ•¸: Type 3å›°é›£ {weight_factor}x, å°å€¼ 1.5x, é‡ç–Š {weight_factor*1.5}x\n")
    
    return weights


# ==========================================
# è¨“ç·´å‡½æ•¸ (åŠ å…¥æ¨£æœ¬åŠ æ¬Š)
# ==========================================

def train_dkl_with_weighting(X_train, y_train, config=None):
    """è¨“ç·´åŠ æ¬Šç‰ˆDKL"""
    
    if config is None:
        config = {
            'hidden_dims': [64, 32, 16],
            'feature_dim': 8,
            'dropout': 0.1,
            'lr': 0.01,
            'epochs': 500,
            'patience': 50,
            'mape_weight': 0.1,
            'sample_weight_strategy': 'difficult_samples',  # æ¬Šé‡ç­–ç•¥
            'sample_weight_factor': 3.0,                     # æ¬Šé‡å€æ•¸
        }
    
    print("="*60)
    print("è¨“ç·´æ¨£æœ¬åŠ æ¬Šç‰ˆDKL")
    print("="*60 + "\n")
    
    print("é…ç½®:")
    for k, v in config.items():
        print(f"  {k}: {v}")
    print()
    
    # è¨ˆç®—æ¨£æœ¬æ¬Šé‡ (åœ¨æ¨™æº–åŒ–å‰!)
    print("è¨ˆç®—æ¨£æœ¬æ¬Šé‡:")
    sample_weights_np = compute_sample_weights(
        X_train, 
        y_train,
        strategy=config['sample_weight_strategy'],
        weight_factor=config['sample_weight_factor']
    )
    
    # æ¨™æº–åŒ–
    scaler_x = StandardScaler()
    scaler_y = StandardScaler()
    
    X_train_scaled = scaler_x.fit_transform(X_train)
    y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
    
    train_x = torch.from_numpy(X_train_scaled).to(device)
    train_y = torch.from_numpy(y_train_scaled).to(device)
    sample_weights = torch.from_numpy(sample_weights_np).to(device)
    
    # å»ºç«‹æ¨¡å‹
    feature_extractor = DnnFeatureExtractor(
        input_dim=train_x.shape[1],
        hidden_dims=config['hidden_dims'],
        output_dim=config['feature_dim'],
        dropout=config['dropout']
    ).to(device)
    
    likelihood = gpytorch.likelihoods.GaussianLikelihood().to(device)
    model = GPRegressionModel(train_x, train_y, likelihood, feature_extractor).to(device)
    
    # å„ªåŒ–å™¨
    optimizer = optim.Adam([
        {'params': model.feature_extractor.parameters(), 'lr': config['lr'], 'weight_decay': 1e-4},
        {'params': model.covar_module.parameters()},
        {'params': model.mean_module.parameters()},
        {'params': model.likelihood.parameters()},
    ], lr=config['lr'])
    
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=2)
    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)
    
    # è¨“ç·´
    model.train()
    likelihood.train()
    
    best_loss = float('inf')
    patience_counter = 0
    
    print("é–‹å§‹è¨“ç·´...")
    
    for epoch in range(config['epochs']):
        optimizer.zero_grad()
        
        output = model(train_x)
        gp_loss = -mll(output, train_y)
        
        # åŠ æ¬ŠMAPE loss
        mape = weighted_mape_loss(output.mean, train_y, sample_weights)
        
        total_loss = gp_loss + config['mape_weight'] * mape
        
        total_loss.backward()
        optimizer.step()
        scheduler.step()
        
        current_loss = total_loss.item()
        
        if current_loss < best_loss:
            best_loss = current_loss
            patience_counter = 0
            best_state = {
                'model': model.state_dict(),
                'likelihood': likelihood.state_dict(),
            }
        else:
            patience_counter += 1
        
        if patience_counter >= config['patience']:
            print(f"æ—©åœ (Epoch {epoch+1}), Best Loss: {best_loss:.4f}")
            break
        
        if (epoch + 1) % 100 == 0:
            print(f"Epoch {epoch+1}: GP Loss={gp_loss.item():.4f}, "
                  f"MAPE={mape.item():.2f}%, Total={total_loss.item():.4f}")
    
    # è¼‰å…¥æœ€ä½³æ¨¡å‹
    model.load_state_dict(best_state['model'])
    likelihood.load_state_dict(best_state['likelihood'])
    
    print(f"è¨“ç·´å®Œæˆ (Final Loss: {best_loss:.4f})\n")
    
    return {
        'model': model,
        'likelihood': likelihood,
        'scaler_x': scaler_x,
        'scaler_y': scaler_y,
        'config': config,
        'sample_weights': sample_weights_np
    }


def evaluate_model(model_dict, X_test, y_test, dataset_name="Test"):
    """è©•ä¼°æ¨¡å‹"""
    
    model = model_dict['model']
    likelihood = model_dict['likelihood']
    scaler_x = model_dict['scaler_x']
    scaler_y = model_dict['scaler_y']
    
    # é æ¸¬
    model.eval()
    likelihood.eval()
    
    X_test_scaled = scaler_x.transform(X_test)
    test_x = torch.from_numpy(X_test_scaled).to(device)
    
    with torch.no_grad(), gpytorch.settings.fast_pred_var():
        pred_dist = likelihood(model(test_x))
        y_pred_scaled = pred_dist.mean.cpu().numpy()
        y_std_scaled = pred_dist.stddev.cpu().numpy()
    
    # åæ¨™æº–åŒ–
    y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
    y_std = y_std_scaled * scaler_y.scale_[0]
    
    # è¨ˆç®—æŒ‡æ¨™
    relative_errors = np.abs((y_test - y_pred) / y_test) * 100
    
    mape = np.mean(relative_errors)
    mae = mean_absolute_error(y_test, y_pred)
    max_err = np.max(relative_errors)
    
    outliers_20 = np.sum(relative_errors > 20)
    outliers_15 = np.sum(relative_errors > 15)
    outliers_10 = np.sum(relative_errors > 10)
    
    ci_lower = y_pred - 1.96 * y_std
    ci_upper = y_pred + 1.96 * y_std
    ci_coverage = np.mean((y_test >= ci_lower) & (y_test <= ci_upper)) * 100
    ci_width = np.mean(ci_upper - ci_lower)
    
    print(f"="*60)
    print(f"{dataset_name} è©•ä¼°çµæœ")
    print(f"="*60)
    print(f"æ¨£æœ¬æ•¸: {len(y_test)}")
    print(f"\næº–ç¢ºåº¦:")
    print(f"  MAPE:      {mape:.2f}%")
    print(f"  MAE:       {mae:.4f}")
    print(f"  Max Error: {max_err:.2f}%")
    print(f"\nç•°å¸¸é»:")
    print(f"  >20%: {outliers_20}/{len(y_test)} ({outliers_20/len(y_test)*100:.2f}%)")
    print(f"  >15%: {outliers_15}/{len(y_test)} ({outliers_15/len(y_test)*100:.2f}%)")
    print(f"  >10%: {outliers_10}/{len(y_test)} ({outliers_10/len(y_test)*100:.2f}%)")
    
    # åˆ†æç•°å¸¸é»é¡å‹
    if outliers_20 > 0:
        print(f"\nç•°å¸¸é»åˆ†æ:")
        outlier_mask = relative_errors > 20
        outlier_types = X_test[outlier_mask, 0]
        
        for tim_type in [1, 2, 3]:
            n_type = np.sum(outlier_types == tim_type)
            if n_type > 0:
                print(f"  Type {tim_type}: {n_type}å€‹ç•°å¸¸é»")
                
                # Type 3çš„è©³ç´°åˆ†æ
                if tim_type == 3:
                    type3_outliers = X_test[outlier_mask & (X_test[:, 0] == 3)]
                    if len(type3_outliers) > 0:
                        print(f"    Coverage 0.8: {np.sum(type3_outliers[:, 2] == 0.8)}å€‹")
                        print(f"    Coverage 1.0: {np.sum(type3_outliers[:, 2] == 1.0)}å€‹")
                        print(f"    THICKNESS>=220: {np.sum(type3_outliers[:, 1] >= 220)}å€‹")
    
    print(f"\nä¸ç¢ºå®šæ€§:")
    print(f"  CI Coverage: {ci_coverage:.2f}%")
    print(f"  CI Width:    {ci_width:.4f}")
    print(f"="*60 + "\n")
    
    return {
        'mape': mape,
        'mae': mae,
        'max_error': max_err,
        'outliers_20': outliers_20,
        'outliers_15': outliers_15,
        'outliers_10': outliers_10,
        'ci_coverage': ci_coverage,
        'ci_width': ci_width,
        'predictions': y_pred,
        'std': y_std,
        'relative_errors': relative_errors
    }


# ==========================================
# ä¸»å‡½æ•¸
# ==========================================

def main_weighted_experiment():
    """æ¨£æœ¬åŠ æ¬Šå¯¦é©—ä¸»æµç¨‹"""
    
    print("\n" + "="*60)
    print("Phase 2B: æ¨£æœ¬åŠ æ¬Š (Sample Weighting)")
    print("="*60 + "\n")
    
    feature_cols = ['TIM_TYPE', 'TIM_THICKNESS', 'TIM_COVERAGE']
    target_col = 'Theta.JC'
    
    # é…ç½® - å¯ä»¥èª¿æ•´ç­–ç•¥å’Œæ¬Šé‡å€æ•¸
    config = {
        'hidden_dims': [64, 32, 16],
        'feature_dim': 8,
        'dropout': 0.1,
        'lr': 0.01,
        'epochs': 500,
        'patience': 50,
        'mape_weight': 0.1,
        'sample_weight_strategy': 'difficult_samples',  # è©¦è©¦é€™å€‹ï¼
        'sample_weight_factor': 3.0,                     # 3å€æ¬Šé‡
    }
    
    results_summary = []
    
    # ==========================================
    # Above
    # ==========================================
    print("\nğŸ”µ Above 50% Coverage\n")
    
    train_above = pd.read_excel('data/train/Above.xlsx')
    test_above = pd.read_excel('data/test/Above.xlsx')
    
    train_above_clean = train_above.groupby(feature_cols, as_index=False).agg({
        target_col: 'mean'
    })
    
    print(f"è¨“ç·´é›†: {len(train_above_clean)} ç­†")
    print(f"æ¸¬è©¦é›†: {len(test_above)} ç­†\n")
    
    X_train_above = train_above_clean[feature_cols].values
    y_train_above = train_above_clean[target_col].values
    
    X_test_above = test_above[feature_cols].values
    y_test_above = test_above[target_col].values
    
    # è¨“ç·´åŠ æ¬Šç‰ˆ
    model_above = train_dkl_with_weighting(X_train_above, y_train_above, config)
    
    # è©•ä¼°
    results_above = evaluate_model(model_above, X_test_above, y_test_above, "Above")
    
    # ä¿å­˜
    test_above_pred = test_above.copy()
    test_above_pred['Prediction'] = results_above['predictions']
    test_above_pred['Std'] = results_above['std']
    test_above_pred['Error%'] = results_above['relative_errors']
    test_above_pred.to_csv('phase2b_above_predictions.csv', index=False)
    
    results_summary.append({
        'Dataset': 'Above',
        'Method': 'Sample Weighting',
        'MAPE': results_above['mape'],
        'Outliers_20': f"{results_above['outliers_20']}/{len(y_test_above)}",
        'Max_Error': results_above['max_error']
    })
    
    # ==========================================
    # Below
    # ==========================================
    print("\nğŸŸ¢ Below 50% Coverage\n")
    
    train_below = pd.read_excel('data/train/Below.xlsx')
    test_below = pd.read_excel('data/test/Below.xlsx')
    
    train_below_clean = train_below.groupby(feature_cols, as_index=False).agg({
        target_col: 'mean'
    })
    
    print(f"è¨“ç·´é›†: {len(train_below_clean)} ç­†")
    print(f"æ¸¬è©¦é›†: {len(test_below)} ç­†\n")
    
    X_train_below = train_below_clean[feature_cols].values
    y_train_below = train_below_clean[target_col].values
    
    X_test_below = test_below[feature_cols].values
    y_test_below = test_below[target_col].values
    
    # è¨“ç·´åŠ æ¬Šç‰ˆ
    model_below = train_dkl_with_weighting(X_train_below, y_train_below, config)
    
    # è©•ä¼°
    results_below = evaluate_model(model_below, X_test_below, y_test_below, "Below")
    
    # ä¿å­˜
    test_below_pred = test_below.copy()
    test_below_pred['Prediction'] = results_below['predictions']
    test_below_pred['Std'] = results_below['std']
    test_below_pred['Error%'] = results_below['relative_errors']
    test_below_pred.to_csv('phase2b_below_predictions.csv', index=False)
    
    results_summary.append({
        'Dataset': 'Below',
        'Method': 'Sample Weighting',
        'MAPE': results_below['mape'],
        'Outliers_20': f"{results_below['outliers_20']}/{len(y_test_below)}",
        'Max_Error': results_below['max_error']
    })
    
    # ==========================================
    # æ¯”è¼ƒ
    # ==========================================
    print("\n" + "="*60)
    print("ğŸ“Š çµæœæ¯”è¼ƒ")
    print("="*60 + "\n")
    
    print("Baseline (çµ„å“¡):")
    print("  Above: MAPE=8.89%, ç•°å¸¸é»=16/138 (11.59%)")
    print("  Below: MAPE=3.76%, ç•°å¸¸é»=0/48 (0.00%)")
    
    print("\nPhase 1 (MAPE Loss):")
    print("  Above: MAPE=8.63%, ç•°å¸¸é»=10/138 (7.25%)")
    print("  Below: MAPE=3.88%, ç•°å¸¸é»=0/48 (0.00%)")
    
    print("\nPhase 2A (Entity Embedding):")
    print("  Above: MAPE=8.83%, ç•°å¸¸é»=10/138 (7.25%)")
    print("  Below: MAPE=3.90%, ç•°å¸¸é»=0/48 (0.00%)")
    
    print("\nPhase 2B (æ¨£æœ¬åŠ æ¬Š):")
    print(f"  Above: MAPE={results_above['mape']:.2f}%, "
          f"ç•°å¸¸é»={results_above['outliers_20']}/{len(y_test_above)} "
          f"({results_above['outliers_20']/len(y_test_above)*100:.2f}%)")
    print(f"  Below: MAPE={results_below['mape']:.2f}%, "
          f"ç•°å¸¸é»={results_below['outliers_20']}/{len(y_test_below)} "
          f"({results_below['outliers_20']/len(y_test_below)*100:.2f}%)")
    
    # è¨ˆç®—æ”¹é€²
    improvement = 10 - results_above['outliers_20']
    mape_improvement = 8.63 - results_above['mape']
    
    if improvement > 0:
        print(f"\nâœ… ç›¸æ¯”Phase 1æ”¹é€²:")
        print(f"  ç•°å¸¸é»: -{improvement} ({improvement/10*100:.1f}% reduction)")
        print(f"  MAPE: {mape_improvement:+.2f}%")
    elif improvement == 0:
        print(f"\nğŸ˜ èˆ‡Phase 1æŒå¹³")
    else:
        print(f"\nâš ï¸ ç›¸æ¯”Phase 1é€€æ­¥: +{-improvement}å€‹ç•°å¸¸é»")
    
    print(f"\n{'='*60}\n")
    
    # ä¿å­˜
    summary_df = pd.DataFrame(results_summary)
    summary_df.to_csv('phase2b_summary.csv', index=False)
    print("âœ“ çµæœå·²ä¿å­˜\n")
    
    return {
        'above': (model_above, results_above, test_above_pred),
        'below': (model_below, results_below, test_below_pred)
    }


if __name__ == "__main__":
    results = main_weighted_experiment()