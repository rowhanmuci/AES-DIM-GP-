"""
Phase 1: ç•°å¸¸é»æ·±åº¦åˆ†æ
ç›®æ¨™: æ‰¾å‡ºAboveè³‡æ–™é›†ä¸­16å€‹ç•°å¸¸é»çš„å…±åŒç‰¹å¾µ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

def load_data():
    """è¼‰å…¥Aboveè³‡æ–™é›†"""
    print("="*60)
    print("è¼‰å…¥Aboveè³‡æ–™é›†...")
    print("="*60 + "\n")
    
    train_above = pd.read_excel('D:/NSYSU/Aes/data1/FOCoS_PKG_Type4_Cavity_TIM_50%_Above_Training_Data.xlsx')
    test_above = pd.read_excel('D:/NSYSU/Aes/data1/FOCoS_PKG_Type4_Cavity_TIM_50%_Above_Test_Data.xlsx')

    print(f"è¨“ç·´é›†: {len(train_above)} ç­†")
    print(f"æ¸¬è©¦é›†: {len(test_above)} ç­†\n")
    
    return train_above, test_above


def analyze_outlier_features(outlier_df, normal_df, feature_cols):
    """æ¯”è¼ƒç•°å¸¸é»å’Œæ­£å¸¸é»çš„ç‰¹å¾µåˆ†å¸ƒ"""
    
    print("\n" + "="*60)
    print("ç‰¹å¾µåˆ†å¸ƒå°æ¯”: ç•°å¸¸é» vs æ­£å¸¸é»")
    print("="*60 + "\n")
    
    analysis_results = {}
    
    for feat in feature_cols:
        print(f"ğŸ“Š {feat}:")
        print("-" * 40)
        
        outlier_vals = outlier_df[feat]
        normal_vals = normal_df[feat]
        
        stats = {
            'outlier': {
                'mean': outlier_vals.mean(),
                'std': outlier_vals.std(),
                'min': outlier_vals.min(),
                'max': outlier_vals.max(),
                'median': outlier_vals.median(),
            },
            'normal': {
                'mean': normal_vals.mean(),
                'std': normal_vals.std(),
                'min': normal_vals.min(),
                'max': normal_vals.max(),
                'median': normal_vals.median(),
            }
        }
        
        print(f"  ç•°å¸¸é» - å‡å€¼: {stats['outlier']['mean']:.4f}, "
              f"æ¨™æº–å·®: {stats['outlier']['std']:.4f}, "
              f"ç¯„åœ: [{stats['outlier']['min']:.4f}, {stats['outlier']['max']:.4f}]")
        
        print(f"  æ­£å¸¸é» - å‡å€¼: {stats['normal']['mean']:.4f}, "
              f"æ¨™æº–å·®: {stats['normal']['std']:.4f}, "
              f"ç¯„åœ: [{stats['normal']['min']:.4f}, {stats['normal']['max']:.4f}]")
        
        # å·®ç•°åˆ†æ
        mean_diff = abs(stats['outlier']['mean'] - stats['normal']['mean'])
        mean_diff_pct = mean_diff / stats['normal']['mean'] * 100
        
        print(f"  âš¡ å‡å€¼å·®ç•°: {mean_diff:.4f} ({mean_diff_pct:.2f}%)")
        
        analysis_results[feat] = stats
        print()
    
    return analysis_results


def check_outlier_patterns(outlier_df):
    """æª¢æŸ¥ç•°å¸¸é»çš„æ¨¡å¼"""
    
    print("\n" + "="*60)
    print("ç•°å¸¸é»æ¨¡å¼åˆ†æ")
    print("="*60 + "\n")
    
    # TIM_TYPEåˆ†å¸ƒ
    print("ğŸ“Œ TIM_TYPEåˆ†å¸ƒ:")
    print("-" * 40)
    type_dist = outlier_df['TIM_TYPE'].value_counts().sort_index()
    for tim_type, count in type_dist.items():
        percentage = count / len(outlier_df) * 100
        print(f"  Type {tim_type}: {count} ç­† ({percentage:.1f}%)")
    
    # THICKNESSç¯„åœ
    print("\nğŸ“Œ TIM_THICKNESSåˆ†å¸ƒ:")
    print("-" * 40)
    thickness = outlier_df['TIM_THICKNESS']
    
    bins = [(0, 0.1, 'Low (<0.1)'),
            (0.1, 0.2, 'Mid (0.1-0.2)'),
            (0.2, float('inf'), 'High (>0.2)')]
    
    for low, high, label in bins:
        if high == float('inf'):
            count = len(thickness[thickness >= low])
        else:
            count = len(thickness[(thickness >= low) & (thickness < high)])
        percentage = count / len(thickness) * 100
        print(f"  {label}: {count} ç­† ({percentage:.1f}%)")
    
    # COVERAGEç¯„åœ
    print("\nğŸ“Œ TIM_COVERAGEåˆ†å¸ƒ:")
    print("-" * 40)
    coverage = outlier_df['TIM_COVERAGE']
    
    bins = [(0, 30, 'Low (<30%)'),
            (30, 70, 'Mid (30-70%)'),
            (70, 100, 'High (>70%)')]
    
    for low, high, label in bins:
        count = len(coverage[(coverage >= low) & (coverage < high)])
        percentage = count / len(coverage) * 100
        print(f"  {label}: {count} ç­† ({percentage:.1f}%)")
    
    # Theta.JCåˆ†å¸ƒ
    print("\nğŸ“Œ Theta.JC (çœŸå¯¦å€¼)åˆ†å¸ƒ:")
    print("-" * 40)
    theta = outlier_df['Theta.JC']
    print(f"  å‡å€¼: {theta.mean():.4f}")
    print(f"  æ¨™æº–å·®: {theta.std():.4f}")
    print(f"  ç¯„åœ: [{theta.min():.4f}, {theta.max():.4f}]")
    
    return {
        'type_dist': type_dist,
        'thickness_stats': thickness.describe(),
        'coverage_stats': coverage.describe(),
        'theta_stats': theta.describe()
    }


def check_training_data_coverage(train_df, outlier_df, feature_cols):
    """æª¢æŸ¥è¨“ç·´é›†æ˜¯å¦è¦†è“‹ç•°å¸¸é»çš„ç‰¹å¾µç©ºé–“"""
    
    print("\n" + "="*60)
    print("è¨“ç·´é›†è¦†è“‹åº¦åˆ†æ")
    print("="*60 + "\n")
    
    for feat in feature_cols:
        outlier_min = outlier_df[feat].min()
        outlier_max = outlier_df[feat].max()
        
        train_min = train_df[feat].min()
        train_max = train_df[feat].max()
        
        print(f"ğŸ“Š {feat}:")
        print(f"  è¨“ç·´é›†ç¯„åœ: [{train_min:.4f}, {train_max:.4f}]")
        print(f"  ç•°å¸¸é»ç¯„åœ: [{outlier_min:.4f}, {outlier_max:.4f}]")
        
        # æª¢æŸ¥æ˜¯å¦è¶…å‡ºè¨“ç·´ç¯„åœ
        if outlier_min < train_min or outlier_max > train_max:
            print(f"  âš ï¸  ç•°å¸¸é»è¶…å‡ºè¨“ç·´ç¯„åœï¼")
            if outlier_min < train_min:
                print(f"     - æœ€å°å€¼è¶…å‡º: {outlier_min:.4f} < {train_min:.4f}")
            if outlier_max > train_max:
                print(f"     - æœ€å¤§å€¼è¶…å‡º: {outlier_max:.4f} > {train_max:.4f}")
        else:
            print(f"  âœ… ç•°å¸¸é»åœ¨è¨“ç·´ç¯„åœå…§")
        print()


def find_similar_training_samples(train_df, outlier_sample, feature_cols, top_k=5):
    """æ‰¾å‡ºè¨“ç·´é›†ä¸­èˆ‡ç•°å¸¸é»æœ€ç›¸ä¼¼çš„æ¨£æœ¬"""
    
    # æ¨™æº–åŒ–
    scaler = StandardScaler()
    train_features = scaler.fit_transform(train_df[feature_cols])
    outlier_features = scaler.transform(outlier_sample[feature_cols].values.reshape(1, -1))
    
    # è¨ˆç®—æ­å¼è·é›¢
    distances = np.linalg.norm(train_features - outlier_features, axis=1)
    
    # æ‰¾æœ€è¿‘çš„kå€‹
    nearest_indices = np.argsort(distances)[:top_k]
    
    similar_samples = train_df.iloc[nearest_indices].copy()
    similar_samples['distance'] = distances[nearest_indices]
    
    return similar_samples


def analyze_each_outlier(train_df, test_df_with_predictions, feature_cols, threshold=20):
    """é€ä¸€åˆ†ææ¯å€‹ç•°å¸¸é»"""
    
    print("\n" + "="*60)
    print("é€ä¸€åˆ†æç•°å¸¸é» (å‰10ç­†è©³ç´°)")
    print("="*60 + "\n")
    
    # ç¯©é¸ç•°å¸¸é»
    outliers = test_df_with_predictions[test_df_with_predictions['Error%'] > threshold].copy()
    outliers = outliers.sort_values('Error%', ascending=False)
    
    for idx, (i, row) in enumerate(outliers.head(10).iterrows()):
        print(f"\n{'â”€'*60}")
        print(f"ç•°å¸¸é» #{idx+1} (æ¸¬è©¦é›†ç¬¬ {i} ç­†)")
        print(f"{'â”€'*60}")
        
        print(f"ç‰¹å¾µå€¼:")
        for feat in feature_cols:
            print(f"  {feat}: {row[feat]:.4f}")
        
        print(f"\né æ¸¬çµæœ:")
        print(f"  çœŸå¯¦å€¼ (Theta.JC): {row['Theta.JC']:.4f}")
        print(f"  é æ¸¬å€¼: {row['Prediction']:.4f}")
        print(f"  èª¤å·®: {row['Error%']:.2f}%")
        
        # æ‰¾ç›¸ä¼¼çš„è¨“ç·´æ¨£æœ¬
        print(f"\nè¨“ç·´é›†ä¸­æœ€ç›¸ä¼¼çš„5å€‹æ¨£æœ¬:")
        similar = find_similar_training_samples(train_df, row, feature_cols, top_k=5)
        
        for j, (_, sim_row) in enumerate(similar.iterrows(), 1):
            print(f"\n  ç›¸ä¼¼æ¨£æœ¬ #{j} (è·é›¢={sim_row['distance']:.4f}):")
            for feat in feature_cols:
                print(f"    {feat}: {sim_row[feat]:.4f}")
            print(f"    Theta.JC: {sim_row['Theta.JC']:.4f}")
    
    print(f"\n{'â”€'*60}")
    print(f"å‰©é¤˜ {len(outliers) - 10} å€‹ç•°å¸¸é»æœªè©³ç´°é¡¯ç¤º")
    print(f"{'â”€'*60}\n")


def visualize_outliers(train_df, test_df_with_predictions, feature_cols, threshold=20):
    """è¦–è¦ºåŒ–ç•°å¸¸é»"""
    
    print("\nç”Ÿæˆè¦–è¦ºåŒ–åœ–è¡¨...")
    
    outliers = test_df_with_predictions[test_df_with_predictions['Error%'] > threshold]
    normals = test_df_with_predictions[test_df_with_predictions['Error%'] <= threshold]
    
    # å‰µå»ºåœ–è¡¨
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('Aboveè³‡æ–™é›† - ç•°å¸¸é»åˆ†æ', fontsize=16, fontweight='bold')
    
    # 1. Featureåˆ†å¸ƒå°æ¯”
    ax1 = axes[0, 0]
    x = np.arange(len(feature_cols))
    width = 0.35
    
    outlier_means = [outliers[f].mean() for f in feature_cols]
    normal_means = [normals[f].mean() for f in feature_cols]
    train_means = [train_df[f].mean() for f in feature_cols]
    
    ax1.bar(x - width, outlier_means, width, label='Outliers', color='red', alpha=0.7)
    ax1.bar(x, normal_means, width, label='Normals', color='green', alpha=0.7)
    ax1.bar(x + width, train_means, width, label='Training', color='blue', alpha=0.7)
    
    ax1.set_xlabel('Features')
    ax1.set_ylabel('Mean Value')
    ax1.set_title('ç‰¹å¾µå‡å€¼å°æ¯”')
    ax1.set_xticks(x)
    ax1.set_xticklabels(feature_cols, rotation=45, ha='right')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. TIM_TYPEåˆ†å¸ƒ
    ax2 = axes[0, 1]
    type_counts_outlier = outliers['TIM_TYPE'].value_counts()
    type_counts_normal = normals['TIM_TYPE'].value_counts()
    
    types = sorted(set(list(type_counts_outlier.index) + list(type_counts_normal.index)))
    outlier_vals = [type_counts_outlier.get(t, 0) for t in types]
    normal_vals = [type_counts_normal.get(t, 0) for t in types]
    
    x = np.arange(len(types))
    ax2.bar(x - width/2, outlier_vals, width, label='Outliers', color='red', alpha=0.7)
    ax2.bar(x + width/2, normal_vals, width, label='Normals', color='green', alpha=0.7)
    
    ax2.set_xlabel('TIM_TYPE')
    ax2.set_ylabel('Count')
    ax2.set_title('TIM_TYPEåˆ†å¸ƒå°æ¯”')
    ax2.set_xticks(x)
    ax2.set_xticklabels(types)
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. THICKNESS vs COVERAGEæ•£é»åœ–
    ax3 = axes[1, 0]
    ax3.scatter(normals['TIM_THICKNESS'], normals['TIM_COVERAGE'], 
               c='green', alpha=0.5, s=50, label='Normal')
    ax3.scatter(outliers['TIM_THICKNESS'], outliers['TIM_COVERAGE'], 
               c='red', alpha=0.7, s=100, marker='X', label='Outliers')
    ax3.scatter(train_df['TIM_THICKNESS'], train_df['TIM_COVERAGE'], 
               c='blue', alpha=0.1, s=10, label='Training')
    
    ax3.set_xlabel('TIM_THICKNESS')
    ax3.set_ylabel('TIM_COVERAGE')
    ax3.set_title('THICKNESS vs COVERAGEåˆ†å¸ƒ')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. èª¤å·®åˆ†å¸ƒç›´æ–¹åœ–
    ax4 = axes[1, 1]
    errors_all = test_df_with_predictions['Error%']
    
    ax4.hist(errors_all, bins=30, color='skyblue', alpha=0.7, edgecolor='black')
    ax4.axvline(x=threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold ({threshold}%)')
    ax4.axvline(x=errors_all.mean(), color='green', linestyle='--', linewidth=2, label=f'Mean ({errors_all.mean():.2f}%)')
    
    ax4.set_xlabel('Relative Error (%)')
    ax4.set_ylabel('Count')
    ax4.set_title('èª¤å·®åˆ†å¸ƒ')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('outlier_analysis.png', dpi=300, bbox_inches='tight')
    print("âœ“ åœ–è¡¨å·²å„²å­˜: outlier_analysis.png\n")
    plt.close()


def generate_summary_report(train_df, test_df_with_predictions, analysis_results, threshold=20):
    """ç”Ÿæˆç¸½çµå ±å‘Š"""
    
    outliers = test_df_with_predictions[test_df_with_predictions['Error%'] > threshold]
    
    print("\n" + "="*60)
    print("ğŸ¯ ç¸½çµå ±å‘Š")
    print("="*60 + "\n")
    
    print(f"ğŸ“Š åŸºæœ¬çµ±è¨ˆ:")
    print(f"  æ¸¬è©¦é›†ç¸½æ•¸: {len(test_df_with_predictions)} ç­†")
    print(f"  ç•°å¸¸é»æ•¸é‡: {len(outliers)} ç­† ({len(outliers)/len(test_df_with_predictions)*100:.2f}%)")
    print(f"  æ­£å¸¸é»æ•¸é‡: {len(test_df_with_predictions) - len(outliers)} ç­†")
    
    print(f"\nğŸ” ç•°å¸¸é»ç‰¹å¾µæ‘˜è¦:")
    
    # æª¢æŸ¥æ˜¯å¦é›†ä¸­åœ¨æŸäº›TIM_TYPE
    type_dist = outliers['TIM_TYPE'].value_counts()
    dominant_type = type_dist.idxmax() if len(type_dist) > 0 else None
    
    if dominant_type is not None:
        dominant_pct = type_dist[dominant_type] / len(outliers) * 100
        print(f"  ä¸»è¦TIM_TYPE: Type {dominant_type} ({type_dist[dominant_type]} ç­†, {dominant_pct:.1f}%)")
    
    # æª¢æŸ¥ç‰¹å¾µç¯„åœ
    print(f"\n  ç‰¹å¾µç¯„åœ:")
    for feat in ['TIM_THICKNESS', 'TIM_COVERAGE']:
        outlier_range = (outliers[feat].min(), outliers[feat].max())
        train_range = (train_df[feat].min(), train_df[feat].max())
        
        print(f"    {feat}:")
        print(f"      ç•°å¸¸é»: [{outlier_range[0]:.4f}, {outlier_range[1]:.4f}]")
        print(f"      è¨“ç·´é›†: [{train_range[0]:.4f}, {train_range[1]:.4f}]")
        
        # è¶…å‡ºè¨“ç·´ç¯„åœçš„ç•°å¸¸é»
        out_of_range = outliers[(outliers[feat] < train_range[0]) | (outliers[feat] > train_range[1])]
        if len(out_of_range) > 0:
            print(f"      âš ï¸  {len(out_of_range)} å€‹ç•°å¸¸é»è¶…å‡ºè¨“ç·´ç¯„åœ")
    
    print(f"\nğŸ’¡ æ”¹é€²å»ºè­°:")
    
    # æ ¹æ“šåˆ†æçµæœæä¾›å»ºè­°
    suggestions = []
    
    # å»ºè­°1: è¶…å‡ºè¨“ç·´ç¯„åœ
    for feat in ['TIM_THICKNESS', 'TIM_COVERAGE']:
        outlier_range = (outliers[feat].min(), outliers[feat].max())
        train_range = (train_df[feat].min(), train_df[feat].max())
        out_of_range = outliers[(outliers[feat] < train_range[0]) | (outliers[feat] > train_range[1])]
        
        if len(out_of_range) > 0:
            suggestions.append(f"å¢åŠ è¨“ç·´é›†åœ¨ {feat} æ¥µç«¯å€¼å€åŸŸçš„æ¨£æœ¬")
    
    # å»ºè­°2: TIM_TYPEä¸å¹³è¡¡
    if dominant_type is not None and dominant_pct > 50:
        suggestions.append(f"ç‰¹åˆ¥é—œæ³¨ TIM_TYPE={dominant_type} çš„é æ¸¬")
    
    # å»ºè­°3: è¶…åƒæ•¸
    suggestions.append("ä½¿ç”¨è¶…åƒæ•¸æœå°‹å„ªåŒ–æ¨¡å‹")
    suggestions.append("å˜—è©¦ä¸åŒçš„kernelçµ„åˆ")
    suggestions.append("èª¿æ•´feature_dim (æ½›åœ¨ç©ºé–“ç¶­åº¦)")
    
    # å»ºè­°4: æå¤±å‡½æ•¸
    suggestions.append("ä½¿ç”¨MAPE lossç›´æ¥å„ªåŒ–ç›¸å°èª¤å·®")
    suggestions.append("å°ç•°å¸¸å€åŸŸæ¨£æœ¬åŠ æ¬Š")
    
    for i, suggestion in enumerate(suggestions, 1):
        print(f"  {i}. {suggestion}")
    
    print(f"\n{'='*60}\n")


def main():
    """ä¸»å‡½æ•¸"""
    
    # è¼‰å…¥è³‡æ–™
    train_df, test_df = load_data()
    
    # ç‰¹å¾µæ¬„ä½
    feature_cols = ['TIM_TYPE', 'TIM_THICKNESS', 'TIM_COVERAGE']
    
    # âš ï¸ é€™è£¡éœ€è¦ä½ æä¾›çµ„å“¡æ¨¡å‹çš„é æ¸¬çµæœ
    # æˆ‘å…ˆå‰µå»ºä¸€å€‹æ¨¡æ“¬çš„é æ¸¬çµæœç¤ºç¯„
    # å¯¦éš›ä½¿ç”¨æ™‚ï¼Œè«‹æ›¿æ›æˆçœŸå¯¦çš„é æ¸¬
    
    # è¼‰å…¥é æ¸¬çµæœ
    test_df = pd.read_csv('phase1_predictions.csv')
    train_df = pd.read_excel('D:/NSYSU/Aes/data1/FOCoS_PKG_Type4_Cavity_TIM_50%_Above_Training_Data.xlsx')

    feature_cols = ['TIM_TYPE', 'TIM_THICKNESS', 'TIM_COVERAGE']

    # åˆ†é›¢ç•°å¸¸é»å’Œæ­£å¸¸é»
    outliers = test_df[test_df['Error%'] > 20]
    normals = test_df[test_df['Error%'] <= 20]

    # åŸ·è¡Œåˆ†æ
    print(f"\næ‰¾åˆ° {len(outliers)} å€‹ç•°å¸¸é»\n")

    # 1. ç‰¹å¾µåˆ†å¸ƒåˆ†æ
    results = analyze_outlier_features(outliers, normals, feature_cols)

    # 2. æ¨¡å¼åˆ†æ
    patterns = check_outlier_patterns(outliers)

    # 3. è¨“ç·´é›†è¦†è“‹åº¦
    check_training_data_coverage(train_df, outliers, feature_cols)

    # 4. é€ä¸€åˆ†æ
    analyze_each_outlier(train_df, test_df, feature_cols)

    # 5. è¦–è¦ºåŒ–
    visualize_outliers(train_df, test_df, feature_cols)

    # 6. ç¸½çµå ±å‘Š
    generate_summary_report(train_df, test_df, results)


if __name__ == "__main__":
    main()
