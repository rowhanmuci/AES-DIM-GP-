"""
Phase 2D Ensemble ç‰ˆæœ¬ - å¤šæ¨¡å‹é›†æˆ
æ•´åˆä»¥ä¸‹ç­–ç•¥:
1. æ¨™æº– DKL æ¨¡å‹
2. Type 3 å°ˆå®¶æ¨¡å‹
3. å°æ•¸ç©ºé–“æ¨¡å‹
4. é«˜æ¬Šé‡å›°é›£æ¨£æœ¬æ¨¡å‹
5. å‹•æ…‹åŠ æ¬Šé›†æˆ

ä½¿ç”¨æ–¹æ³•:
    python phase2d_ensemble.py --seed 2024 --n-models 3
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import gpytorch
from sklearn.preprocessing import StandardScaler
import warnings
import random
import os
import argparse
from tqdm import tqdm

warnings.filterwarnings('ignore')

torch.set_default_dtype(torch.float64)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def set_seed(seed):
    """è¨­ç½®éš¨æ©Ÿç¨®å­"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)


def clear_gpu_cache():
    """æ¸…ç©ºGPUå¿«å–"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()


# ==========================================
# ç‰¹å¾µå·¥ç¨‹ (å¾ phase2c å°å…¥)
# ==========================================

def create_enhanced_features(X):
    """å¢å¼·ç‰¹å¾µ"""
    tim_type = X[:, 0:1]
    thickness = X[:, 1:2]
    coverage = X[:, 2:3]
    
    features = [X]
    
    # Coverage éç·šæ€§
    features.append(coverage ** 2)
    features.append(coverage ** 3)
    features.append(np.sqrt(coverage))
    
    # Coverage è‡¨ç•Œå€¼æŒ‡ç¤ºå™¨
    features.append((np.abs(coverage - 0.8) < 0.1).astype(float))
    features.append((np.abs(coverage - 1.0) < 0.1).astype(float))
    features.append((coverage >= 0.75).astype(float))
    features.append((coverage >= 0.9).astype(float))
    
    # äº¤äº’ä½œç”¨
    features.append(thickness * coverage)
    features.append(thickness * coverage ** 2)
    features.append(thickness ** 2 * coverage)
    features.append(thickness / (1.01 - coverage + 1e-8))
    
    # å°æ•¸ç‰¹å¾µ
    features.append(np.log(thickness + 1))
    features.append(np.log(coverage + 0.01))
    features.append(np.exp(coverage))
    
    # Type-specific
    features.append(tim_type * thickness)
    features.append(tim_type * coverage)
    features.append(tim_type * thickness * coverage)
    
    return np.hstack(features)


def compute_advanced_weights(X, y=None, weight_config=None):
    """é€²éšæ¨£æœ¬åŠ æ¬Š"""
    if weight_config is None:
        weight_config = {
            'type3_base': 2.0,
            'high_coverage': 5.0,
            'small_value': 3.0,
        }
    
    weights = np.ones(len(X))
    
    tim_type = X[:, 0]
    coverage = X[:, 2]
    
    # Type 3 åŸºç¤æ¬Šé‡
    type3_mask = tim_type == 3
    weights[type3_mask] *= weight_config['type3_base']
    
    # é«˜ Coverage å€åŸŸ
    high_cov_mask = (
        ((coverage >= 0.75) & (coverage <= 0.85)) |
        (coverage >= 0.95)
    )
    weights[type3_mask & high_cov_mask] *= weight_config['high_coverage']
    
    # æ¥µå°çœŸå¯¦å€¼
    if y is not None:
        small_value_mask = y < 0.03
        weights[small_value_mask] *= weight_config['small_value']
    
    return weights


# ==========================================
# æ¨¡å‹å®šç¾©
# ==========================================

class DnnFeatureExtractor(nn.Module):
    """æ·±åº¦ç¥ç¶“ç¶²è·¯ç‰¹å¾µæå–å™¨"""
    
    def __init__(self, input_dim, hidden_dims=[128, 64, 32], output_dim=16, dropout=0.2):
        super().__init__()
        
        layers = []
        prev_dim = input_dim
        
        for h_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, h_dim),
                nn.BatchNorm1d(h_dim),
                nn.ReLU(),
                nn.Dropout(dropout)
            ])
            prev_dim = h_dim
        
        layers.append(nn.Linear(prev_dim, output_dim))
        
        self.network = nn.Sequential(*layers)
        self.output_dim = output_dim
    
    def forward(self, x):
        return self.network(x)


class GPRegressionModel(gpytorch.models.ExactGP):
    """é«˜æ–¯éç¨‹å›æ­¸æ¨¡å‹"""
    
    def __init__(self, train_x, train_y, likelihood, feature_extractor):
        super().__init__(train_x, train_y, likelihood)
        
        self.feature_extractor = feature_extractor
        self.mean_module = gpytorch.means.ConstantMean()
        
        self.covar_module = gpytorch.kernels.ScaleKernel(
            gpytorch.kernels.RBFKernel(ard_num_dims=feature_extractor.output_dim) +
            gpytorch.kernels.MaternKernel(nu=2.5, ard_num_dims=feature_extractor.output_dim) +
            gpytorch.kernels.LinearKernel()
        )
    
    def forward(self, x):
        projected_x = self.feature_extractor(x)
        mean_x = self.mean_module(projected_x)
        covar_x = self.covar_module(projected_x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)


# ==========================================
# æå¤±å‡½æ•¸
# ==========================================

def weighted_mape_loss(y_pred, y_true, weights, epsilon=1e-8):
    """åŠ æ¬ŠMAPEæå¤±"""
    mape_per_sample = torch.abs((y_true - y_pred) / 
                                (torch.abs(y_true) + epsilon)) * 100
    weighted_mape = torch.sum(mape_per_sample * weights) / torch.sum(weights)
    return weighted_mape


# ==========================================
# è¨“ç·´å‡½æ•¸
# ==========================================

def train_single_model(X_train, y_train, config, sample_weights=None, 
                      model_type='standard', verbose=False):
    """
    è¨“ç·´å–®å€‹æ¨¡å‹
    
    Args:
        model_type: 'standard', 'type3_specialist', 'log_space', 'high_weight'
    """
    # æ ¹æ“šæ¨¡å‹é¡å‹èª¿æ•´è³‡æ–™å’Œæ¬Šé‡
    if model_type == 'type3_specialist':
        # Type 3 å°ˆå®¶æ¨¡å‹ - åªç”¨ Type 3 è³‡æ–™
        type3_mask = X_train[:, 0] == 3
        X_train_use = X_train[type3_mask]
        y_train_use = y_train[type3_mask]
        if sample_weights is not None:
            sample_weights = sample_weights[type3_mask]
        if verbose:
            print(f"  Type 3 å°ˆå®¶æ¨¡å‹: ä½¿ç”¨ {len(X_train_use)} ç­† Type 3 æ¨£æœ¬")
    
    elif model_type == 'log_space':
        # å°æ•¸ç©ºé–“æ¨¡å‹
        X_train_use = X_train
        y_train_use = np.log(y_train + 1e-6)  # è½‰æ›åˆ°å°æ•¸ç©ºé–“
        if verbose:
            print(f"  å°æ•¸ç©ºé–“æ¨¡å‹: åœ¨ log ç©ºé–“è¨“ç·´")
    
    elif model_type == 'high_weight':
        # è¶…é«˜æ¬Šé‡å›°é›£æ¨£æœ¬
        X_train_use = X_train
        y_train_use = y_train
        if sample_weights is None:
            sample_weights = compute_advanced_weights(X_train, y_train)
        sample_weights = sample_weights.copy()
        
        # é€²ä¸€æ­¥å¢åŠ å›°é›£æ¨£æœ¬æ¬Šé‡
        type3_mask = X_train[:, 0] == 3
        high_cov_mask = (X_train[:, 2] >= 0.75)
        sample_weights[type3_mask & high_cov_mask] *= 3.0
        
        if verbose:
            print(f"  é«˜æ¬Šé‡æ¨¡å‹: å›°é›£æ¨£æœ¬æ¬Šé‡ Ã— 15")
    
    else:  # standard
        X_train_use = X_train
        y_train_use = y_train
    
    # ç‰¹å¾µå¢å¼·
    X_train_enhanced = create_enhanced_features(X_train_use)
    
    # è¨ˆç®—æ¨£æœ¬æ¬Šé‡ (å¦‚æœé‚„æ²’æœ‰)
    if sample_weights is None:
        sample_weights = compute_advanced_weights(X_train_use, y_train_use)
    
    # æ¨™æº–åŒ–
    scaler_x = StandardScaler()
    scaler_y = StandardScaler()
    
    X_train_scaled = scaler_x.fit_transform(X_train_enhanced)
    y_train_scaled = scaler_y.fit_transform(y_train_use.reshape(-1, 1)).flatten()
    
    train_x = torch.from_numpy(X_train_scaled).to(device)
    train_y = torch.from_numpy(y_train_scaled).to(device)
    weights_tensor = torch.from_numpy(sample_weights).to(device)
    
    # å»ºç«‹æ¨¡å‹
    feature_extractor = DnnFeatureExtractor(
        input_dim=train_x.shape[1],
        hidden_dims=config['hidden_dims'],
        output_dim=config['feature_dim'],
        dropout=config['dropout']
    ).to(device)
    
    likelihood = gpytorch.likelihoods.GaussianLikelihood().to(device)
    model = GPRegressionModel(train_x, train_y, likelihood, feature_extractor).to(device)
    
    # å„ªåŒ–å™¨
    optimizer = optim.AdamW([
        {'params': model.feature_extractor.parameters(), 'lr': config['lr'], 'weight_decay': 1e-4},
        {'params': model.covar_module.parameters(), 'lr': config['lr']},
        {'params': model.mean_module.parameters(), 'lr': config['lr']},
        {'params': model.likelihood.parameters(), 'lr': config['lr']},
    ], lr=config['lr'])
    
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=2)
    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)
    
    # è¨“ç·´
    model.train()
    likelihood.train()
    
    best_loss = float('inf')
    patience_counter = 0
    
    for epoch in range(config['epochs']):
        optimizer.zero_grad()
        
        output = model(train_x)
        gp_loss = -mll(output, train_y)
        mape = weighted_mape_loss(output.mean, train_y, weights_tensor)
        total_loss = gp_loss + config['mape_weight'] * mape
        
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step()
        
        current_loss = total_loss.item()
        
        # Early stopping
        if current_loss < best_loss:
            best_loss = current_loss
            patience_counter = 0
            best_state = {
                'model': model.state_dict(),
                'likelihood': likelihood.state_dict(),
            }
        else:
            patience_counter += 1
        
        if patience_counter >= config['patience']:
            break
    
    # è¼‰å…¥æœ€ä½³æ¨¡å‹
    model.load_state_dict(best_state['model'])
    likelihood.load_state_dict(best_state['likelihood'])
    
    return {
        'model': model,
        'likelihood': likelihood,
        'scaler_x': scaler_x,
        'scaler_y': scaler_y,
        'model_type': model_type,
        'is_log_space': (model_type == 'log_space')
    }


def predict_single_model(model_dict, X_test):
    """å–®å€‹æ¨¡å‹é æ¸¬"""
    model = model_dict['model']
    likelihood = model_dict['likelihood']
    scaler_x = model_dict['scaler_x']
    scaler_y = model_dict['scaler_y']
    is_log_space = model_dict['is_log_space']
    
    model.eval()
    likelihood.eval()
    
    # ç‰¹å¾µå¢å¼·
    X_test_enhanced = create_enhanced_features(X_test)
    X_test_scaled = scaler_x.transform(X_test_enhanced)
    test_x = torch.from_numpy(X_test_scaled).to(device)
    
    with torch.no_grad(), gpytorch.settings.fast_pred_var():
        pred_dist = likelihood(model(test_x))
        y_pred_scaled = pred_dist.mean.cpu().numpy()
        y_std_scaled = pred_dist.stddev.cpu().numpy()
    
    # åæ¨™æº–åŒ–
    y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
    y_std = y_std_scaled * scaler_y.scale_[0]
    
    # å¦‚æœæ˜¯å°æ•¸ç©ºé–“ï¼Œè½‰å›åŸå§‹ç©ºé–“
    if is_log_space:
        y_pred = np.exp(y_pred) - 1e-6
        y_pred = np.maximum(y_pred, 0.001)  # ç¢ºä¿éè² 
    
    return y_pred, y_std


# ==========================================
# Ensemble é›†æˆ
# ==========================================

def train_ensemble(X_train, y_train, config, n_models=3, verbose=True):
    """
    è¨“ç·´å¤šæ¨¡å‹é›†æˆ
    
    Returns:
        models: æ¨¡å‹åˆ—è¡¨
    """
    models = []
    
    model_types = ['standard', 'type3_specialist', 'high_weight']
    if n_models > 3:
        model_types.extend(['log_space'] * (n_models - 3))
    
    print(f"\nè¨“ç·´ {n_models} å€‹å­æ¨¡å‹...")
    
    for i in range(n_models):
        model_type = model_types[i % len(model_types)]
        
        if verbose:
            print(f"\næ¨¡å‹ {i+1}/{n_models}: {model_type}")
        
        # æ¯å€‹æ¨¡å‹ä½¿ç”¨ä¸åŒç¨®å­
        set_seed(config['seed'] + i * 100)
        
        model_dict = train_single_model(
            X_train, y_train, config,
            model_type=model_type,
            verbose=verbose
        )
        
        models.append(model_dict)
        
        clear_gpu_cache()
    
    print(f"\nâœ“ {n_models} å€‹å­æ¨¡å‹è¨“ç·´å®Œæˆ")
    
    return models


def ensemble_predict(models, X_test):
    """
    é›†æˆé æ¸¬ - å‹•æ…‹åŠ æ¬Š
    
    Args:
        models: æ¨¡å‹åˆ—è¡¨
        X_test: æ¸¬è©¦ç‰¹å¾µ
    
    Returns:
        y_pred: é›†æˆé æ¸¬çµæœ
        y_std: ä¸ç¢ºå®šæ€§
    """
    n_models = len(models)
    all_preds = []
    all_stds = []
    
    # ç²å–æ‰€æœ‰æ¨¡å‹é æ¸¬
    for model_dict in models:
        y_pred, y_std = predict_single_model(model_dict, X_test)
        all_preds.append(y_pred)
        all_stds.append(y_std)
    
    all_preds = np.array(all_preds)  # (n_models, n_samples)
    all_stds = np.array(all_stds)
    
    # è¨ˆç®—å‹•æ…‹æ¬Šé‡
    weights = np.ones((len(X_test), n_models))
    
    # Type 3 + é«˜ coverage â†’ å¢åŠ å°ˆå®¶æ¨¡å‹æ¬Šé‡
    for i, model_dict in enumerate(models):
        if model_dict['model_type'] == 'type3_specialist':
            type3_high_cov = (X_test[:, 0] == 3) & (X_test[:, 2] >= 0.75)
            weights[type3_high_cov, i] *= 3.0
        
        elif model_dict['model_type'] == 'log_space':
            # å°æ•¸ç©ºé–“æ¨¡å‹é©åˆæ¥µå°å€¼
            type3_high_cov = (X_test[:, 0] == 3) & (X_test[:, 2] >= 0.75)
            weights[type3_high_cov, i] *= 2.0
    
    # æ­¸ä¸€åŒ–æ¬Šé‡
    weights = weights / weights.sum(axis=1, keepdims=True)
    
    # åŠ æ¬Šå¹³å‡
    y_pred_ensemble = np.sum(all_preds.T * weights, axis=1)
    
    # é›†æˆä¸ç¢ºå®šæ€§ (è€ƒæ…®æ¨¡å‹é–“å·®ç•°)
    y_std_ensemble = np.sqrt(
        np.mean(all_stds ** 2, axis=0) +  # å¹³å‡æ–¹å·®
        np.var(all_preds, axis=0)  # æ¨¡å‹é–“æ–¹å·®
    )
    
    return y_pred_ensemble, y_std_ensemble


# ==========================================
# è©•ä¼°å‡½æ•¸
# ==========================================

def evaluate_ensemble(models, X_test, y_test, verbose=True):
    """è©•ä¼°é›†æˆæ¨¡å‹"""
    
    # é›†æˆé æ¸¬
    y_pred, y_std = ensemble_predict(models, X_test)
    
    # è¨ˆç®—æŒ‡æ¨™
    relative_errors = np.abs((y_test - y_pred) / y_test) * 100
    
    mape = np.mean(relative_errors)
    mae = np.mean(np.abs(y_test - y_pred))
    max_error = np.max(relative_errors)
    
    outliers_20 = np.sum(relative_errors > 20)
    outliers_15 = np.sum(relative_errors > 15)
    outliers_10 = np.sum(relative_errors > 10)
    
    # Type 3 åˆ†æ
    type3_mask = X_test[:, 0] == 3
    if np.sum(type3_mask) > 0:
        type3_errors = relative_errors[type3_mask]
        type3_mape = np.mean(type3_errors)
        type3_outliers = np.sum(type3_errors > 20)
        
        # Coverage 0.8 åˆ†æ
        cov08_mask = type3_mask & (X_test[:, 2] == 0.8)
        if np.sum(cov08_mask) > 0:
            cov08_errors = relative_errors[cov08_mask]
            cov08_mape = np.mean(cov08_errors)
            cov08_outliers = np.sum(cov08_errors > 20)
        else:
            cov08_mape = 0
            cov08_outliers = 0
    else:
        type3_mape = 0
        type3_outliers = 0
        cov08_mape = 0
        cov08_outliers = 0
    
    if verbose:
        print(f"\n{'='*60}")
        print(f"Ensemble è©•ä¼°çµæœ ({len(models)} å€‹å­æ¨¡å‹)")
        print(f"{'='*60}")
        print(f"æ¨£æœ¬æ•¸: {len(y_test)}")
        print(f"\næº–ç¢ºåº¦:")
        print(f"  MAPE:      {mape:.2f}%")
        print(f"  MAE:       {mae:.4f}")
        print(f"  Max Error: {max_error:.2f}%")
        print(f"\nç•°å¸¸é»:")
        print(f"  >20%: {outliers_20}/{len(y_test)} ({outliers_20/len(y_test)*100:.2f}%)")
        print(f"  >15%: {outliers_15}/{len(y_test)} ({outliers_15/len(y_test)*100:.2f}%)")
        print(f"  >10%: {outliers_10}/{len(y_test)} ({outliers_10/len(y_test)*100:.2f}%)")
        
        if np.sum(type3_mask) > 0:
            print(f"\nType 3 è©³ç´°:")
            print(f"  æ¨£æœ¬æ•¸: {np.sum(type3_mask)}")
            print(f"  MAPE: {type3_mape:.2f}%")
            print(f"  ç•°å¸¸é»: {type3_outliers}/{np.sum(type3_mask)}")
            if np.sum(cov08_mask) > 0:
                print(f"\n  Coverage 0.8 å­é›†:")
                print(f"    MAPE: {cov08_mape:.2f}%")
                print(f"    ç•°å¸¸é»: {cov08_outliers}/{np.sum(cov08_mask)}")
        print(f"{'='*60}\n")
    
    results = {
        'mape': mape,
        'mae': mae,
        'max_error': max_error,
        'outliers_20': outliers_20,
        'outliers_15': outliers_15,
        'outliers_10': outliers_10,
        'type3_mape': type3_mape,
        'type3_outliers': type3_outliers,
        'cov08_mape': cov08_mape,
        'cov08_outliers': cov08_outliers,
        'predictions': y_pred,
        'std': y_std,
        'errors': relative_errors
    }
    
    return results


def save_predictions(X_test, y_test, results, filename):
    """ä¿å­˜é æ¸¬çµæœ"""
    df = pd.DataFrame({
        'TIM_TYPE': X_test[:, 0],
        'TIM_THICKNESS': X_test[:, 1],
        'TIM_COVERAGE': X_test[:, 2],
        'True': y_test,
        'Predicted': results['predictions'],
        'Error%': results['errors'],
        'Std': results['std']
    })
    
    df.to_csv(filename, index=False)
    print(f"âœ“ é æ¸¬çµæœå·²ä¿å­˜åˆ°: {filename}")


# ==========================================
# ä¸»å‡½æ•¸
# ==========================================

def main(seed=2024, n_models=3, verbose=True):
    """ä¸»è¨“ç·´æµç¨‹"""
    clear_gpu_cache()
    set_seed(seed)
    
    print(f"\nä½¿ç”¨è£ç½®: {device}\n")
    
    print("="*60)
    print(f"Phase 2D: Ensemble é›†æˆç‰ˆæœ¬ ({n_models} å€‹å­æ¨¡å‹)")
    print("="*60)
    
    # ç‰¹å¾µå’Œç›®æ¨™
    feature_cols = ['TIM_TYPE', 'TIM_THICKNESS', 'TIM_COVERAGE']
    target_col = 'Theta.JC'
    
    # é…ç½®
    config = {
        'hidden_dims': [128, 64, 32],
        'feature_dim': 16,
        'dropout': 0.2,
        'lr': 0.008,
        'epochs': 500,
        'patience': 50,
        'mape_weight': 0.15,
        'seed': seed,
    }
    
    if verbose:
        print(f"\né…ç½®:")
        for key, value in config.items():
            print(f"  {key}: {value}")
    
    # ==========================================
    # Above Dataset
    # ==========================================
    
    print(f"\n\n{'ğŸ”µ Above 50% Coverage (Ensemble)'}\n")
    
    # è¼‰å…¥è³‡æ–™
    train_above = pd.read_excel('data/train/Above.xlsx')
    test_above = pd.read_excel('data/test/Above.xlsx')
    
    # è¨“ç·´é›†æ¸…ç†
    train_above_clean = train_above.groupby(feature_cols, as_index=False).agg({
        target_col: 'mean'
    })
    
    print(f"è¨“ç·´é›†: {len(train_above_clean)} ç­†")
    print(f"æ¸¬è©¦é›†: {len(test_above)} ç­†")
    
    X_train_above = train_above_clean[feature_cols].values
    y_train_above = train_above_clean[target_col].values
    
    X_test_above = test_above[feature_cols].values
    y_test_above = test_above[target_col].values
    
    # è¨“ç·´ Ensemble
    models = train_ensemble(
        X_train_above, y_train_above, config,
        n_models=n_models, verbose=verbose
    )
    
    # è©•ä¼°
    results_above = evaluate_ensemble(
        models, X_test_above, y_test_above, verbose=verbose
    )
    
    # ä¿å­˜é æ¸¬çµæœ
    save_predictions(X_test_above, y_test_above, results_above, 
                     f'phase2d_ensemble_{n_models}models_seed{seed}_predictions.csv')
    
    # ==========================================
    # ç¸½çµ
    # ==========================================
    
    print("\n" + "="*60)
    print(f"æœ€çµ‚çµæœç¸½çµ (Phase 2D Ensemble - {n_models} å€‹å­æ¨¡å‹)")
    print("="*60)
    print(f"éš¨æ©Ÿç¨®å­: {seed}")
    print(f"\nAboveè³‡æ–™é›†:")
    print(f"  ç¸½é«” MAPE: {results_above['mape']:.2f}%")
    print(f"  ç•°å¸¸é» (>20%): {results_above['outliers_20']}/{len(y_test_above)} ({results_above['outliers_20']/len(y_test_above)*100:.2f}%)")
    print(f"\n  Type 3 MAPE: {results_above['type3_mape']:.2f}%")
    print(f"  Type 3 ç•°å¸¸é»: {results_above['type3_outliers']}")
    print(f"\n  Coverage 0.8 (Type 3) MAPE: {results_above['cov08_mape']:.2f}%")
    print(f"  Coverage 0.8 ç•°å¸¸é»: {results_above['cov08_outliers']}")
    
    print("\nEnsemble ç­–ç•¥:")
    print(f"  âœ“ {n_models} å€‹å­æ¨¡å‹å‹•æ…‹åŠ æ¬Šé›†æˆ")
    print("  âœ“ Type 3 å°ˆå®¶æ¨¡å‹")
    print("  âœ“ å°æ•¸ç©ºé–“æ¨¡å‹")
    print("  âœ“ é«˜æ¬Šé‡å›°é›£æ¨£æœ¬æ¨¡å‹")
    print("  âœ“ ä¸ç¢ºå®šæ€§é‡åŒ–")
    
    print("\n" + "="*60)
    print("âœ“ è¨“ç·´å®Œæˆï¼")
    print("="*60 + "\n")
    
    return results_above


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Phase 2D Ensemble ç‰ˆæœ¬')
    parser.add_argument('--seed', type=int, default=2024, help='éš¨æ©Ÿç¨®å­')
    parser.add_argument('--n-models', type=int, default=3, help='å­æ¨¡å‹æ•¸é‡ (å»ºè­° 3-5)')
    parser.add_argument('-v', '--verbose', action='store_true', help='è©³ç´°æ¨¡å¼')
    
    args = parser.parse_args()
    
    results = main(seed=args.seed, n_models=args.n_models, verbose=args.verbose)
    
    print("\nğŸ’¡ ä½¿ç”¨èªªæ˜:")
    print(f"  ç•¶å‰: {args.n_models} å€‹å­æ¨¡å‹é›†æˆ")
    print("  å»ºè­°å˜—è©¦: 3-5 å€‹å­æ¨¡å‹é”åˆ°æœ€ä½³æ•ˆæœ")
    print("  é æœŸæ”¹å–„: Type 3 ç•°å¸¸é» 5/18 â†’ 2-3/18")
    print("           Coverage 0.8 MAPE 26.92% â†’ <12%\n")
