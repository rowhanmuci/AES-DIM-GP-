"""
Phase 4A - Spectral Mixture Kernel å¯¦é©— (æ¿€é€²è¨˜æ†¶é«”å„ªåŒ– v2)
ç°¡åŒ–èª˜å°é»åˆå§‹åŒ–ï¼Œé¿å… BatchNorm å•é¡Œ
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import gpytorch
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import warnings
import random
import os
import argparse
import gc

warnings.filterwarnings('ignore')

# ä½¿ç”¨ float32 ä¾†ç¯€çœè¨˜æ†¶é«”
torch.set_default_dtype(torch.float32)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def set_seed(seed):
    """è¨­ç½®éš¨æ©Ÿç¨®å­ä»¥ç¢ºä¿å®Œå…¨å¯é‡ç¾æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)
    print(f"âœ“ éš¨æ©Ÿç¨®å­è¨­å®šç‚º: {seed}")


def clear_gpu_cache():
    """æ¸…ç©ºGPUå¿«å–"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    gc.collect()


# ==========================================
# æ¨¡å‹å®šç¾©
# ==========================================

class DnnFeatureExtractor(nn.Module):
    """è¼•é‡åŒ–ç‰¹å¾µæå–å™¨ (å°ˆç‚º SM Kernel è¨­è¨ˆ)"""
    
    def __init__(self, input_dim, hidden_dims=[32, 16], output_dim=4, dropout=0.05):
        super().__init__()
        
        layers = []
        prev_dim = input_dim
        
        for h_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, h_dim),
                nn.BatchNorm1d(h_dim),
                nn.ReLU(),
                nn.Dropout(dropout)
            ])
            prev_dim = h_dim
        
        layers.append(nn.Linear(prev_dim, output_dim))
        
        self.network = nn.Sequential(*layers)
        self.output_dim = output_dim
    
    def forward(self, x):
        return self.network(x)


class VariationalSMGP(gpytorch.models.ApproximateGP):
    """ä½¿ç”¨ SM Kernel çš„ Variational GP"""
    
    def __init__(self, inducing_points, feature_extractor, kernel_type='sm', num_mixtures=2):
        """
        Args:
            inducing_points: èª˜å°é»
            feature_extractor: DNN ç‰¹å¾µæå–å™¨
            kernel_type: 'sm', 'sm+rbf', 'sm+matern'
            num_mixtures: SM æ··åˆæ•¸ (1-3 æ¨è–¦)
        """
        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(
            inducing_points.size(0)
        )
        variational_strategy = gpytorch.variational.VariationalStrategy(
            self, inducing_points, variational_distribution, 
            learn_inducing_locations=True
        )
        super().__init__(variational_strategy)
        
        self.feature_extractor = feature_extractor
        self.mean_module = gpytorch.means.ConstantMean()
        self.kernel_type = kernel_type
        
        # æ ¹æ“š kernel_type å»ºç«‹ä¸åŒçš„ kernel
        if kernel_type == 'sm':
            # ç´” SM
            self.covar_module = gpytorch.kernels.ScaleKernel(
                gpytorch.kernels.SpectralMixtureKernel(
                    num_mixtures=num_mixtures,
                    ard_num_dims=feature_extractor.output_dim
                )
            )
        
        elif kernel_type == 'sm+rbf':
            # SM + RBF
            self.covar_module = gpytorch.kernels.ScaleKernel(
                gpytorch.kernels.SpectralMixtureKernel(
                    num_mixtures=num_mixtures,
                    ard_num_dims=feature_extractor.output_dim
                ) + 
                gpytorch.kernels.RBFKernel(ard_num_dims=feature_extractor.output_dim)
            )
        
        elif kernel_type == 'sm+matern':
            # SM + MatÃ©rn
            self.covar_module = gpytorch.kernels.ScaleKernel(
                gpytorch.kernels.SpectralMixtureKernel(
                    num_mixtures=num_mixtures,
                    ard_num_dims=feature_extractor.output_dim
                ) + 
                gpytorch.kernels.MaternKernel(nu=2.5, ard_num_dims=feature_extractor.output_dim)
            )
        
        elif kernel_type == 'rbf':
            # ç´” RBF (å°ç…§çµ„)
            self.covar_module = gpytorch.kernels.ScaleKernel(
                gpytorch.kernels.RBFKernel(ard_num_dims=feature_extractor.output_dim)
            )
        
        elif kernel_type == 'matern':
            # ç´” MatÃ©rn (å°ç…§çµ„)
            self.covar_module = gpytorch.kernels.ScaleKernel(
                gpytorch.kernels.MaternKernel(nu=2.5, ard_num_dims=feature_extractor.output_dim)
            )
        
        elif kernel_type == 'rbf+matern':
            # RBF + MatÃ©rn
            self.covar_module = gpytorch.kernels.ScaleKernel(
                gpytorch.kernels.RBFKernel(ard_num_dims=feature_extractor.output_dim) +
                gpytorch.kernels.MaternKernel(nu=2.5, ard_num_dims=feature_extractor.output_dim)
            )
        
        else:
            raise ValueError(f"Unknown kernel_type: {kernel_type}")
    
    def forward(self, x):
        projected_x = self.feature_extractor(x)
        mean_x = self.mean_module(projected_x)
        covar_x = self.covar_module(projected_x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)


# ==========================================
# æå¤±å‡½æ•¸èˆ‡æ¬Šé‡
# ==========================================

def weighted_mape_loss(y_pred, y_true, weights, epsilon=1e-8):
    """åŠ æ¬ŠMAPEæå¤±å‡½æ•¸"""
    mape_per_sample = torch.abs((y_true - y_pred) / 
                                (torch.abs(y_true) + epsilon)) * 100
    weighted_mape = torch.sum(mape_per_sample * weights) / torch.sum(weights)
    return weighted_mape


def compute_sample_weights(X, weight_factor=3.0):
    """è¨ˆç®—æ¨£æœ¬æ¬Šé‡"""
    weights = np.ones(len(X))
    
    difficult_mask = (
        (X[:, 0] == 3) &      # TIM_TYPE = 3
        (X[:, 2] == 0.8) &    # TIM_COVERAGE = 0.8
        (X[:, 1] >= 220)      # TIM_THICKNESS >= 220
    )
    
    weights[difficult_mask] *= weight_factor
    
    return weights


# ==========================================
# è¨“ç·´èˆ‡è©•ä¼°
# ==========================================

def train_model(X_train, y_train, config, verbose=True):
    """è¨“ç·´ Variational DKL æ¨¡å‹"""
    
    # è¨ˆç®—æ¨£æœ¬æ¬Šé‡
    sample_weights_np = compute_sample_weights(X_train, config['sample_weight_factor'])
    
    if verbose:
        difficult_count = np.sum(sample_weights_np > 1.0)
        print(f"\nè¨ˆç®—æ¨£æœ¬æ¬Šé‡:")
        print(f"  å›°é›£æ¨£æœ¬æ•¸: {difficult_count} ({difficult_count/len(X_train)*100:.2f}%)")
        print(f"  æ¬Šé‡å€æ•¸: {config['sample_weight_factor']}x")
        print(f"  Kernelé¡å‹: {config['kernel_type']}")
        if 'sm' in config['kernel_type']:
            print(f"  SM Mixtures: {config['num_mixtures']}")
        print(f"  èª˜å°é»æ•¸é‡: {config['num_inducing']}")
        print(f"  Batch Size: {config['batch_size']}")
        print(f"  è³‡æ–™å‹åˆ¥: float32 (è¨˜æ†¶é«”å„ªåŒ–)")
    
    # æ¨™æº–åŒ–
    scaler_x = StandardScaler()
    scaler_y = StandardScaler()
    
    X_train_scaled = scaler_x.fit_transform(X_train)
    y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
    
    train_x = torch.from_numpy(X_train_scaled).float().to(device)
    train_y = torch.from_numpy(y_train_scaled).float().to(device)
    sample_weights = torch.from_numpy(sample_weights_np).float().to(device)
    
    # å»ºç«‹ç‰¹å¾µæå–å™¨
    feature_extractor = DnnFeatureExtractor(
        input_dim=train_x.shape[1],
        hidden_dims=config['hidden_dims'],
        output_dim=config['feature_dim'],
        dropout=config['dropout']
    ).to(device)
    
    # é¸æ“‡èª˜å°é» (ä½¿ç”¨ k-means åœ¨åŸå§‹ç©ºé–“)
    num_inducing = min(config['num_inducing'], len(train_x))
    
    if verbose:
        print(f"\nåˆå§‹åŒ–èª˜å°é» (ä½¿ç”¨ k-means)...")
    
    # åœ¨åŸå§‹ç‰¹å¾µç©ºé–“åš k-means
    kmeans = KMeans(n_clusters=num_inducing, random_state=config.get('seed', 2024), n_init=10)
    kmeans.fit(X_train_scaled)
    
    # !!!é—œéµä¿®æ”¹ï¼šèª˜å°é»æ‡‰è©²åœ¨è¼¸å…¥ç©ºé–“ï¼ˆæ¨™æº–åŒ–å¾Œçš„ Xï¼‰ï¼Œä¸æ˜¯ç‰¹å¾µç©ºé–“!!!
    inducing_points = torch.from_numpy(kmeans.cluster_centers_).float().to(device)
    
    if verbose:
        print(f"âœ“ èª˜å°é»åˆå§‹åŒ–å®Œæˆ (shape: {inducing_points.shape})")
    
    # å»ºç«‹æ¨¡å‹
    likelihood = gpytorch.likelihoods.GaussianLikelihood().to(device)
    model = VariationalSMGP(
        inducing_points, 
        feature_extractor,
        kernel_type=config['kernel_type'],
        num_mixtures=config.get('num_mixtures', 2)
    ).to(device)
    
    # å„ªåŒ–å™¨
    optimizer = optim.Adam([
        {'params': model.feature_extractor.parameters(), 'lr': config['lr'], 'weight_decay': 1e-4},
        {'params': model.variational_parameters(), 'lr': config['lr'] * 0.5},
        {'params': model.covar_module.parameters(), 'lr': config['lr'] * 0.1},
        {'params': model.mean_module.parameters()},
        {'params': likelihood.parameters()},
    ], lr=config['lr'])
    
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=2)
    
    # Variational ELBO
    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=len(train_y))
    
    # è¨“ç·´
    if verbose:
        print(f"\né–‹å§‹è¨“ç·´...")
    
    model.train()
    likelihood.train()
    
    best_loss = float('inf')
    patience_counter = 0
    batch_size = config['batch_size']
    n_batches = (len(train_x) + batch_size - 1) // batch_size
    
    for epoch in range(config['epochs']):
        epoch_loss = 0.0
        epoch_elbo = 0.0
        epoch_mape = 0.0
        
        # Mini-batch è¨“ç·´
        indices_perm = torch.randperm(len(train_x))
        
        for i in range(n_batches):
            start_idx = i * batch_size
            end_idx = min((i + 1) * batch_size, len(train_x))
            batch_indices = indices_perm[start_idx:end_idx]
            
            batch_x = train_x[batch_indices]
            batch_y = train_y[batch_indices]
            batch_weights = sample_weights[batch_indices]
            
            optimizer.zero_grad()
            
            # å‰å‘å‚³æ’­
            output = model(batch_x)
            
            # ELBO loss
            elbo_loss = -mll(output, batch_y)
            
            # MAPE loss
            mape = weighted_mape_loss(output.mean, batch_y, batch_weights)
            
            # ç¸½æå¤±
            total_loss = elbo_loss + config['mape_weight'] * mape
            
            # åå‘å‚³æ’­
            total_loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            
            optimizer.step()
            
            epoch_loss += total_loss.item()
            epoch_elbo += elbo_loss.item()
            epoch_mape += mape.item()
            
            # æ¸…ç†è¨˜æ†¶é«”
            del output, elbo_loss, mape, total_loss
            
            # æ¯å€‹ batch éƒ½æ¸…ç†ä¸€æ¬¡
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        
        scheduler.step()
        
        # å¹³å‡æå¤±
        avg_loss = epoch_loss / n_batches
        avg_elbo = epoch_elbo / n_batches
        avg_mape = epoch_mape / n_batches
        
        # é¡¯ç¤ºè¨“ç·´é€²åº¦
        if verbose and (epoch + 1) % 50 == 0:
            print(f"Epoch {epoch+1}: ELBO={avg_elbo:.4f}, "
                  f"MAPE={avg_mape:.2f}%, Total={avg_loss:.4f}")
        
        # Early stopping
        if avg_loss < best_loss:
            best_loss = avg_loss
            patience_counter = 0
            best_state = {
                'model': model.state_dict(),
                'likelihood': likelihood.state_dict(),
            }
        else:
            patience_counter += 1
        
        if patience_counter >= config['patience']:
            if verbose:
                print(f"æ—©åœ at Epoch {epoch+1}")
            break
    
    # è¼‰å…¥æœ€ä½³æ¨¡å‹
    model.load_state_dict(best_state['model'])
    likelihood.load_state_dict(best_state['likelihood'])
    
    if verbose:
        print(f"è¨“ç·´å®Œæˆ (Final Loss: {best_loss:.4f})")
    
    clear_gpu_cache()
    
    return model, likelihood, scaler_x, scaler_y


def evaluate_model(model, likelihood, X_test, y_test, scaler_x, scaler_y, verbose=True):
    """è©•ä¼°æ¨¡å‹"""
    model.eval()
    likelihood.eval()
    
    X_test_scaled = scaler_x.transform(X_test)
    test_x = torch.from_numpy(X_test_scaled).float().to(device)
    
    with torch.no_grad(), gpytorch.settings.fast_pred_var():
        pred_dist = likelihood(model(test_x))
        y_pred_scaled = pred_dist.mean.cpu().numpy()
        y_std_scaled = pred_dist.stddev.cpu().numpy()
    
    # åæ¨™æº–åŒ–
    y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
    y_std = y_std_scaled * scaler_y.scale_[0]
    
    # è¨ˆç®—æŒ‡æ¨™
    relative_errors = np.abs((y_test - y_pred) / y_test) * 100
    
    mape = np.mean(relative_errors)
    mae = np.mean(np.abs(y_test - y_pred))
    max_error = np.max(relative_errors)
    
    outliers_20 = np.sum(relative_errors > 20)
    outliers_15 = np.sum(relative_errors > 15)
    outliers_10 = np.sum(relative_errors > 10)
    
    # Type 3åˆ†æ
    type3_mask = X_test[:, 0] == 3
    type3_outliers = np.sum((relative_errors > 20) & type3_mask)
    
    if verbose:
        print(f"\n{'='*60}")
        print(f"è©•ä¼°çµæœ")
        print(f"{'='*60}")
        print(f"æ¨£æœ¬æ•¸: {len(y_test)}")
        print(f"\næº–ç¢ºåº¦:")
        print(f"  MAPE:      {mape:.2f}%")
        print(f"  MAE:       {mae:.4f}")
        print(f"  Max Error: {max_error:.2f}%")
        print(f"\nç•°å¸¸é»:")
        print(f"  >20%: {outliers_20}/{len(y_test)} ({outliers_20/len(y_test)*100:.2f}%)")
        print(f"  >15%: {outliers_15}/{len(y_test)} ({outliers_15/len(y_test)*100:.2f}%)")
        print(f"  >10%: {outliers_10}/{len(y_test)} ({outliers_10/len(y_test)*100:.2f}%)")
        if np.sum(type3_mask) > 0:
            print(f"\nType 3ç•°å¸¸é»: {type3_outliers}/{np.sum(type3_mask)}")
        print(f"{'='*60}\n")
    
    results = {
        'mape': mape,
        'mae': mae,
        'max_error': max_error,
        'outliers_20': outliers_20,
        'outliers_15': outliers_15,
        'outliers_10': outliers_10,
        'type3_outliers': type3_outliers,
        'predictions': y_pred,
        'std': y_std,
        'errors': relative_errors
    }
    
    return results


def save_predictions(X_test, y_test, results, filename):
    """ä¿å­˜é æ¸¬çµæœåˆ°CSV"""
    df = pd.DataFrame({
        'TIM_TYPE': X_test[:, 0],
        'TIM_THICKNESS': X_test[:, 1],
        'TIM_COVERAGE': X_test[:, 2],
        'True': y_test,
        'Predicted': results['predictions'],
        'Error%': results['errors'],
        'Std': results['std']
    })
    
    df.to_csv(filename, index=False)
    print(f"âœ“ é æ¸¬çµæœå·²ä¿å­˜åˆ°: {filename}")


# ==========================================
# ä¸»å‡½æ•¸
# ==========================================

def main(seed=2024, kernel_type='sm', num_mixtures=2, verbose=True):
    """ä¸»è¨“ç·´æµç¨‹"""
    
    clear_gpu_cache()
    set_seed(seed)
    
    print(f"\nä½¿ç”¨è£ç½®: {device}\n")
    
    print("="*60)
    print(f"Phase 4A: SM Kernel å¯¦é©— (æ¿€é€²è¨˜æ†¶é«”å„ªåŒ– v2)")
    print(f"Kernel: {kernel_type}")
    print("="*60)
    
    # ç‰¹å¾µå’Œç›®æ¨™
    feature_cols = ['TIM_TYPE', 'TIM_THICKNESS', 'TIM_COVERAGE']
    target_col = 'Theta.JC'
    
    # é…ç½® (æ¿€é€²è¨˜æ†¶é«”å„ªåŒ–)
    config = {
        'hidden_dims': [32, 16],       # å°ç¶²è·¯
        'feature_dim': 4,               # å°ç‰¹å¾µç¶­åº¦
        'dropout': 0.05,
        'lr': 0.01,
        'epochs': 500,
        'patience': 50,
        'mape_weight': 0.1,
        'sample_weight_factor': 3.0,
        'kernel_type': kernel_type,
        'num_mixtures': num_mixtures,
        'batch_size': 128,              # å° batch
        'num_inducing': 256,            # å°‘é‡èª˜å°é»
        'seed': seed,
    }
    
    if verbose:
        print(f"\né…ç½®:")
        for key, value in config.items():
            if key != 'seed':
                print(f"  {key}: {value}")
    
    # ==========================================
    # Above Dataset
    # ==========================================
    
    print(f"\n\n{'ğŸ”µ Above 50% Coverage'}\n")
    
    train_above = pd.read_excel('data/train/Above.xlsx')
    test_above = pd.read_excel('data/test/Above.xlsx')
    
    train_above_clean = train_above.groupby(feature_cols, as_index=False).agg({
        target_col: 'mean'
    })
    
    print(f"è¨“ç·´é›†: {len(train_above_clean)} ç­†")
    print(f"æ¸¬è©¦é›†: {len(test_above)} ç­†")
    
    X_train_above = train_above_clean[feature_cols].values
    y_train_above = train_above_clean[target_col].values
    
    X_test_above = test_above[feature_cols].values
    y_test_above = test_above[target_col].values
    
    # è¨“ç·´
    model_above, likelihood_above, scaler_x_above, scaler_y_above = train_model(
        X_train_above, y_train_above, config, verbose=verbose
    )
    
    # è©•ä¼°
    results_above = evaluate_model(
        model_above, likelihood_above, 
        X_test_above, y_test_above, 
        scaler_x_above, scaler_y_above,
        verbose=verbose
    )
    
    # ä¿å­˜é æ¸¬çµæœ
    save_predictions(X_test_above, y_test_above, results_above, 
                     f'phase4a_{kernel_type}_m{num_mixtures}_above_seed{seed}_predictions.csv')
    
    clear_gpu_cache()
    
    # ==========================================
    # Below Dataset
    # ==========================================
    
    print(f"\n\n{'ğŸ”µ Below 50% Coverage'}\n")
    
    train_below = pd.read_excel('data/train/Below.xlsx')
    test_below = pd.read_excel('data/test/Below.xlsx')
    
    train_below_clean = train_below.groupby(feature_cols, as_index=False).agg({
        target_col: 'mean'
    })
    
    print(f"è¨“ç·´é›†: {len(train_below_clean)} ç­†")
    print(f"æ¸¬è©¦é›†: {len(test_below)} ç­†")
    
    X_train_below = train_below_clean[feature_cols].values
    y_train_below = train_below_clean[target_col].values
    
    X_test_below = test_below[feature_cols].values
    y_test_below = test_below[target_col].values
    
    # è¨“ç·´
    model_below, likelihood_below, scaler_x_below, scaler_y_below = train_model(
        X_train_below, y_train_below, config, verbose=verbose
    )
    
    # è©•ä¼°
    results_below = evaluate_model(
        model_below, likelihood_below,
        X_test_below, y_test_below,
        scaler_x_below, scaler_y_below,
        verbose=verbose
    )
    
    # ä¿å­˜é æ¸¬çµæœ
    save_predictions(X_test_below, y_test_below, results_below,
                     f'phase4a_{kernel_type}_m{num_mixtures}_below_seed{seed}_predictions.csv')
    
    # ==========================================
    # ç¸½çµ
    # ==========================================
    
    print("\n" + "="*60)
    print(f"Phase 4A çµæœç¸½çµ - Kernel: {kernel_type}")
    if 'sm' in kernel_type:
        print(f"Mixtures: {num_mixtures}")
    print("="*60)
    print(f"éš¨æ©Ÿç¨®å­: {seed}")
    
    print(f"\nAboveè³‡æ–™é›†:")
    print(f"  ç•°å¸¸é» (>20%): {results_above['outliers_20']}/{len(y_test_above)} ({results_above['outliers_20']/len(y_test_above)*100:.2f}%)")
    print(f"  MAPE: {results_above['mape']:.2f}%")
    print(f"  Type 3ç•°å¸¸é»: {results_above['type3_outliers']}")
    
    print(f"\nBelowè³‡æ–™é›†:")
    print(f"  ç•°å¸¸é» (>20%): {results_below['outliers_20']}/{len(y_test_below)} ({results_below['outliers_20']/len(y_test_below)*100:.2f}%)")
    print(f"  MAPE: {results_below['mape']:.2f}%")
    
    print("\n" + "="*60)
    print("âœ“ è¨“ç·´å®Œæˆï¼")
    print("="*60 + "\n")
    
    return {
        'above': results_above,
        'below': results_below,
        'kernel_type': kernel_type,
        'seed': seed
    }


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Phase 4A - SM Kernel å¯¦é©—')
    parser.add_argument('--seed', type=int, default=2024, help='éš¨æ©Ÿç¨®å­')
    parser.add_argument('--kernel', type=str, default='sm', 
                        choices=['sm', 'sm+rbf', 'sm+matern', 'rbf', 'matern', 'rbf+matern'],
                        help='Kernelé¡å‹')
    parser.add_argument('--mixtures', type=int, default=2, 
                        help='SM mixtures æ•¸é‡ (1-3 æ¨è–¦)')
    parser.add_argument('-v', '--verbose', action='store_true', help='é¡¯ç¤ºè©³ç´°è¨“ç·´éç¨‹')
    
    args = parser.parse_args()
    
    results = main(
        seed=args.seed,
        kernel_type=args.kernel,
        num_mixtures=args.mixtures,
        verbose=args.verbose
    )
    
    print("\nğŸ’¡ ä½¿ç”¨ç¯„ä¾‹:")
    print("  python phase4a_sm_kernel.py --kernel sm --mixtures 2 -v         # ç´” SM (2 mixtures)")
    print("  python phase4a_sm_kernel.py --kernel sm --mixtures 3 -v         # ç´” SM (3 mixtures)")
    print("  python phase4a_sm_kernel.py --kernel sm+rbf --mixtures 2 -v     # SM + RBF")
    print("  python phase4a_sm_kernel.py --kernel sm+matern --mixtures 2 -v  # SM + MatÃ©rn")
    print("  python phase4a_sm_kernel.py --kernel rbf+matern -v              # RBF + MatÃ©rn (å°ç…§)\n")