"""
Phase 3D: æœ€çµ‚ä¿®æ­£ç‰ˆ - å›æ­¸ç°¡å–®ä½†æ­£ç¢ºçš„å¯¦ç¾

æ ¸å¿ƒä¿®æ­£:
1. ä½¿ç”¨ scaler.inverse_transform() ç¢ºä¿åæ¨™æº–åŒ–æ­£ç¢º
2. å»é™¤è¤‡é›œçš„ Barrier Lossï¼Œä½¿ç”¨ç°¡å–®çš„åŠ æ¬Š MAPE
3. åœ¨åŸå§‹ç©ºé–“æ­£ç¢ºè¨ˆç®— Loss

ä½¿ç”¨æ–¹æ³•:
    python phase3d_final.py --n_trials 30
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import gpytorch
from sklearn.preprocessing import StandardScaler
import warnings
import random
import os
import argparse
import optuna
from optuna.samplers import TPESampler
import json

warnings.filterwarnings('ignore')
torch.set_default_dtype(torch.float64)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)


def clear_gpu_cache():
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()


# ==========================================
# æ¨¡å‹å®šç¾©
# ==========================================

class DnnFeatureExtractorWithEmbedding(nn.Module):
    """æ·±åº¦ç¥ç¶“ç¶²è·¯ç‰¹å¾µæå–å™¨ + Entity Embedding"""
    
    def __init__(self, num_categories=3, embedding_dim=4, numerical_dim=2, 
                 hidden_dims=[64, 32, 16], output_dim=8, dropout=0.1):
        super().__init__()
        
        self.embedding = nn.Embedding(num_categories + 1, embedding_dim)
        input_dim = embedding_dim + numerical_dim
        
        layers = []
        prev_dim = input_dim
        
        for h_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, h_dim),
                nn.BatchNorm1d(h_dim),
                nn.ReLU(),
                nn.Dropout(dropout)
            ])
            prev_dim = h_dim
        
        layers.append(nn.Linear(prev_dim, output_dim))
        
        self.network = nn.Sequential(*layers)
        self.output_dim = output_dim
        
        nn.init.xavier_uniform_(self.embedding.weight)
    
    def forward(self, x_cat, x_num):
        embedded = self.embedding(x_cat.long())
        combined = torch.cat([embedded, x_num], dim=1)
        return self.network(combined)


class StandardGPModel(gpytorch.models.ExactGP):
    """æ¨™æº– GP æ¨¡å‹"""
    
    def __init__(self, train_x, train_y, likelihood, feature_dim):
        super().__init__(train_x, train_y, likelihood)
        self.mean_module = gpytorch.means.ConstantMean()
        self.covar_module = gpytorch.kernels.ScaleKernel(
            gpytorch.kernels.RBFKernel(ard_num_dims=feature_dim)
        )
    
    def forward(self, x):
        mean_x = self.mean_module(x)
        covar_x = self.covar_module(x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)


# ==========================================
# æå¤±å‡½æ•¸ï¼ˆç°¡åŒ–ç‰ˆï¼‰
# ==========================================

def compute_sample_weights(X, weight_factor=3.0):
    """è¨ˆç®—æ¨£æœ¬æ¬Šé‡"""
    weights = np.ones(len(X))
    
    difficult_mask = (
        (X[:, 0] == 3) &
        (X[:, 2] == 0.8) &
        (X[:, 1] >= 220)
    )
    
    weights[difficult_mask] *= weight_factor
    return weights


def safe_mape_loss(y_pred, y_true, weights, epsilon=1e-8):
    """
    å®‰å…¨çš„ MAPE Loss - åœ¨åŸå§‹ç©ºé–“è¨ˆç®—
    
    ç°¡å–®ä½†æ­£ç¢ºçš„å¯¦ç¾ï¼Œä¸ä½¿ç”¨è¤‡é›œçš„ Barrier
    """
    # è¨ˆç®—ç™¾åˆ†æ¯”èª¤å·®
    abs_error_percent = torch.abs((y_true - y_pred) / (torch.abs(y_true) + epsilon)) * 100
    
    # ã€å®‰å…¨æ€§ã€‘Clamp åˆ°åˆç†ç¯„åœï¼Œé¿å…æ¥µç«¯å€¼
    abs_error_percent = torch.clamp(abs_error_percent, max=100.0)
    
    # åŠ æ¬Šå¹³å‡
    weighted_mape = torch.sum(abs_error_percent * weights) / torch.sum(weights)
    
    return weighted_mape


# ==========================================
# è¨“ç·´å‡½æ•¸ï¼ˆå®Œå…¨é‡å¯«ï¼‰
# ==========================================

def train_dkl_model(X_train, y_train, config, verbose=False):
    """
    è¨“ç·´ DKL æ¨¡å‹ - æœ€çµ‚ä¿®æ­£ç‰ˆ
    
    é—œéµ: ä½¿ç”¨ scaler.inverse_transform() ç¢ºä¿åæ¨™æº–åŒ–æ­£ç¢º
    """
    # åˆ†é›¢ç‰¹å¾µ
    X_cat = X_train[:, 0]
    X_num = X_train[:, 1:]
    
    # æ¨™æº–åŒ–æ•¸å€¼ç‰¹å¾µ
    scaler_num = StandardScaler()
    X_num_scaled = scaler_num.fit_transform(X_num)
    
    # æ¨™æº–åŒ–ç›®æ¨™è®Šæ•¸
    scaler_y = StandardScaler()
    y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
    
    # è½‰ Tensor
    train_x_cat = torch.from_numpy(X_cat).to(device)
    train_x_num = torch.from_numpy(X_num_scaled).to(device)
    train_y = torch.from_numpy(y_train_scaled).to(device)
    
    # ã€é‡è¦ã€‘åŒæ™‚ä¿å­˜åŸå§‹ y ç”¨æ–¼ Loss è¨ˆç®—
    train_y_original = torch.from_numpy(y_train).to(device)
    
    # æ¨£æœ¬æ¬Šé‡
    sample_weights_np = compute_sample_weights(X_train, config['sample_weight_factor'])
    sample_weights = torch.from_numpy(sample_weights_np).to(device)
    
    # å»ºç«‹æ¨¡å‹
    feature_extractor = DnnFeatureExtractorWithEmbedding(
        num_categories=3,
        embedding_dim=config['embedding_dim'],
        numerical_dim=2,
        hidden_dims=config['hidden_dims'],
        output_dim=config['feature_dim'],
        dropout=config['dropout']
    ).to(device)
    
    with torch.no_grad():
        initial_features = feature_extractor(train_x_cat, train_x_num)
    
    likelihood = gpytorch.likelihoods.GaussianLikelihood().to(device)
    gp_model = StandardGPModel(
        initial_features, train_y, likelihood, config['feature_dim']
    ).to(device)
    
    # å„ªåŒ–å™¨
    optimizer = optim.Adam([
        {'params': feature_extractor.parameters(), 
         'lr': config['lr'], 'weight_decay': config['weight_decay']},
        {'params': gp_model.covar_module.parameters()},
        {'params': gp_model.mean_module.parameters()},
        {'params': likelihood.parameters()},
    ], lr=config['lr'])
    
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=2)
    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp_model)
    
    # è¨“ç·´
    feature_extractor.train()
    gp_model.train()
    likelihood.train()
    
    best_loss = float('inf')
    patience_counter = 0
    
    for epoch in range(config['epochs']):
        optimizer.zero_grad()
        
        # æå–ç‰¹å¾µ
        features = feature_extractor(train_x_cat, train_x_num)
        gp_model.set_train_data(features, train_y, strict=False)
        output = gp_model(features)
        
        # Loss 1: GP Loss (åœ¨æ¨™æº–åŒ–ç©ºé–“)
        gp_loss = -mll(output, train_y)
        
        # Loss 2: MAPE (åœ¨åŸå§‹ç©ºé–“ï¼)
        # ã€é—œéµã€‘ä½¿ç”¨ scaler æ­£ç¢ºåæ¨™æº–åŒ–
        pred_scaled = output.mean.cpu().detach().numpy().reshape(-1, 1)
        pred_original = scaler_y.inverse_transform(pred_scaled).flatten()
        pred_original_tensor = torch.from_numpy(pred_original).to(device)
        
        # è¨ˆç®—åŸå§‹ç©ºé–“çš„ MAPE
        mape_loss_val = safe_mape_loss(pred_original_tensor, train_y_original, sample_weights)
        
        # ç¸½æå¤±
        total_loss = gp_loss + config['mape_weight'] * mape_loss_val
        
        # åå‘å‚³æ’­
        total_loss.backward()
        optimizer.step()
        scheduler.step()
        
        current_loss = total_loss.item()
        
        # ç›£æ§
        if verbose and (epoch + 1) % 100 == 0:
            print(f"Epoch {epoch+1}: GP Loss={gp_loss.item():.4f}, "
                  f"MAPE={mape_loss_val.item():.2f}%, Total={total_loss.item():.4f}")
        
        # Early stopping
        if current_loss < best_loss:
            best_loss = current_loss
            patience_counter = 0
            best_state = {
                'feature_extractor': feature_extractor.state_dict(),
                'gp_model': gp_model.state_dict(),
                'likelihood': likelihood.state_dict(),
            }
        else:
            patience_counter += 1
        
        if patience_counter >= config['patience']:
            if verbose:
                print(f"æ—©åœ at Epoch {epoch+1}")
            break
    
    # è¼‰å…¥æœ€ä½³æ¨¡å‹
    feature_extractor.load_state_dict(best_state['feature_extractor'])
    gp_model.load_state_dict(best_state['gp_model'])
    likelihood.load_state_dict(best_state['likelihood'])
    
    return feature_extractor, gp_model, likelihood, scaler_num, scaler_y


def evaluate_dkl_model(feature_extractor, gp_model, likelihood, 
                       X_test, y_test, scaler_num, scaler_y, verbose=False):
    """è©•ä¼°æ¨¡å‹"""
    feature_extractor.eval()
    gp_model.eval()
    likelihood.eval()
    
    X_cat = X_test[:, 0]
    X_num = X_test[:, 1:]
    X_num_scaled = scaler_num.transform(X_num)
    
    test_x_cat = torch.from_numpy(X_cat).to(device)
    test_x_num = torch.from_numpy(X_num_scaled).to(device)
    
    with torch.no_grad(), gpytorch.settings.fast_pred_var():
        test_features = feature_extractor(test_x_cat, test_x_num)
        pred_dist = likelihood(gp_model(test_features))
        y_pred_scaled = pred_dist.mean.cpu().numpy()
        y_std_scaled = pred_dist.stddev.cpu().numpy()
    
    # ä½¿ç”¨ scaler.inverse_transform() åæ¨™æº–åŒ–
    y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
    y_std = y_std_scaled * scaler_y.scale_[0]
    
    relative_errors = np.abs((y_test - y_pred) / y_test) * 100
    
    mape = np.mean(relative_errors)
    mae = np.mean(np.abs(y_test - y_pred))
    max_error = np.max(relative_errors)
    
    outliers_20 = np.sum(relative_errors > 20)
    outliers_15 = np.sum(relative_errors > 15)
    outliers_10 = np.sum(relative_errors > 10)
    
    type3_mask = X_test[:, 0] == 3
    type3_outliers = np.sum((relative_errors > 20) & type3_mask)
    
    if verbose:
        print(f"\n{'='*60}")
        print(f"è©•ä¼°çµæœ")
        print(f"{'='*60}")
        print(f"æ¨£æœ¬æ•¸: {len(y_test)}")
        print(f"\næº–ç¢ºåº¦:")
        print(f"  MAPE:      {mape:.2f}%")
        print(f"  MAE:       {mae:.4f}")
        print(f"  Max Error: {max_error:.2f}%")
        print(f"\nç•°å¸¸é»:")
        print(f"  >20%: {outliers_20}/{len(y_test)} ({outliers_20/len(y_test)*100:.2f}%)")
        print(f"  >15%: {outliers_15}/{len(y_test)}")
        print(f"  >10%: {outliers_10}/{len(y_test)}")
        if np.sum(type3_mask) > 0:
            print(f"\nType 3ç•°å¸¸é»: {type3_outliers}/{np.sum(type3_mask)}")
        print(f"{'='*60}\n")
    
    return {
        'mape': mape,
        'mae': mae,
        'max_error': max_error,
        'outliers_20': outliers_20,
        'outliers_15': outliers_15,
        'outliers_10': outliers_10,
        'type3_outliers': type3_outliers,
        'predictions': y_pred,
        'std': y_std,
        'errors': relative_errors
    }


# ==========================================
# Optuna
# ==========================================

def objective(trial, data_dict, seed=2024):
    """Optuna å„ªåŒ–"""
    clear_gpu_cache()
    set_seed(seed)
    
    config = {
        'embedding_dim': trial.suggest_int('embedding_dim', 4, 8),
        'hidden_dims': [64, 32, 16],
        'feature_dim': trial.suggest_int('feature_dim', 8, 16),
        'dropout': trial.suggest_float('dropout', 0.05, 0.15),
        'lr': trial.suggest_float('lr', 0.008, 0.012, log=False),
        'weight_decay': trial.suggest_float('weight_decay', 1e-5, 5e-4, log=True),
        'mape_weight': trial.suggest_float('mape_weight', 0.1, 0.3),
        'sample_weight_factor': trial.suggest_float('sample_weight_factor', 2.5, 3.5),
        'epochs': 500,
        'patience': 50,
    }
    
    try:
        # Above
        fe_above, gp_above, ll_above, scaler_num_above, scaler_y_above = train_dkl_model(
            data_dict['X_train_above'], data_dict['y_train_above'], config, verbose=False
        )
        
        results_above = evaluate_dkl_model(
            fe_above, gp_above, ll_above,
            data_dict['X_test_above'], data_dict['y_test_above'],
            scaler_num_above, scaler_y_above, verbose=False
        )
        
        # Below
        fe_below, gp_below, ll_below, scaler_num_below, scaler_y_below = train_dkl_model(
            data_dict['X_train_below'], data_dict['y_train_below'], config, verbose=False
        )
        
        results_below = evaluate_dkl_model(
            fe_below, gp_below, ll_below,
            data_dict['X_test_below'], data_dict['y_test_below'],
            scaler_num_below, scaler_y_below, verbose=False
        )
        
        # ç›®æ¨™å€¼
        objective_value = (results_above['outliers_20'] + 0.3 * results_above['mape'] + 
                          0.1 * results_below['mape'])
        
        # è¨˜éŒ„æŒ‡æ¨™
        trial.set_user_attr('above_outliers_20', results_above['outliers_20'])
        trial.set_user_attr('above_mape', results_above['mape'])
        trial.set_user_attr('above_type3_outliers', results_above['type3_outliers'])
        trial.set_user_attr('below_outliers_20', results_below['outliers_20'])
        trial.set_user_attr('below_mape', results_below['mape'])
        
        return objective_value
        
    except Exception as e:
        print(f"Trial {trial.number} failed: {e}")
        return float('inf')


# ==========================================
# ä¸»å‡½æ•¸
# ==========================================

def main_optuna(n_trials=30, seed=2024):
    """ä¸»è¨“ç·´æµç¨‹"""
    set_seed(seed)
    
    print("="*60)
    print("Phase 3D: æœ€çµ‚ä¿®æ­£ç‰ˆ")
    print("="*60)
    print(f"è£ç½®: {device}")
    print(f"éš¨æ©Ÿç¨®å­: {seed}")
    print(f"Optuna è©¦é©—æ¬¡æ•¸: {n_trials}\n")
    
    print("ã€é—œéµä¿®æ­£ã€‘")
    print("  âœ“ ä½¿ç”¨ scaler.inverse_transform() æ­£ç¢ºåæ¨™æº–åŒ–")
    print("  âœ“ åœ¨åŸå§‹ç©ºé–“è¨ˆç®— MAPE Loss")
    print("  âœ“ ç°¡åŒ–ä½†æ­£ç¢ºçš„å¯¦ç¾\n")
    
    # è¼‰å…¥è³‡æ–™
    feature_cols = ['TIM_TYPE', 'TIM_THICKNESS', 'TIM_COVERAGE']
    target_col = 'Theta.JC'
    
    train_above = pd.read_excel('data/train/Above.xlsx')
    test_above = pd.read_excel('data/test/Above.xlsx')
    train_below = pd.read_excel('data/train/Below.xlsx')
    test_below = pd.read_excel('data/test/Below.xlsx')
    
    train_above_clean = train_above.groupby(feature_cols, as_index=False).agg({target_col: 'mean'})
    train_below_clean = train_below.groupby(feature_cols, as_index=False).agg({target_col: 'mean'})
    
    X_train_above = train_above_clean[feature_cols].values
    y_train_above = train_above_clean[target_col].values
    X_test_above = test_above[feature_cols].values
    y_test_above = test_above[target_col].values
    
    X_train_below = train_below_clean[feature_cols].values
    y_train_below = train_below_clean[target_col].values
    X_test_below = test_below[feature_cols].values
    y_test_below = test_below[target_col].values
    
    print(f"Above: è¨“ç·´{len(X_train_above)}ç­†, æ¸¬è©¦{len(X_test_above)}ç­†")
    print(f"Below: è¨“ç·´{len(X_train_below)}ç­†, æ¸¬è©¦{len(X_test_below)}ç­†\n")
    
    data_dict = {
        'X_train_above': X_train_above, 'y_train_above': y_train_above,
        'X_test_above': X_test_above, 'y_test_above': y_test_above,
        'X_train_below': X_train_below, 'y_train_below': y_train_below,
        'X_test_below': X_test_below, 'y_test_below': y_test_below,
    }
    
    # Optuna
    study = optuna.create_study(
        direction='minimize',
        sampler=TPESampler(seed=seed),
        study_name=f'phase3d_final_seed{seed}'
    )
    
    print("é–‹å§‹ Optuna æœå°‹...\n")
    study.optimize(
        lambda trial: objective(trial, data_dict, seed=seed),
        n_trials=n_trials,
        show_progress_bar=True
    )
    
    # æª¢æŸ¥çµæœ
    successful_trials = [t for t in study.trials if t.value != float('inf')]
    
    if len(successful_trials) == 0:
        print("\nâŒ æ‰€æœ‰ trials éƒ½å¤±æ•—äº†ï¼\n")
        return None, None, None
    
    # æœ€ä½³çµæœ
    best_trial = study.best_trial
    print("\n" + "="*60)
    print("Optuna æœå°‹å®Œæˆï¼")
    print("="*60)
    print(f"\næœ€ä½³ Trial: #{best_trial.number}")
    print(f"ç›®æ¨™å€¼: {best_trial.value:.4f}")
    print(f"æˆåŠŸ trials: {len(successful_trials)}/{n_trials}")
    
    print(f"\nAbove:")
    print(f"  ç•°å¸¸é»: {best_trial.user_attrs['above_outliers_20']}")
    print(f"  MAPE: {best_trial.user_attrs['above_mape']:.2f}%")
    print(f"  Type 3ç•°å¸¸é»: {best_trial.user_attrs['above_type3_outliers']}")
    
    print(f"\nBelow:")
    print(f"  ç•°å¸¸é»: {best_trial.user_attrs['below_outliers_20']}")
    print(f"  MAPE: {best_trial.user_attrs['below_mape']:.2f}%")
    
    print(f"\næœ€ä½³è¶…åƒæ•¸:")
    for key, value in best_trial.params.items():
        print(f"  {key}: {value}")
    
    # é‡ç½®ç¨®å­ä¸¦é‡æ–°è¨“ç·´
    print("\n" + "="*60)
    print("ç”¨æœ€ä½³é…ç½®é‡æ–°è¨“ç·´...")
    print("="*60 + "\n")
    
    set_seed(seed)
    clear_gpu_cache()
    
    best_config = {
        'embedding_dim': best_trial.params['embedding_dim'],
        'hidden_dims': [64, 32, 16],
        'feature_dim': best_trial.params['feature_dim'],
        'dropout': best_trial.params['dropout'],
        'lr': best_trial.params['lr'],
        'weight_decay': best_trial.params['weight_decay'],
        'mape_weight': best_trial.params['mape_weight'],
        'sample_weight_factor': best_trial.params['sample_weight_factor'],
        'epochs': 500,
        'patience': 50,
    }
    
    # Above
    print("ğŸ”µ Above\n")
    fe_above, gp_above, ll_above, scaler_num_above, scaler_y_above = train_dkl_model(
        X_train_above, y_train_above, best_config, verbose=True
    )
    results_above = evaluate_dkl_model(
        fe_above, gp_above, ll_above, X_test_above, y_test_above,
        scaler_num_above, scaler_y_above, verbose=True
    )
    
    # Below
    print("\nğŸ”µ Below\n")
    fe_below, gp_below, ll_below, scaler_num_below, scaler_y_below = train_dkl_model(
        X_train_below, y_train_below, best_config, verbose=True
    )
    results_below = evaluate_dkl_model(
        fe_below, gp_below, ll_below, X_test_below, y_test_below,
        scaler_num_below, scaler_y_below, verbose=True
    )
    
    # ä¿å­˜çµæœ
    pd.DataFrame({
        'TIM_TYPE': X_test_above[:, 0],
        'TIM_THICKNESS': X_test_above[:, 1],
        'TIM_COVERAGE': X_test_above[:, 2],
        'True': y_test_above,
        'Predicted': results_above['predictions'],
        'Error%': results_above['errors'],
        'Std': results_above['std']
    }).to_csv(f'phase3d_above_seed{seed}.csv', index=False)
    
    pd.DataFrame({
        'TIM_TYPE': X_test_below[:, 0],
        'TIM_THICKNESS': X_test_below[:, 1],
        'TIM_COVERAGE': X_test_below[:, 2],
        'True': y_test_below,
        'Predicted': results_below['predictions'],
        'Error%': results_below['errors'],
        'Std': results_below['std']
    }).to_csv(f'phase3d_below_seed{seed}.csv', index=False)
    
    with open(f'phase3d_config_seed{seed}.json', 'w') as f:
        json.dump(best_config, f, indent=2)
    
    print(f"âœ“ çµæœå·²ä¿å­˜\n")
    
    # ç¸½çµ
    print("="*60)
    print("æœ€çµ‚çµæœ")
    print("="*60)
    print(f"\nAbove: ç•°å¸¸é» {results_above['outliers_20']}/138, MAPE {results_above['mape']:.2f}%")
    print(f"Below: ç•°å¸¸é» {results_below['outliers_20']}/48, MAPE {results_below['mape']:.2f}%")
    
    baseline_outliers, baseline_mape = 7, 8.34
    improvement_outliers = (baseline_outliers - results_above['outliers_20']) / baseline_outliers * 100
    improvement_mape = (baseline_mape - results_above['mape']) / baseline_mape * 100
    
    print(f"\nèˆ‡ Phase 2B æ¯”è¼ƒ:")
    print(f"  ç•°å¸¸é»: {baseline_outliers} â†’ {results_above['outliers_20']} ({improvement_outliers:+.1f}%)")
    print(f"  MAPE: {baseline_mape:.2f}% â†’ {results_above['mape']:.2f}% ({improvement_mape:+.1f}%)")
    print("="*60 + "\n")
    
    return study, results_above, results_below


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('--n_trials', type=int, default=30)
    parser.add_argument('--seed', type=int, default=2024)
    args = parser.parse_args()
    
    study, results_above, results_below = main_optuna(args.n_trials, args.seed)