"""
Phase 2H: å°æŠ— Variance Collapse
é‡å° GP éåº¦å£“ç¸®è®Šç•°æ€§çš„å•é¡Œ

é—œéµç­–ç•¥ï¼š
1. é™ä½ GP likelihood å™ªéŸ³
2. å¢åŠ  feature extractor çš„è¡¨é”èƒ½åŠ›
3. é‡å° Type 3 ä½¿ç”¨æ›´æ¿€é€²çš„è¨“ç·´ç­–ç•¥

ä½¿ç”¨æ–¹æ³•:
    python phase2h_anti_collapse.py --seed 2024
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import gpytorch
from sklearn.preprocessing import StandardScaler
import warnings
import random
import os
import argparse

warnings.filterwarnings('ignore')

torch.set_default_dtype(torch.float64)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def set_seed(seed):
    """è¨­ç½®éš¨æ©Ÿç¨®å­"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)
    print(f"âœ“ éš¨æ©Ÿç¨®å­è¨­å®šç‚º: {seed}")


def clear_gpu_cache():
    """æ¸…ç©ºGPUå¿«å–"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()


# ==========================================
# å¢å¼·çš„ç‰¹å¾µæå–å™¨ (æ›´å¤§å®¹é‡)
# ==========================================

class EnhancedDnnFeatureExtractor(nn.Module):
    """
    å¢å¼·çš„ç‰¹å¾µæå–å™¨
    
    é—œéµæ”¹é€²ï¼š
    1. æ›´æ·±çš„ç¶²è·¯ (æ•æ‰è¤‡é›œæ¨¡å¼)
    2. æ®˜å·®é€£æ¥ (é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±)
    3. æ›´å¤§çš„ç‰¹å¾µç©ºé–“ (ä¿ç•™è®Šç•°æ€§)
    """
    
    def __init__(self, input_dim, hidden_dims=[128, 128, 64, 32], output_dim=16, dropout=0.1):
        super().__init__()
        
        self.input_dim = input_dim
        self.output_dim = output_dim
        
        # è¼¸å…¥å±¤
        self.input_layer = nn.Sequential(
            nn.Linear(input_dim, hidden_dims[0]),
            nn.BatchNorm1d(hidden_dims[0]),
            nn.ReLU(),
            nn.Dropout(dropout)
        )
        
        # éš±è—å±¤ï¼ˆå¸¶æ®˜å·®é€£æ¥ï¼‰
        self.hidden_layers = nn.ModuleList()
        for i in range(len(hidden_dims) - 1):
            self.hidden_layers.append(
                nn.Sequential(
                    nn.Linear(hidden_dims[i], hidden_dims[i+1]),
                    nn.BatchNorm1d(hidden_dims[i+1]),
                    nn.ReLU(),
                    nn.Dropout(dropout)
                )
            )
        
        # è¼¸å‡ºå±¤
        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)
        
    def forward(self, x):
        x = self.input_layer(x)
        
        for layer in self.hidden_layers:
            x_new = layer(x)
            # å¦‚æœç¶­åº¦ç›¸åŒï¼ŒåŠ å…¥æ®˜å·®
            if x_new.shape[1] == x.shape[1]:
                x = x + x_new
            else:
                x = x_new
        
        x = self.output_layer(x)
        return x


class AntiCollapseGPModel(gpytorch.models.ExactGP):
    """
    å°æŠ— Variance Collapse çš„ GP æ¨¡å‹
    
    é—œéµæ”¹é€²ï¼š
    1. æ›´å°çš„ likelihood å™ªéŸ³ï¼ˆå¼·åˆ¶æ¨¡å‹æ“¬åˆç´°ç¯€ï¼‰
    2. æ›´æ•æ„Ÿçš„ kernelï¼ˆå° lengthscaleï¼‰
    3. ä½¿ç”¨ FixedNoiseGaussianLikelihoodï¼ˆæ‰‹å‹•æ§åˆ¶å™ªéŸ³ï¼‰
    """
    
    def __init__(self, train_x, train_y, likelihood, feature_extractor):
        super().__init__(train_x, train_y, likelihood)
        
        self.feature_extractor = feature_extractor
        self.mean_module = gpytorch.means.ConstantMean()
        
        # æ•æ„Ÿçš„ RBF kernelï¼ˆå° lengthscaleï¼‰
        self.covar_module = gpytorch.kernels.ScaleKernel(
            gpytorch.kernels.RBFKernel(
                ard_num_dims=feature_extractor.output_dim,
                lengthscale_constraint=gpytorch.constraints.Interval(0.01, 1.0)  # é™åˆ¶ lengthscale æ›´å°
            )
        )
    
    def forward(self, x):
        projected_x = self.feature_extractor(x)
        mean_x = self.mean_module(projected_x)
        covar_x = self.covar_module(projected_x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)


# ==========================================
# æå¤±å‡½æ•¸ï¼ˆåŠ å…¥æ–¹å·®æ‡²ç½°ï¼‰
# ==========================================

def variance_aware_loss(y_pred, y_true, weights, target_std, epsilon=1e-8):
    """
    åŠ å…¥æ–¹å·®æ‡²ç½°çš„æå¤±å‡½æ•¸
    
    Args:
        y_pred: é æ¸¬å€¼
        y_true: çœŸå¯¦å€¼
        weights: æ¨£æœ¬æ¬Šé‡
        target_std: ç›®æ¨™æ¨™æº–å·®ï¼ˆçœŸå¯¦å€¼çš„ stdï¼‰
    """
    # MAPE æå¤±
    mape = torch.abs((y_true - y_pred) / (torch.abs(y_true) + epsilon)) * 100
    weighted_mape = torch.sum(mape * weights) / torch.sum(weights)
    
    # æ–¹å·®æ‡²ç½°ï¼ˆé¼“å‹µé æ¸¬ä¿æŒè®Šç•°æ€§ï¼‰
    pred_std = torch.std(y_pred)
    std_penalty = torch.abs(pred_std - target_std) / target_std
    
    return weighted_mape + 10.0 * std_penalty  # æ¬Šé‡ 10.0


# ==========================================
# æ¨£æœ¬æ¬Šé‡
# ==========================================

def compute_advanced_weights(X, y=None):
    """è¨ˆç®—æ¨£æœ¬æ¬Šé‡"""
    weights = np.ones(len(X))
    
    tim_type = X[:, 0]
    coverage = X[:, 2]
    
    # Type 3 åŸºç¤æ¬Šé‡
    type3_mask = tim_type == 3
    weights[type3_mask] *= 3.0
    
    # é«˜ Coverage å€åŸŸ
    high_cov_mask = (coverage >= 0.75)
    weights[type3_mask & high_cov_mask] *= 5.0
    
    # æ¥µå°çœŸå¯¦å€¼
    if y is not None:
        small_value_mask = y < 0.03
        weights[small_value_mask] *= 3.0
    
    return weights


# ==========================================
# è¨“ç·´å‡½æ•¸
# ==========================================

def train_anti_collapse_model(X_train, y_train, config, verbose=True):
    """
    è¨“ç·´å°æŠ— Variance Collapse çš„æ¨¡å‹
    """
    # è¨ˆç®—æ¨£æœ¬æ¬Šé‡
    sample_weights = compute_advanced_weights(X_train, y_train)
    
    if verbose:
        print(f"\né«˜æ¬Šé‡æ¨£æœ¬æ•¸: {np.sum(sample_weights > 5.0)}")
    
    # æ¨™æº–åŒ–
    scaler_x = StandardScaler()
    scaler_y = StandardScaler()
    
    X_train_scaled = scaler_x.fit_transform(X_train)
    y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
    
    train_x = torch.from_numpy(X_train_scaled).to(device)
    train_y = torch.from_numpy(y_train_scaled).to(device)
    sample_weights_tensor = torch.from_numpy(sample_weights).to(device)
    
    # è¨ˆç®—ç›®æ¨™æ¨™æº–å·®
    target_std = torch.std(train_y)
    
    # å»ºç«‹æ¨¡å‹
    feature_extractor = EnhancedDnnFeatureExtractor(
        input_dim=train_x.shape[1],
        hidden_dims=config['hidden_dims'],
        output_dim=config['feature_dim'],
        dropout=config['dropout']
    ).to(device)
    
    # ä½¿ç”¨å›ºå®šå™ªéŸ³ï¼ˆéå¸¸å°çš„å™ªéŸ³ï¼‰
    noise = torch.ones(len(train_x)) * 1e-4  # éå¸¸å°çš„å™ªéŸ³
    likelihood = gpytorch.likelihoods.FixedNoiseGaussianLikelihood(
        noise=noise.to(device),
        learn_additional_noise=False  # ä¸å­¸ç¿’å™ªéŸ³
    ).to(device)
    
    model = AntiCollapseGPModel(train_x, train_y, likelihood, feature_extractor).to(device)
    
    # å„ªåŒ–å™¨
    optimizer = optim.AdamW([
        {'params': model.feature_extractor.parameters(), 'lr': config['lr'], 'weight_decay': 1e-4},
        {'params': model.covar_module.parameters(), 'lr': config['lr'] * 0.1},
        {'params': model.mean_module.parameters(), 'lr': config['lr'] * 0.1},
    ], lr=config['lr'])
    
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20)
    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)
    
    # è¨“ç·´
    if verbose:
        print(f"\né–‹å§‹è¨“ç·´ (å°æŠ— Variance Collapse)...")
    
    model.train()
    likelihood.train()
    
    best_loss = float('inf')
    patience_counter = 0
    
    for epoch in range(config['epochs']):
        optimizer.zero_grad()
        
        output = model(train_x)
        gp_loss = -mll(output, train_y)
        
        # åŠ å…¥æ–¹å·®æ‡²ç½°
        variance_loss = variance_aware_loss(
            output.mean, train_y, sample_weights_tensor, target_std
        )
        
        total_loss = gp_loss + config['variance_weight'] * variance_loss
        
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step(total_loss)
        
        current_loss = total_loss.item()
        
        if verbose and (epoch + 1) % 100 == 0:
            pred_std = torch.std(output.mean).item()
            print(f"Epoch {epoch+1}: GP Loss={gp_loss.item():.4f}, "
                  f"Var Loss={variance_loss.item():.4f}, "
                  f"Pred Std={pred_std:.4f}, Target Std={target_std.item():.4f}")
        
        # Early stopping
        if current_loss < best_loss:
            best_loss = current_loss
            patience_counter = 0
            best_state = {
                'model': model.state_dict(),
                'likelihood': likelihood.state_dict(),
            }
        else:
            patience_counter += 1
        
        if patience_counter >= config['patience']:
            if verbose:
                print(f"æ—©åœ at Epoch {epoch+1}")
            break
    
    # è¼‰å…¥æœ€ä½³æ¨¡å‹
    model.load_state_dict(best_state['model'])
    likelihood.load_state_dict(best_state['likelihood'])
    
    if verbose:
        print(f"è¨“ç·´å®Œæˆ")
    
    return model, likelihood, scaler_x, scaler_y


def evaluate_model(model, likelihood, X_test, y_test, scaler_x, scaler_y, verbose=True):
    """è©•ä¼°æ¨¡å‹"""
    model.eval()
    likelihood.eval()
    
    X_test_scaled = scaler_x.transform(X_test)
    test_x = torch.from_numpy(X_test_scaled).to(device)
    
    with torch.no_grad(), gpytorch.settings.fast_pred_var():
        pred_dist = likelihood(model(test_x))
        y_pred_scaled = pred_dist.mean.cpu().numpy()
        y_std_scaled = pred_dist.stddev.cpu().numpy()
    
    # åæ¨™æº–åŒ–
    y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
    y_std = y_std_scaled * scaler_y.scale_[0]
    
    # è¨ˆç®—æŒ‡æ¨™
    relative_errors = np.abs((y_test - y_pred) / y_test) * 100
    
    mape = np.mean(relative_errors)
    mae = np.mean(np.abs(y_test - y_pred))
    max_error = np.max(relative_errors)
    
    outliers_20 = np.sum(relative_errors > 20)
    outliers_15 = np.sum(relative_errors > 15)
    outliers_10 = np.sum(relative_errors > 10)
    
    # Type 3 åˆ†æ
    type3_mask = X_test[:, 0] == 3
    if np.sum(type3_mask) > 0:
        type3_errors = relative_errors[type3_mask]
        type3_mape = np.mean(type3_errors)
        type3_outliers = np.sum(type3_errors > 20)
        
        # Coverage 0.8 åˆ†æ
        cov08_mask = type3_mask & (X_test[:, 2] == 0.8)
        if np.sum(cov08_mask) > 0:
            cov08_errors = relative_errors[cov08_mask]
            cov08_mape = np.mean(cov08_errors)
            cov08_outliers = np.sum(cov08_errors > 20)
            
            # æª¢æŸ¥ variance collapse
            cov08_pred = y_pred[cov08_mask]
            cov08_true = y_test[cov08_mask]
            
            pred_std = np.std(cov08_pred)
            true_std = np.std(cov08_true)
            compression_ratio = pred_std / true_std if true_std > 0 else 0
            
            if verbose:
                print(f"\n{'='*60}")
                print("Coverage 0.8 è©³ç´°åˆ†æ")
                print(f"{'='*60}")
                print(f"  çœŸå¯¦å€¼ Std: {true_std:.4f}")
                print(f"  é æ¸¬å€¼ Std: {pred_std:.4f}")
                print(f"  å£“ç¸®æ¯”: {compression_ratio:.3f} {'âŒ éåº¦å£“ç¸®' if compression_ratio < 0.5 else 'âœ“ åˆç†'}")
                print(f"\n  è©³ç´°é æ¸¬:")
                for i in range(len(cov08_true)):
                    marker = "âŒ" if cov08_errors[i] > 20 else "âœ“"
                    print(f"    {marker} Thick={X_test[cov08_mask][i, 1]:.0f}, "
                          f"True={cov08_true[i]:.3f}, Pred={cov08_pred[i]:.3f}, "
                          f"Error={cov08_errors[i]:.1f}%")
        else:
            cov08_mape = 0
            cov08_outliers = 0
    else:
        type3_mape = 0
        type3_outliers = 0
        cov08_mape = 0
        cov08_outliers = 0
    
    if verbose:
        print(f"\n{'='*60}")
        print(f"è©•ä¼°çµæœ")
        print(f"{'='*60}")
        print(f"æ¨£æœ¬æ•¸: {len(y_test)}")
        print(f"\næº–ç¢ºåº¦:")
        print(f"  MAPE:      {mape:.2f}%")
        print(f"  MAE:       {mae:.4f}")
        print(f"  Max Error: {max_error:.2f}%")
        print(f"\nç•°å¸¸é»:")
        print(f"  >20%: {outliers_20}/{len(y_test)} ({outliers_20/len(y_test)*100:.2f}%)")
        print(f"  >15%: {outliers_15}/{len(y_test)} ({outliers_15/len(y_test)*100:.2f}%)")
        print(f"  >10%: {outliers_10}/{len(y_test)} ({outliers_10/len(y_test)*100:.2f}%)")
        
        if np.sum(type3_mask) > 0:
            print(f"\nType 3 è©³ç´°:")
            print(f"  æ¨£æœ¬æ•¸: {np.sum(type3_mask)}")
            print(f"  MAPE: {type3_mape:.2f}%")
            print(f"  ç•°å¸¸é»: {type3_outliers}/{np.sum(type3_mask)}")
            if np.sum(cov08_mask) > 0:
                print(f"\n  Coverage 0.8 å­é›†:")
                print(f"    MAPE: {cov08_mape:.2f}%")
                print(f"    ç•°å¸¸é»: {cov08_outliers}/{np.sum(cov08_mask)}")
        print(f"{'='*60}\n")
    
    results = {
        'mape': mape,
        'mae': mae,
        'max_error': max_error,
        'outliers_20': outliers_20,
        'type3_mape': type3_mape,
        'type3_outliers': type3_outliers,
        'cov08_mape': cov08_mape,
        'cov08_outliers': cov08_outliers,
        'predictions': y_pred,
        'std': y_std,
        'errors': relative_errors
    }
    
    return results


def save_predictions(X_test, y_test, results, filename):
    """ä¿å­˜é æ¸¬çµæœ"""
    df = pd.DataFrame({
        'TIM_TYPE': X_test[:, 0],
        'TIM_THICKNESS': X_test[:, 1],
        'TIM_COVERAGE': X_test[:, 2],
        'True': y_test,
        'Predicted': results['predictions'],
        'Error%': results['errors'],
        'Std': results['std']
    })
    
    df.to_csv(filename, index=False)
    print(f"âœ“ é æ¸¬çµæœå·²ä¿å­˜åˆ°: {filename}")


# ==========================================
# ä¸»å‡½æ•¸
# ==========================================

def main(seed=2024, verbose=True):
    """ä¸»è¨“ç·´æµç¨‹"""
    clear_gpu_cache()
    set_seed(seed)
    
    print(f"\nä½¿ç”¨è£ç½®: {device}\n")
    
    print("="*60)
    print("Phase 2H: å°æŠ— Variance Collapse")
    print("="*60)
    
    # ç‰¹å¾µå’Œç›®æ¨™
    feature_cols = ['TIM_TYPE', 'TIM_THICKNESS', 'TIM_COVERAGE']
    target_col = 'Theta.JC'
    
    # é…ç½®
    config = {
        'hidden_dims': [128, 128, 64, 32],
        'feature_dim': 16,
        'dropout': 0.1,
        'lr': 0.01,
        'epochs': 600,
        'patience': 60,
        'variance_weight': 0.5,  # æ–¹å·®æ‡²ç½°æ¬Šé‡
    }
    
    if verbose:
        print(f"\né…ç½®:")
        for key, value in config.items():
            print(f"  {key}: {value}")
    
    # è¼‰å…¥è³‡æ–™
    train_above = pd.read_excel('data/train/Above.xlsx')
    test_above = pd.read_excel('data/test/Above.xlsx')
    
    # è¨“ç·´é›†æ¸…ç†
    train_above_clean = train_above.groupby(feature_cols, as_index=False).agg({
        target_col: 'mean'
    })
    
    print(f"\nè¨“ç·´é›†: {len(train_above_clean)} ç­†")
    print(f"æ¸¬è©¦é›†: {len(test_above)} ç­†")
    
    X_train = train_above_clean[feature_cols].values
    y_train = train_above_clean[target_col].values
    
    X_test = test_above[feature_cols].values
    y_test = test_above[target_col].values
    
    # è¨“ç·´
    model, likelihood, scaler_x, scaler_y = train_anti_collapse_model(
        X_train, y_train, config, verbose=verbose
    )
    
    # è©•ä¼°
    results = evaluate_model(
        model, likelihood, X_test, y_test, 
        scaler_x, scaler_y, verbose=verbose
    )
    
    # ä¿å­˜é æ¸¬çµæœ
    save_predictions(X_test, y_test, results,
                     f'phase2h_anti_collapse_seed{seed}_predictions.csv')
    
    # ç¸½çµ
    print("\n" + "="*60)
    print("æœ€çµ‚çµæœç¸½çµ (Phase 2H)")
    print("="*60)
    print(f"ç­–ç•¥:")
    print(f"  âœ“ å›ºå®šæ¥µå°å™ªéŸ³ (1e-4)")
    print(f"  âœ“ æ•æ„Ÿ kernel (å° lengthscale)")
    print(f"  âœ“ æ–¹å·®æ‡²ç½° (ä¿æŒè®Šç•°æ€§)")
    print(f"  âœ“ æ›´æ·±ç¶²è·¯ (128-128-64-32)")
    print(f"\nçµæœ:")
    print(f"  ç¸½é«” MAPE: {results['mape']:.2f}%")
    print(f"  Type 3 MAPE: {results['type3_mape']:.2f}%")
    print(f"  Coverage 0.8 MAPE: {results['cov08_mape']:.2f}%")
    print(f"  ç•°å¸¸é»: {results['outliers_20']}/{len(y_test)}")
    print("="*60 + "\n")
    
    return results


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Phase 2H å°æŠ— Variance Collapse')
    parser.add_argument('--seed', type=int, default=2024, help='éš¨æ©Ÿç¨®å­')
    parser.add_argument('-v', '--verbose', action='store_true', help='è©³ç´°æ¨¡å¼')
    
    args = parser.parse_args()
    
    results = main(seed=args.seed, verbose=args.verbose)
    
    print("\nğŸ’¡ èªªæ˜:")
    print("  æ­¤ç‰ˆæœ¬é‡å° Variance Collapse å•é¡Œ")
    print("  ç›®æ¨™ï¼šå£“ç¸®æ¯”å¾ 0.29 æé«˜åˆ° >0.6\n")
