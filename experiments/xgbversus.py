"""
XGBoost Baseline - èˆ‡ Phase 2B DKL å°ç…§å¯¦é©—
ä½¿ç”¨ç›¸åŒçš„è©•ä¼°æŒ‡æ¨™å’Œè³‡æ–™é›†
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
import warnings
import argparse

warnings.filterwarnings('ignore')


def set_seed(seed):
    """è¨­ç½®éš¨æ©Ÿç¨®å­"""
    np.random.seed(seed)
    print(f"âœ“ éš¨æ©Ÿç¨®å­è¨­å®šç‚º: {seed}")


def compute_sample_weights(X, weight_factor=3.0):
    """
    è¨ˆç®—æ¨£æœ¬æ¬Šé‡ï¼ˆèˆ‡ Phase 2B ç›¸åŒï¼‰
    å›°é›£æ¨£æœ¬å®šç¾©: TIM_TYPE=3 AND Coverage=0.8 AND THICKNESS>=220
    """
    weights = np.ones(len(X))
    
    difficult_mask = (
        (X[:, 0] == 3) &      # TIM_TYPE = 3
        (X[:, 2] == 0.8) &    # TIM_COVERAGE = 0.8
        (X[:, 1] >= 220)      # TIM_THICKNESS >= 220
    )
    
    weights[difficult_mask] *= weight_factor
    
    return weights


def train_xgboost(X_train, y_train, use_weights=True, weight_factor=3.0, 
                  tune_hyperparams=False, seed=2024, verbose=True):
    """
    è¨“ç·´ XGBoost æ¨¡å‹
    
    Args:
        X_train: è¨“ç·´ç‰¹å¾µ
        y_train: è¨“ç·´æ¨™ç±¤
        use_weights: æ˜¯å¦ä½¿ç”¨æ¨£æœ¬æ¬Šé‡
        weight_factor: æ¬Šé‡å€æ•¸
        tune_hyperparams: æ˜¯å¦é€²è¡Œè¶…åƒæ•¸æœç´¢
        seed: éš¨æ©Ÿç¨®å­
        verbose: æ˜¯å¦é¡¯ç¤ºè¨“ç·´éç¨‹
        
    Returns:
        model: è¨“ç·´å¥½çš„ XGBoost æ¨¡å‹
    """
    
    # è¨ˆç®—æ¨£æœ¬æ¬Šé‡
    if use_weights:
        sample_weights = compute_sample_weights(X_train, weight_factor)
        if verbose:
            difficult_count = np.sum(sample_weights > 1.0)
            print(f"\nè¨ˆç®—æ¨£æœ¬æ¬Šé‡:")
            print(f"  å›°é›£æ¨£æœ¬æ•¸: {difficult_count} ({difficult_count/len(X_train)*100:.2f}%)")
            print(f"  æ¬Šé‡å€æ•¸: {weight_factor}x")
    else:
        sample_weights = None
        if verbose:
            print(f"\nä¸ä½¿ç”¨æ¨£æœ¬æ¬Šé‡")
    
    if tune_hyperparams:
        # è¶…åƒæ•¸æœç´¢
        if verbose:
            print(f"\né€²è¡Œè¶…åƒæ•¸æœç´¢...")
        
        param_grid = {
            'max_depth': [3, 5, 7, 9],
            'learning_rate': [0.01, 0.05, 0.1],
            'n_estimators': [100, 200, 300, 500],
            'min_child_weight': [1, 3, 5],
            'subsample': [0.7, 0.8, 0.9, 1.0],
            'colsample_bytree': [0.7, 0.8, 0.9, 1.0],
            'gamma': [0, 0.1, 0.2],
        }
        
        xgb_model = xgb.XGBRegressor(
            random_state=seed,
            tree_method='hist'
        )
        
        grid_search = GridSearchCV(
            xgb_model, 
            param_grid, 
            cv=5,
            scoring='neg_mean_absolute_percentage_error',
            n_jobs=-1,
            verbose=1 if verbose else 0
        )
        
        grid_search.fit(X_train, y_train, sample_weight=sample_weights)
        
        if verbose:
            print(f"\næœ€ä½³åƒæ•¸:")
            for key, value in grid_search.best_params_.items():
                print(f"  {key}: {value}")
        
        model = grid_search.best_estimator_
    
    else:
        # ä½¿ç”¨é è¨­è‰¯å¥½åƒæ•¸
        params = {
            'max_depth': 7,
            'learning_rate': 0.05,
            'n_estimators': 300,
            'min_child_weight': 3,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'gamma': 0.1,
            'random_state': seed,
            'tree_method': 'hist',
            'n_jobs': -1,
        }
        
        if verbose:
            print(f"\nä½¿ç”¨åƒæ•¸:")
            for key, value in params.items():
                if key not in ['random_state', 'tree_method', 'n_jobs']:
                    print(f"  {key}: {value}")
        
        model = xgb.XGBRegressor(**params)
        
        # è¨“ç·´
        if verbose:
            print(f"\né–‹å§‹è¨“ç·´...")
        
        model.fit(
            X_train, y_train,
            sample_weight=sample_weights,
            eval_set=[(X_train, y_train)],
            verbose=50 if verbose else 0
        )
    
    if verbose:
        print(f"è¨“ç·´å®Œæˆ")
    
    return model


def evaluate_model(model, X_test, y_test, verbose=True):
    """
    è©•ä¼°æ¨¡å‹ï¼ˆèˆ‡ Phase 2B ç›¸åŒçš„è©•ä¼°é‚è¼¯ï¼‰
    
    Returns:
        results: åŒ…å« MAPE, outliers ç­‰æŒ‡æ¨™çš„å­—å…¸
    """
    # é æ¸¬
    y_pred = model.predict(X_test)
    
    # è¨ˆç®—æŒ‡æ¨™ï¼ˆåœ¨åŸå§‹ç©ºé–“ï¼‰
    relative_errors = np.abs((y_test - y_pred) / y_test) * 100
    
    mape = np.mean(relative_errors)
    mae = np.mean(np.abs(y_test - y_pred))
    max_error = np.max(relative_errors)
    
    outliers_20 = np.sum(relative_errors > 20)
    outliers_15 = np.sum(relative_errors > 15)
    outliers_10 = np.sum(relative_errors > 10)
    
    # Type 3 åˆ†æ
    type3_mask = X_test[:, 0] == 3
    type3_outliers = np.sum((relative_errors > 20) & type3_mask)
    
    if verbose:
        print(f"\n{'='*60}")
        print(f"è©•ä¼°çµæœ")
        print(f"{'='*60}")
        print(f"æ¨£æœ¬æ•¸: {len(y_test)}")
        print(f"\næº–ç¢ºåº¦:")
        print(f"  MAPE:      {mape:.2f}%")
        print(f"  MAE:       {mae:.4f}")
        print(f"  Max Error: {max_error:.2f}%")
        print(f"\nç•°å¸¸é»:")
        print(f"  >20%: {outliers_20}/{len(y_test)} ({outliers_20/len(y_test)*100:.2f}%)")
        print(f"  >15%: {outliers_15}/{len(y_test)} ({outliers_15/len(y_test)*100:.2f}%)")
        print(f"  >10%: {outliers_10}/{len(y_test)} ({outliers_10/len(y_test)*100:.2f}%)")
        if np.sum(type3_mask) > 0:
            print(f"\nType 3ç•°å¸¸é»: {type3_outliers}/{np.sum(type3_mask)}")
        print(f"{'='*60}\n")
    
    results = {
        'mape': mape,
        'mae': mae,
        'max_error': max_error,
        'outliers_20': outliers_20,
        'outliers_15': outliers_15,
        'outliers_10': outliers_10,
        'type3_outliers': type3_outliers,
        'predictions': y_pred,
        'errors': relative_errors
    }
    
    return results


def save_predictions(X_test, y_test, results, filename):
    """ä¿å­˜é æ¸¬çµæœåˆ° CSV"""
    df = pd.DataFrame({
        'TIM_TYPE': X_test[:, 0],
        'TIM_THICKNESS': X_test[:, 1],
        'TIM_COVERAGE': X_test[:, 2],
        'True': y_test,
        'Predicted': results['predictions'],
        'Error%': results['errors']
    })
    
    df.to_csv(filename, index=False)
    print(f"âœ“ é æ¸¬çµæœå·²ä¿å­˜åˆ°: {filename}")


def main(seed=2024, use_weights=True, weight_factor=3.0, 
         tune_hyperparams=False, verbose=True):
    """
    ä¸»è¨“ç·´æµç¨‹
    
    Args:
        seed: éš¨æ©Ÿç¨®å­
        use_weights: æ˜¯å¦ä½¿ç”¨æ¨£æœ¬æ¬Šé‡
        weight_factor: æ¬Šé‡å€æ•¸
        tune_hyperparams: æ˜¯å¦é€²è¡Œè¶…åƒæ•¸æœç´¢
        verbose: æ˜¯å¦é¡¯ç¤ºè©³ç´°ä¿¡æ¯
    """
    set_seed(seed)
    
    print("="*60)
    print("XGBoost Baseline - èˆ‡ DKL å°ç…§å¯¦é©—")
    print("="*60)
    
    # ç‰¹å¾µå’Œç›®æ¨™
    feature_cols = ['TIM_TYPE', 'TIM_THICKNESS', 'TIM_COVERAGE']
    target_col = 'Theta.JC'
    
    # ==========================================
    # Above Dataset
    # ==========================================
    
    print(f"\n\n{'ğŸ”µ Above 50% Coverage'}\n")
    
    # è¼‰å…¥è³‡æ–™
    train_above = pd.read_excel('data/train/Above.xlsx')
    test_above = pd.read_excel('data/test/Above.xlsx')
    
    # è¨“ç·´é›†æ¸…ç†ï¼ˆå»é™¤é‡è¤‡ï¼Œå–å¹³å‡ï¼‰
    train_above_clean = train_above.groupby(feature_cols, as_index=False).agg({
        target_col: 'mean'
    })
    
    print(f"è¨“ç·´é›†: {len(train_above_clean)} ç­†")
    print(f"æ¸¬è©¦é›†: {len(test_above)} ç­†")
    
    X_train_above = train_above_clean[feature_cols].values
    y_train_above = train_above_clean[target_col].values
    
    X_test_above = test_above[feature_cols].values
    y_test_above = test_above[target_col].values
    
    # è¨“ç·´
    model_above = train_xgboost(
        X_train_above, y_train_above,
        use_weights=use_weights,
        weight_factor=weight_factor,
        tune_hyperparams=tune_hyperparams,
        seed=seed,
        verbose=verbose
    )
    
    # è©•ä¼°
    results_above = evaluate_model(
        model_above,
        X_test_above, y_test_above,
        verbose=verbose
    )
    
    # ä¿å­˜é æ¸¬çµæœ
    save_predictions(X_test_above, y_test_above, results_above,
                     f'xgboost_above_seed{seed}_predictions.csv')
    
    # ==========================================
    # Below Dataset
    # ==========================================
    
    print(f"\n\n{'ğŸ”µ Below 50% Coverage'}\n")
    
    # è¼‰å…¥è³‡æ–™
    train_below = pd.read_excel('data/train/Below.xlsx')
    test_below = pd.read_excel('data/test/Below.xlsx')
    
    # è¨“ç·´é›†æ¸…ç†
    train_below_clean = train_below.groupby(feature_cols, as_index=False).agg({
        target_col: 'mean'
    })
    
    print(f"è¨“ç·´é›†: {len(train_below_clean)} ç­†")
    print(f"æ¸¬è©¦é›†: {len(test_below)} ç­†")
    
    X_train_below = train_below_clean[feature_cols].values
    y_train_below = train_below_clean[target_col].values
    
    X_test_below = test_below[feature_cols].values
    y_test_below = test_below[target_col].values
    
    # è¨“ç·´
    model_below = train_xgboost(
        X_train_below, y_train_below,
        use_weights=use_weights,
        weight_factor=weight_factor,
        tune_hyperparams=tune_hyperparams,
        seed=seed,
        verbose=verbose
    )
    
    # è©•ä¼°
    results_below = evaluate_model(
        model_below,
        X_test_below, y_test_below,
        verbose=verbose
    )
    
    # ä¿å­˜é æ¸¬çµæœ
    save_predictions(X_test_below, y_test_below, results_below,
                     f'xgboost_below_seed{seed}_predictions.csv')
    
    # ==========================================
    # ç¸½çµ
    # ==========================================
    
    print("\n" + "="*60)
    print("XGBoost æœ€çµ‚çµæœç¸½çµ")
    print("="*60)
    print(f"éš¨æ©Ÿç¨®å­: {seed}")
    print(f"æ¨£æœ¬æ¬Šé‡: {'å•Ÿç”¨' if use_weights else 'åœç”¨'} (factor={weight_factor})")
    
    print(f"\nAboveè³‡æ–™é›†:")
    print(f"  ç•°å¸¸é» (>20%): {results_above['outliers_20']}/{len(y_test_above)} ({results_above['outliers_20']/len(y_test_above)*100:.2f}%)")
    print(f"  MAPE: {results_above['mape']:.2f}%")
    print(f"  Type 3ç•°å¸¸é»: {results_above['type3_outliers']}")
    
    print(f"\nBelowè³‡æ–™é›†:")
    print(f"  ç•°å¸¸é» (>20%): {results_below['outliers_20']}/{len(y_test_below)} ({results_below['outliers_20']/len(y_test_below)*100:.2f}%)")
    print(f"  MAPE: {results_below['mape']:.2f}%")
    
    print("\n" + "="*60)
    print("âœ“ XGBoost è¨“ç·´å®Œæˆï¼")
    print("="*60 + "\n")
    
    return {
        'above': results_above,
        'below': results_below,
        'seed': seed
    }


if __name__ == "__main__":
    # å‘½ä»¤è¡Œåƒæ•¸è§£æ
    parser = argparse.ArgumentParser(description='XGBoost Baseline')
    parser.add_argument('--seed', type=int, default=2024,
                        help='éš¨æ©Ÿç¨®å­ (é è¨­: 2024)')
    parser.add_argument('--no-weights', action='store_true',
                        help='åœç”¨æ¨£æœ¬æ¬Šé‡')
    parser.add_argument('--weight-factor', type=float, default=3.0,
                        help='æ¨£æœ¬æ¬Šé‡å€æ•¸ (é è¨­: 3.0)')
    parser.add_argument('--tune', action='store_true',
                        help='é€²è¡Œè¶…åƒæ•¸æœç´¢')
    parser.add_argument('-v', '--verbose', action='store_true',
                        help='é¡¯ç¤ºè©³ç´°è¨“ç·´éç¨‹')
    
    args = parser.parse_args()
    
    # é‹è¡Œè¨“ç·´
    results = main(
        seed=args.seed,
        use_weights=not args.no_weights,
        weight_factor=args.weight_factor,
        tune_hyperparams=args.tune,
        verbose=args.verbose
    )
    
    print("\nğŸ’¡ ä½¿ç”¨ç¯„ä¾‹:")
    print("  python xgboost_baseline.py                    # åŸºæœ¬ç‰ˆæœ¬")
    print("  python xgboost_baseline.py --seed 42 -v       # æŒ‡å®šç¨®å­ï¼Œè©³ç´°æ¨¡å¼")
    print("  python xgboost_baseline.py --no-weights       # ä¸ä½¿ç”¨æ¨£æœ¬æ¬Šé‡")
    print("  python xgboost_baseline.py --tune             # è¶…åƒæ•¸æœç´¢ï¼ˆè¼ƒæ…¢ï¼‰\n")