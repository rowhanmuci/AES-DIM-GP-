"""
Phase 2E: æŸ¥è¡¨æ³• + æ®˜å·®å­¸ç¿’
é‡å° Type 3 çš„ç‰¹æ®Šè™•ç†ï¼š
1. Type 3 åªä½¿ç”¨ Coverage åšä¸»é æ¸¬ï¼ˆThickness ç„¡é—œï¼‰
2. ç”¨æŸ¥è¡¨æ³•ç²å– Coverage å°æ‡‰çš„å¹³å‡å€¼
3. ç”¨è¼•é‡ GP æ¨¡å‹å­¸ç¿’æ®˜å·®ï¼ˆæ•æ‰å€‹é«”å·®ç•°ï¼‰

ä½¿ç”¨æ–¹æ³•:
    python phase2e_lookup_residual.py --seed 2024
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import gpytorch
from sklearn.preprocessing import StandardScaler
from scipy.interpolate import interp1d
import warnings
import random
import os
import argparse

warnings.filterwarnings('ignore')

torch.set_default_dtype(torch.float64)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def set_seed(seed):
    """è¨­ç½®éš¨æ©Ÿç¨®å­"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)
    print(f"âœ“ éš¨æ©Ÿç¨®å­è¨­å®šç‚º: {seed}")


def clear_gpu_cache():
    """æ¸…ç©ºGPUå¿«å–"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()


# ==========================================
# Type 3 æŸ¥è¡¨æ¨¡å‹
# ==========================================

class Type3LookupModel:
    """
    Type 3 å°ˆç”¨æŸ¥è¡¨æ¨¡å‹
    
    ç­–ç•¥ï¼š
    1. å»ºç«‹ Coverage â†’ Theta.JC çš„æŸ¥æ‰¾è¡¨ï¼ˆå¿½ç•¥ Thicknessï¼‰
    2. ä½¿ç”¨åˆ†ä½æ•¸å›æ­¸æ•æ‰ä¸ç¢ºå®šæ€§
    """
    
    def __init__(self, X_train, y_train):
        """
        Args:
            X_train: [TIM_TYPE, TIM_THICKNESS, TIM_COVERAGE]
            y_train: Theta.JC
        """
        # åªç”¨ Type 3 è³‡æ–™
        type3_mask = X_train[:, 0] == 3
        coverage = X_train[type3_mask, 2]
        theta = y_train[type3_mask]
        
        # æŒ‰ Coverage åˆ†çµ„çµ±è¨ˆ
        coverage_unique = np.unique(coverage)
        
        self.lookup_table = {}
        for cov in coverage_unique:
            cov_mask = coverage == cov
            cov_theta = theta[cov_mask]
            
            self.lookup_table[cov] = {
                'mean': np.mean(cov_theta),
                'median': np.median(cov_theta),
                'q25': np.percentile(cov_theta, 25),
                'q75': np.percentile(cov_theta, 75),
                'std': np.std(cov_theta),
                'min': np.min(cov_theta),
                'max': np.max(cov_theta),
                'count': len(cov_theta),
            }
        
        # å»ºç«‹æ’å€¼å‡½æ•¸ï¼ˆç”¨æ–¼æœªè¦‹éçš„ Coverage å€¼ï¼‰
        coverages = sorted(self.lookup_table.keys())
        means = [self.lookup_table[c]['mean'] for c in coverages]
        medians = [self.lookup_table[c]['median'] for c in coverages]
        
        self.interp_mean = interp1d(coverages, means, kind='cubic', 
                                    fill_value='extrapolate')
        self.interp_median = interp1d(coverages, medians, kind='cubic',
                                      fill_value='extrapolate')
        
        print(f"âœ“ Type 3 æŸ¥è¡¨æ¨¡å‹å·²å»ºç«‹ ({len(coverages)} å€‹ Coverage å€¼)")
    
    def predict(self, X_test, use_median=False):
        """
        é æ¸¬
        
        Args:
            X_test: [TIM_TYPE, TIM_THICKNESS, TIM_COVERAGE]
            use_median: æ˜¯å¦ä½¿ç”¨ä¸­ä½æ•¸ï¼ˆæ›´ç©©å¥ï¼‰
        
        Returns:
            predictions, std
        """
        type3_mask = X_test[:, 0] == 3
        coverage = X_test[type3_mask, 2]
        
        predictions = np.zeros(len(X_test))
        stds = np.zeros(len(X_test))
        
        for i, cov in enumerate(coverage):
            if cov in self.lookup_table:
                # ç›´æ¥æŸ¥è¡¨
                predictions[type3_mask][i] = (
                    self.lookup_table[cov]['median'] if use_median 
                    else self.lookup_table[cov]['mean']
                )
                stds[type3_mask][i] = self.lookup_table[cov]['std']
            else:
                # æ’å€¼
                predictions[type3_mask][i] = (
                    self.interp_median(cov) if use_median
                    else self.interp_mean(cov)
                )
                # ä¼°è¨ˆæ¨™æº–å·®ï¼ˆç”¨æœ€è¿‘çš„ Coverageï¼‰
                nearest_cov = min(self.lookup_table.keys(), 
                                 key=lambda x: abs(x - cov))
                stds[type3_mask][i] = self.lookup_table[nearest_cov]['std']
        
        return predictions, stds


# ==========================================
# ç°¡åŒ–çš„ DKL æ¨¡å‹ï¼ˆç”¨æ–¼ Type 1, 2 å’Œæ®˜å·®å­¸ç¿’ï¼‰
# ==========================================

class SimpleDnnFeatureExtractor(nn.Module):
    """è¼•é‡ DNN ç‰¹å¾µæå–å™¨"""
    
    def __init__(self, input_dim, hidden_dims=[32, 16], output_dim=4, dropout=0.1):
        super().__init__()
        
        layers = []
        prev_dim = input_dim
        
        for h_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, h_dim),
                nn.BatchNorm1d(h_dim),
                nn.ReLU(),
                nn.Dropout(dropout)
            ])
            prev_dim = h_dim
        
        layers.append(nn.Linear(prev_dim, output_dim))
        
        self.network = nn.Sequential(*layers)
        self.output_dim = output_dim
    
    def forward(self, x):
        return self.network(x)


class SimpleGPRegressionModel(gpytorch.models.ExactGP):
    """ç°¡åŒ– GP æ¨¡å‹"""
    
    def __init__(self, train_x, train_y, likelihood, feature_extractor):
        super().__init__(train_x, train_y, likelihood)
        
        self.feature_extractor = feature_extractor
        self.mean_module = gpytorch.means.ConstantMean()
        
        # ç°¡å–® RBF kernelï¼ˆé«˜æ•æ„Ÿåº¦ï¼‰
        self.covar_module = gpytorch.kernels.ScaleKernel(
            gpytorch.kernels.RBFKernel(
                ard_num_dims=feature_extractor.output_dim,
                lengthscale_constraint=gpytorch.constraints.Interval(0.1, 2.0)  # é™åˆ¶ lengthscale
            )
        )
    
    def forward(self, x):
        projected_x = self.feature_extractor(x)
        mean_x = self.mean_module(projected_x)
        covar_x = self.covar_module(projected_x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)


# ==========================================
# Type 3 æ®˜å·®æ¨¡å‹
# ==========================================

class Type3ResidualModel:
    """
    Type 3 æ®˜å·®å­¸ç¿’æ¨¡å‹
    
    æ­¥é©Ÿï¼š
    1. ç”¨æŸ¥è¡¨æ³•ç²å–åŸºç¤é æ¸¬
    2. è¨ˆç®—æ®˜å·®
    3. ç”¨ GP å­¸ç¿’æ®˜å·®æ¨¡å¼ï¼ˆCoverage, Thickness ä½œç‚ºè¼”åŠ©ç‰¹å¾µï¼‰
    """
    
    def __init__(self, X_train, y_train, lookup_model, config):
        """
        Args:
            X_train: Type 3 è¨“ç·´è³‡æ–™
            y_train: Type 3 è¨“ç·´æ¨™ç±¤
            lookup_model: å·²è¨“ç·´çš„æŸ¥è¡¨æ¨¡å‹
            config: é…ç½®
        """
        # ç²å–æŸ¥è¡¨åŸºç¤é æ¸¬
        base_pred, _ = lookup_model.predict(X_train)
        
        # è¨ˆç®—æ®˜å·®
        type3_mask = X_train[:, 0] == 3
        residuals = y_train[type3_mask] - base_pred[type3_mask]
        
        print(f"æ®˜å·®çµ±è¨ˆ: mean={np.mean(residuals):.4f}, std={np.std(residuals):.4f}")
        
        # ç‰¹å¾µï¼šåªç”¨ Coverage å’Œ Thicknessï¼ˆç·¨ç¢¼å€‹é«”å·®ç•°ï¼‰
        X_residual = X_train[type3_mask][:, 1:]  # [Thickness, Coverage]
        
        # æ¨™æº–åŒ–
        self.scaler_x = StandardScaler()
        self.scaler_y = StandardScaler()
        
        X_scaled = self.scaler_x.fit_transform(X_residual)
        y_scaled = self.scaler_y.fit_transform(residuals.reshape(-1, 1)).flatten()
        
        train_x = torch.from_numpy(X_scaled).to(device)
        train_y = torch.from_numpy(y_scaled).to(device)
        
        # å»ºç«‹è¼•é‡æ¨¡å‹
        feature_extractor = SimpleDnnFeatureExtractor(
            input_dim=2,
            hidden_dims=[16, 8],
            output_dim=4,
            dropout=0.05
        ).to(device)
        
        self.likelihood = gpytorch.likelihoods.GaussianLikelihood().to(device)
        self.model = SimpleGPRegressionModel(
            train_x, train_y, self.likelihood, feature_extractor
        ).to(device)
        
        # è¨“ç·´
        self._train(train_x, train_y, config)
    
    def _train(self, train_x, train_y, config):
        """è¨“ç·´æ®˜å·®æ¨¡å‹"""
        optimizer = optim.Adam([
            {'params': self.model.feature_extractor.parameters(), 'lr': 0.01},
            {'params': self.model.covar_module.parameters()},
            {'params': self.model.mean_module.parameters()},
            {'params': self.model.likelihood.parameters()},
        ], lr=0.01)
        
        mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model)
        
        self.model.train()
        self.likelihood.train()
        
        for epoch in range(200):  # å°‘é‡ epoch
            optimizer.zero_grad()
            output = self.model(train_x)
            loss = -mll(output, train_y)
            loss.backward()
            optimizer.step()
        
        print(f"âœ“ æ®˜å·®æ¨¡å‹è¨“ç·´å®Œæˆ")
    
    def predict(self, X_test):
        """é æ¸¬æ®˜å·®"""
        self.model.eval()
        self.likelihood.eval()
        
        type3_mask = X_test[:, 0] == 3
        X_residual = X_test[type3_mask][:, 1:]  # [Thickness, Coverage]
        
        X_scaled = self.scaler_x.transform(X_residual)
        test_x = torch.from_numpy(X_scaled).to(device)
        
        with torch.no_grad(), gpytorch.settings.fast_pred_var():
            pred_dist = self.likelihood(self.model(test_x))
            residual_scaled = pred_dist.mean.cpu().numpy()
        
        # åæ¨™æº–åŒ–
        residual = self.scaler_y.inverse_transform(residual_scaled.reshape(-1, 1)).flatten()
        
        predictions = np.zeros(len(X_test))
        predictions[type3_mask] = residual
        
        return predictions


# ==========================================
# æ··åˆæ¨¡å‹
# ==========================================

class HybridModel:
    """
    æ··åˆæ¨¡å‹ï¼š
    - Type 1, 2: æ¨™æº– DKL
    - Type 3: æŸ¥è¡¨æ³• + æ®˜å·®å­¸ç¿’
    """
    
    def __init__(self, X_train, y_train, config):
        """åˆå§‹åŒ–æ··åˆæ¨¡å‹"""
        # Type 3 æŸ¥è¡¨æ¨¡å‹
        self.lookup_model = Type3LookupModel(X_train, y_train)
        
        # Type 3 æ®˜å·®æ¨¡å‹
        type3_mask = X_train[:, 0] == 3
        X_type3 = X_train[type3_mask]
        y_type3 = y_train[type3_mask]
        
        self.residual_model = Type3ResidualModel(
            X_train, y_train, self.lookup_model, config
        )
        
        # Type 1, 2 æ¨™æº–æ¨¡å‹
        others_mask = ~type3_mask
        X_others = X_train[others_mask]
        y_others = y_train[others_mask]
        
        print(f"\nè¨“ç·´ Type 1, 2 æ¨¡å‹ ({len(X_others)} ç­†)...")
        self.standard_model = self._train_standard_model(X_others, y_others, config)
    
    def _train_standard_model(self, X_train, y_train, config):
        """è¨“ç·´ Type 1, 2 çš„æ¨™æº–æ¨¡å‹"""
        # æ¨™æº–åŒ–
        scaler_x = StandardScaler()
        scaler_y = StandardScaler()
        
        X_scaled = scaler_x.fit_transform(X_train)
        y_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
        
        train_x = torch.from_numpy(X_scaled).to(device)
        train_y = torch.from_numpy(y_scaled).to(device)
        
        # å»ºç«‹æ¨¡å‹
        feature_extractor = SimpleDnnFeatureExtractor(
            input_dim=3,
            hidden_dims=[64, 32],
            output_dim=8,
            dropout=0.1
        ).to(device)
        
        likelihood = gpytorch.likelihoods.GaussianLikelihood().to(device)
        model = SimpleGPRegressionModel(train_x, train_y, likelihood, feature_extractor).to(device)
        
        # è¨“ç·´
        optimizer = optim.Adam(model.parameters(), lr=0.01)
        mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)
        
        model.train()
        likelihood.train()
        
        for epoch in range(300):
            optimizer.zero_grad()
            output = model(train_x)
            loss = -mll(output, train_y)
            loss.backward()
            optimizer.step()
            
            if (epoch + 1) % 100 == 0:
                print(f"  Epoch {epoch+1}: Loss={loss.item():.4f}")
        
        print(f"âœ“ Type 1, 2 æ¨¡å‹è¨“ç·´å®Œæˆ")
        
        return {
            'model': model,
            'likelihood': likelihood,
            'scaler_x': scaler_x,
            'scaler_y': scaler_y
        }
    
    def predict(self, X_test):
        """æ··åˆé æ¸¬"""
        predictions = np.zeros(len(X_test))
        stds = np.zeros(len(X_test))
        
        # Type 3 é æ¸¬
        type3_mask = X_test[:, 0] == 3
        if np.sum(type3_mask) > 0:
            # æŸ¥è¡¨åŸºç¤é æ¸¬
            base_pred, base_std = self.lookup_model.predict(X_test, use_median=True)
            
            # æ®˜å·®é æ¸¬
            residual_pred = self.residual_model.predict(X_test)
            
            # çµ„åˆ
            predictions[type3_mask] = base_pred[type3_mask] + residual_pred[type3_mask]
            stds[type3_mask] = base_std[type3_mask]
        
        # Type 1, 2 é æ¸¬
        others_mask = ~type3_mask
        if np.sum(others_mask) > 0:
            model_dict = self.standard_model
            model = model_dict['model']
            likelihood = model_dict['likelihood']
            scaler_x = model_dict['scaler_x']
            scaler_y = model_dict['scaler_y']
            
            model.eval()
            likelihood.eval()
            
            X_scaled = scaler_x.transform(X_test[others_mask])
            test_x = torch.from_numpy(X_scaled).to(device)
            
            with torch.no_grad(), gpytorch.settings.fast_pred_var():
                pred_dist = likelihood(model(test_x))
                y_pred_scaled = pred_dist.mean.cpu().numpy()
                y_std_scaled = pred_dist.stddev.cpu().numpy()
            
            predictions[others_mask] = scaler_y.inverse_transform(
                y_pred_scaled.reshape(-1, 1)
            ).flatten()
            stds[others_mask] = y_std_scaled * scaler_y.scale_[0]
        
        return predictions, stds


# ==========================================
# è©•ä¼°å‡½æ•¸
# ==========================================

def evaluate_model(model, X_test, y_test, verbose=True):
    """è©•ä¼°æ¨¡å‹"""
    y_pred, y_std = model.predict(X_test)
    
    # è¨ˆç®—æŒ‡æ¨™
    relative_errors = np.abs((y_test - y_pred) / y_test) * 100
    
    mape = np.mean(relative_errors)
    mae = np.mean(np.abs(y_test - y_pred))
    max_error = np.max(relative_errors)
    
    outliers_20 = np.sum(relative_errors > 20)
    outliers_15 = np.sum(relative_errors > 15)
    outliers_10 = np.sum(relative_errors > 10)
    
    # Type 3 åˆ†æ
    type3_mask = X_test[:, 0] == 3
    if np.sum(type3_mask) > 0:
        type3_errors = relative_errors[type3_mask]
        type3_mape = np.mean(type3_errors)
        type3_outliers = np.sum(type3_errors > 20)
        
        # Coverage 0.8 åˆ†æ
        cov08_mask = type3_mask & (X_test[:, 2] == 0.8)
        if np.sum(cov08_mask) > 0:
            cov08_errors = relative_errors[cov08_mask]
            cov08_mape = np.mean(cov08_errors)
            cov08_outliers = np.sum(cov08_errors > 20)
        else:
            cov08_mape = 0
            cov08_outliers = 0
    else:
        type3_mape = 0
        type3_outliers = 0
        cov08_mape = 0
        cov08_outliers = 0
    
    if verbose:
        print(f"\n{'='*60}")
        print(f"è©•ä¼°çµæœ (æŸ¥è¡¨æ³• + æ®˜å·®å­¸ç¿’)")
        print(f"{'='*60}")
        print(f"æ¨£æœ¬æ•¸: {len(y_test)}")
        print(f"\næº–ç¢ºåº¦:")
        print(f"  MAPE:      {mape:.2f}%")
        print(f"  MAE:       {mae:.4f}")
        print(f"  Max Error: {max_error:.2f}%")
        print(f"\nç•°å¸¸é»:")
        print(f"  >20%: {outliers_20}/{len(y_test)} ({outliers_20/len(y_test)*100:.2f}%)")
        print(f"  >15%: {outliers_15}/{len(y_test)} ({outliers_15/len(y_test)*100:.2f}%)")
        print(f"  >10%: {outliers_10}/{len(y_test)} ({outliers_10/len(y_test)*100:.2f}%)")
        
        if np.sum(type3_mask) > 0:
            print(f"\nType 3 è©³ç´°:")
            print(f"  æ¨£æœ¬æ•¸: {np.sum(type3_mask)}")
            print(f"  MAPE: {type3_mape:.2f}%")
            print(f"  ç•°å¸¸é»: {type3_outliers}/{np.sum(type3_mask)}")
            if np.sum(cov08_mask) > 0:
                print(f"\n  Coverage 0.8 å­é›†:")
                print(f"    MAPE: {cov08_mape:.2f}%")
                print(f"    ç•°å¸¸é»: {cov08_outliers}/{np.sum(cov08_mask)}")
        print(f"{'='*60}\n")
    
    results = {
        'mape': mape,
        'mae': mae,
        'max_error': max_error,
        'outliers_20': outliers_20,
        'type3_mape': type3_mape,
        'type3_outliers': type3_outliers,
        'cov08_mape': cov08_mape,
        'cov08_outliers': cov08_outliers,
        'predictions': y_pred,
        'std': y_std,
        'errors': relative_errors
    }
    
    return results


def save_predictions(X_test, y_test, results, filename):
    """ä¿å­˜é æ¸¬çµæœ"""
    df = pd.DataFrame({
        'TIM_TYPE': X_test[:, 0],
        'TIM_THICKNESS': X_test[:, 1],
        'TIM_COVERAGE': X_test[:, 2],
        'True': y_test,
        'Predicted': results['predictions'],
        'Error%': results['errors'],
        'Std': results['std']
    })
    
    df.to_csv(filename, index=False)
    print(f"âœ“ é æ¸¬çµæœå·²ä¿å­˜åˆ°: {filename}")


# ==========================================
# ä¸»å‡½æ•¸
# ==========================================

def main(seed=2024, verbose=True):
    """ä¸»è¨“ç·´æµç¨‹"""
    clear_gpu_cache()
    set_seed(seed)
    
    print(f"\nä½¿ç”¨è£ç½®: {device}\n")
    
    print("="*60)
    print("Phase 2E: æŸ¥è¡¨æ³• + æ®˜å·®å­¸ç¿’")
    print("="*60)
    
    # ç‰¹å¾µå’Œç›®æ¨™
    feature_cols = ['TIM_TYPE', 'TIM_THICKNESS', 'TIM_COVERAGE']
    target_col = 'Theta.JC'
    
    # é…ç½®
    config = {
        'lr': 0.01,
        'epochs': 300,
        'seed': seed,
    }
    
    # è¼‰å…¥è³‡æ–™
    train_above = pd.read_excel('data/train/Above.xlsx')
    test_above = pd.read_excel('data/test/Above.xlsx')
    
    # è¨“ç·´é›†æ¸…ç†
    train_above_clean = train_above.groupby(feature_cols, as_index=False).agg({
        target_col: 'mean'
    })
    
    print(f"\nè¨“ç·´é›†: {len(train_above_clean)} ç­†")
    print(f"æ¸¬è©¦é›†: {len(test_above)} ç­†")
    
    X_train = train_above_clean[feature_cols].values
    y_train = train_above_clean[target_col].values
    
    X_test = test_above[feature_cols].values
    y_test = test_above[target_col].values
    
    # è¨“ç·´æ··åˆæ¨¡å‹
    print(f"\n{'='*60}")
    print("è¨“ç·´æ··åˆæ¨¡å‹")
    print(f"{'='*60}\n")
    
    model = HybridModel(X_train, y_train, config)
    
    # è©•ä¼°
    results = evaluate_model(model, X_test, y_test, verbose=verbose)
    
    # ä¿å­˜é æ¸¬çµæœ
    save_predictions(X_test, y_test, results,
                     f'phase2e_lookup_residual_seed{seed}_predictions.csv')
    
    # ç¸½çµ
    print("\n" + "="*60)
    print("æœ€çµ‚çµæœç¸½çµ (Phase 2E)")
    print("="*60)
    print(f"ç­–ç•¥:")
    print(f"  âœ“ Type 3: æŸ¥è¡¨æ³• (åªç”¨ Coverage)")
    print(f"  âœ“ Type 3: GP æ®˜å·®å­¸ç¿’ (æ•æ‰å€‹é«”å·®ç•°)")
    print(f"  âœ“ Type 1, 2: æ¨™æº– DKL")
    print(f"\nçµæœ:")
    print(f"  ç¸½é«” MAPE: {results['mape']:.2f}%")
    print(f"  Type 3 MAPE: {results['type3_mape']:.2f}%")
    print(f"  Coverage 0.8 MAPE: {results['cov08_mape']:.2f}%")
    print(f"  ç•°å¸¸é»: {results['outliers_20']}/{len(y_test)}")
    print("="*60 + "\n")
    
    return results


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Phase 2E æŸ¥è¡¨æ³• + æ®˜å·®å­¸ç¿’')
    parser.add_argument('--seed', type=int, default=2024, help='éš¨æ©Ÿç¨®å­')
    parser.add_argument('-v', '--verbose', action='store_true', help='è©³ç´°æ¨¡å¼')
    
    args = parser.parse_args()
    
    results = main(seed=args.seed, verbose=args.verbose)
    
    print("\nğŸ’¡ èªªæ˜:")
    print("  æ­¤ç‰ˆæœ¬é‡å° Type 3 ä½¿ç”¨æŸ¥è¡¨æ³• + æ®˜å·®å­¸ç¿’")
    print("  é æœŸ: Coverage 0.8 MAPE < 20%")
    print("        Type 3 ç•°å¸¸é» < 4/18\n")
