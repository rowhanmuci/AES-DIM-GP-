"""
Phase 4B - åˆ†æ²»è¨“ç·´ç­–ç•¥ (Divide and Conquer)
åˆ†åˆ¥è¨“ç·´ Type 1&2 å’Œ Type 3 æ¨¡å‹
"""

import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import gpytorch
from sklearn.preprocessing import StandardScaler
import warnings
import random
import os
import argparse
import time

warnings.filterwarnings('ignore')

torch.set_default_dtype(torch.float64)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def set_seed(seed):
    """è¨­ç½®éš¨æ©Ÿç¨®å­ä»¥ç¢ºä¿å®Œå…¨å¯é‡ç¾æ€§"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)
    print(f"âœ“ éš¨æ©Ÿç¨®å­è¨­å®šç‚º: {seed}")


def clear_gpu_cache():
    """æ¸…ç©ºGPUå¿«å–"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()


# ==========================================
# æ¨¡å‹å®šç¾©
# ==========================================

class DnnFeatureExtractor(nn.Module):
    """æ·±åº¦ç¥ç¶“ç¶²è·¯ç‰¹å¾µæå–å™¨"""
    
    def __init__(self, input_dim, hidden_dims=[64, 32, 16], output_dim=8, dropout=0.1):
        super().__init__()
        
        layers = []
        prev_dim = input_dim
        
        for h_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, h_dim),
                nn.BatchNorm1d(h_dim),
                nn.ReLU(),
                nn.Dropout(dropout)
            ])
            prev_dim = h_dim
        
        layers.append(nn.Linear(prev_dim, output_dim))
        
        self.network = nn.Sequential(*layers)
        self.output_dim = output_dim
    
    def forward(self, x):
        return self.network(x)


class GPRegressionModel(gpytorch.models.ExactGP):
    """é«˜æ–¯éç¨‹å›æ­¸æ¨¡å‹"""
    
    def __init__(self, train_x, train_y, likelihood, feature_extractor):
        super().__init__(train_x, train_y, likelihood)
        
        self.feature_extractor = feature_extractor
        self.mean_module = gpytorch.means.ConstantMean()
        
        self.covar_module = gpytorch.kernels.ScaleKernel(
            gpytorch.kernels.RBFKernel(ard_num_dims=feature_extractor.output_dim)
        )
        '''
        self.covar_module = gpytorch.kernels.ScaleKernel(
            gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=feature_extractor.output_dim)
        )
        '''        
    def forward(self, x):
        projected_x = self.feature_extractor(x)
        mean_x = self.mean_module(projected_x)
        covar_x = self.covar_module(projected_x)
        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)


# ==========================================
# æå¤±å‡½æ•¸èˆ‡æ¬Šé‡
# ==========================================

def weighted_mape_loss(y_pred, y_true, weights, epsilon=1e-8):
    """åŠ æ¬ŠMAPEæå¤±å‡½æ•¸"""
    mape_per_sample = torch.abs((y_true - y_pred) / 
                                (torch.abs(y_true) + epsilon)) * 100
    weighted_mape = torch.sum(mape_per_sample * weights) / torch.sum(weights)
    return weighted_mape


def compute_sample_weights_type3(X, weight_factor=10.0):
    """
    Type 3 å°ˆç”¨æ¬Šé‡è¨ˆç®—
    é‡å°é«˜ Coverage (â‰¥0.8) å’Œå¤§ Thickness (â‰¥220) çµ¦äºˆæ›´é«˜æ¬Šé‡
    """
    weights = np.ones(len(X))
    
    # Type 3 çš„å›°é›£æ¨£æœ¬: Coverage >= 0.8 AND Thickness >= 220
    difficult_mask = (
        (X[:, 1] >= 0.8) &    # Coverage >= 0.8 (æ³¨æ„é€™è£¡ X æ˜¯ [THICKNESS, COVERAGE])
        (X[:, 0] >= 220)      # THICKNESS >= 220
    )
    
    weights[difficult_mask] *= weight_factor
    
    return weights


# ==========================================
# è¨“ç·´èˆ‡è©•ä¼°
# ==========================================

def train_model(X_train, y_train, config, model_name="Model", verbose=True):
    """
    è¨“ç·´ DKL æ¨¡å‹
    
    Args:
        X_train: è¨“ç·´ç‰¹å¾µ
        y_train: è¨“ç·´æ¨™ç±¤
        config: è¨“ç·´é…ç½®
        model_name: æ¨¡å‹åç¨± (ç”¨æ–¼é¡¯ç¤º)
        verbose: æ˜¯å¦é¡¯ç¤ºè¨“ç·´éç¨‹
        
    Returns:
        model, likelihood, scaler_x, scaler_y
    """
    
    if verbose:
        print(f"\n{'='*60}")
        print(f"è¨“ç·´ {model_name}")
        print(f"{'='*60}")
        print(f"è¨“ç·´æ¨£æœ¬æ•¸: {len(X_train)}")
    
    # è¨ˆç®—æ¨£æœ¬æ¬Šé‡ (åªæœ‰ Type 3 æ¨¡å‹ç”¨ç‰¹æ®Šæ¬Šé‡)
    if 'Type 3' in model_name:
        sample_weights_np = compute_sample_weights_type3(X_train, config['type3_weight_factor'])
        if verbose:
            difficult_count = np.sum(sample_weights_np > 1.0)
            print(f"å›°é›£æ¨£æœ¬ (Coverageâ‰¥0.8, Thicknessâ‰¥220): {difficult_count} ({difficult_count/len(X_train)*100:.2f}%)")
            print(f"æ¬Šé‡å€æ•¸: {config['type3_weight_factor']}x")
    else:
        sample_weights_np = np.ones(len(X_train))
        if verbose:
            print(f"ä½¿ç”¨å‡å‹»æ¬Šé‡")
    
    # æ¨™æº–åŒ–
    scaler_x = StandardScaler()
    scaler_y = StandardScaler()
    
    X_train_scaled = scaler_x.fit_transform(X_train)
    y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()
    
    train_x = torch.from_numpy(X_train_scaled).to(device)
    train_y = torch.from_numpy(y_train_scaled).to(device)
    sample_weights = torch.from_numpy(sample_weights_np).to(device)
    
    # å»ºç«‹æ¨¡å‹
    feature_extractor = DnnFeatureExtractor(
        input_dim=train_x.shape[1],
        hidden_dims=config['hidden_dims'],
        output_dim=config['feature_dim'],
        dropout=config['dropout']
    ).to(device)
    
    likelihood = gpytorch.likelihoods.GaussianLikelihood().to(device)
    model = GPRegressionModel(train_x, train_y, likelihood, feature_extractor).to(device)
    
    # å„ªåŒ–å™¨
    optimizer = optim.Adam([
        {'params': model.feature_extractor.parameters(), 'lr': config['lr'], 'weight_decay': 1e-4},
        {'params': model.covar_module.parameters()},
        {'params': model.mean_module.parameters()},
        {'params': model.likelihood.parameters()},
    ], lr=config['lr'])
    
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=50, T_mult=2)
    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)
    
    # è¨“ç·´
    if verbose:
        print(f"\né–‹å§‹è¨“ç·´...")
    
    model.train()
    likelihood.train()
    
    best_loss = float('inf')
    patience_counter = 0
    start_time = time.time()
    
    for epoch in range(config['epochs']):
        epoch_start = time.time()
        
        optimizer.zero_grad()
        
        output = model(train_x)
        gp_loss = -mll(output, train_y)
        mape = weighted_mape_loss(output.mean, train_y, sample_weights)
        total_loss = gp_loss + config['mape_weight'] * mape
        
        total_loss.backward()
        optimizer.step()
        scheduler.step()
        
        current_loss = total_loss.item()
        epoch_time = time.time() - epoch_start
        
        # é¡¯ç¤ºè¨“ç·´é€²åº¦
        if verbose and (epoch + 1) % 50 == 0:
            elapsed = time.time() - start_time
            avg_epoch_time = elapsed / (epoch + 1)
            eta = avg_epoch_time * (config['epochs'] - epoch - 1)
            
            print(f"Epoch {epoch+1:3d}/{config['epochs']}: "
                  f"GP Loss={gp_loss.item():7.4f}, "
                  f"MAPE={mape.item():6.2f}%, "
                  f"Total={total_loss.item():7.4f} | "
                  f"Time: {epoch_time:.2f}s | "
                  f"ETA: {eta/60:.1f}min")
        
        # Early stopping
        if current_loss < best_loss:
            best_loss = current_loss
            patience_counter = 0
            best_state = {
                'model': model.state_dict(),
                'likelihood': likelihood.state_dict(),
            }
        else:
            patience_counter += 1
        
        if patience_counter >= config['patience']:
            if verbose:
                print(f"\næ—©åœ at Epoch {epoch+1}")
            break
    
    # è¼‰å…¥æœ€ä½³æ¨¡å‹
    model.load_state_dict(best_state['model'])
    likelihood.load_state_dict(best_state['likelihood'])
    
    if verbose:
        total_time = time.time() - start_time
        print(f"è¨“ç·´å®Œæˆ (Final Loss: {best_loss:.4f}, æ™‚é–“: {total_time/60:.2f} åˆ†é˜)")
    
    return model, likelihood, scaler_x, scaler_y


def evaluate_model(model, likelihood, X_test, y_test, scaler_x, scaler_y, 
                   model_name="Model", verbose=True):
    """
    è©•ä¼°æ¨¡å‹
    
    Returns:
        results: åŒ…å« MAPE, outliers ç­‰æŒ‡æ¨™çš„å­—å…¸
    """
    model.eval()
    likelihood.eval()
    
    X_test_scaled = scaler_x.transform(X_test)
    test_x = torch.from_numpy(X_test_scaled).to(device)
    
    with torch.no_grad(), gpytorch.settings.fast_pred_var():
        pred_dist = likelihood(model(test_x))
        y_pred_scaled = pred_dist.mean.cpu().numpy()
        y_std_scaled = pred_dist.stddev.cpu().numpy()
    
    # åæ¨™æº–åŒ–
    y_pred = scaler_y.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()
    y_std = y_std_scaled * scaler_y.scale_[0]
    
    # è¨ˆç®—æŒ‡æ¨™
    relative_errors = np.abs((y_test - y_pred) / y_test) * 100
    
    mape = np.mean(relative_errors)
    mae = np.mean(np.abs(y_test - y_pred))
    max_error = np.max(relative_errors)
    
    outliers_20 = np.sum(relative_errors > 20)
    outliers_15 = np.sum(relative_errors > 15)
    outliers_10 = np.sum(relative_errors > 10)
    
    if verbose:
        print(f"\n{'='*60}")
        print(f"{model_name} è©•ä¼°çµæœ")
        print(f"{'='*60}")
        print(f"æ¨£æœ¬æ•¸: {len(y_test)}")
        print(f"\næº–ç¢ºåº¦:")
        print(f"  MAPE:      {mape:.2f}%")
        print(f"  MAE:       {mae:.4f}")
        print(f"  Max Error: {max_error:.2f}%")
        print(f"\nç•°å¸¸é»:")
        print(f"  >20%: {outliers_20}/{len(y_test)} ({outliers_20/len(y_test)*100:.2f}%)")
        print(f"  >15%: {outliers_15}/{len(y_test)} ({outliers_15/len(y_test)*100:.2f}%)")
        print(f"  >10%: {outliers_10}/{len(y_test)} ({outliers_10/len(y_test)*100:.2f}%)")
        print(f"{'='*60}\n")
    
    results = {
        'mape': mape,
        'mae': mae,
        'max_error': max_error,
        'outliers_20': outliers_20,
        'outliers_15': outliers_15,
        'outliers_10': outliers_10,
        'predictions': y_pred,
        'std': y_std,
        'errors': relative_errors
    }
    
    return results


def save_predictions(X_test, y_test, results, filename, include_type=False):
    """ä¿å­˜é æ¸¬çµæœåˆ°CSV"""
    if include_type:
        df = pd.DataFrame({
            'TIM_TYPE': X_test[:, 0],
            'TIM_THICKNESS': X_test[:, 1],
            'TIM_COVERAGE': X_test[:, 2],
            'True': y_test,
            'Predicted': results['predictions'],
            'Error%': results['errors'],
            'Std': results['std']
        })
    else:
        df = pd.DataFrame({
            'TIM_THICKNESS': X_test[:, 0],
            'TIM_COVERAGE': X_test[:, 1],
            'True': y_test,
            'Predicted': results['predictions'],
            'Error%': results['errors'],
            'Std': results['std']
        })
    
    df.to_csv(filename, index=False)
    print(f"âœ“ é æ¸¬çµæœå·²ä¿å­˜åˆ°: {filename}")


# ==========================================
# ä¸»å‡½æ•¸
# ==========================================

def main(seed=2024, verbose=True):
    """
    ä¸»è¨“ç·´æµç¨‹ï¼šåˆ†æ²»è¨“ç·´ Type 1&2 å’Œ Type 3
    """
    clear_gpu_cache()
    set_seed(seed)
    
    print(f"\nä½¿ç”¨è£ç½®: {device}\n")
    
    print("="*60)
    print("Phase 4B: åˆ†æ²»è¨“ç·´ç­–ç•¥ (Divide and Conquer)")
    print("="*60)
    print("ç­–ç•¥: åˆ†åˆ¥è¨“ç·´ Type 1&2 å’Œ Type 3 æ¨¡å‹")
    print("="*60)
    
    # ç‰¹å¾µå’Œç›®æ¨™
    feature_cols = ['TIM_TYPE', 'TIM_THICKNESS', 'TIM_COVERAGE']
    target_col = 'Theta.JC'
    
    # é…ç½®
    config = {
        'hidden_dims': [64, 32, 16],
        'feature_dim': 8,
        'dropout': 0.1,
        'lr': 0.01,
        'epochs': 500,
        'patience': 50,
        'mape_weight': 0.1,
        'type3_weight_factor': 3.0,  # Type 3 å›°é›£æ¨£æœ¬æ¬Šé‡
    }
    
    if verbose:
        print(f"\né…ç½®:")
        for key, value in config.items():
            print(f"  {key}: {value}")
    
    # ==========================================
    # Above Dataset
    # ==========================================
    
    print(f"\n\n{'='*80}")
    print(f"ğŸ”µ Above 50% Coverage")
    print(f"{'='*80}\n")
    
    # è¼‰å…¥è³‡æ–™
    train_above = pd.read_excel('data/train/Above.xlsx')
    test_above = pd.read_excel('data/test/Above.xlsx')
    
    # è¨“ç·´é›†æ¸…ç†
    train_above_clean = train_above.groupby(feature_cols, as_index=False).agg({
        target_col: 'mean'
    })
    
    print(f"ç¸½è¨“ç·´é›†: {len(train_above)} ç­†")
    print(f"ç¸½æ¸¬è©¦é›†: {len(test_above)} ç­†")
    
    # ========== åˆ†å‰²è¨“ç·´é›† ==========
    
    # Type 1 & 2
    train_type12 = train_above[train_above['TIM_TYPE'].isin([1, 2])]
    # ç§»é™¤ TIM_TYPE æ¬„ä½ (ä¸éœ€è¦ä½œç‚ºç‰¹å¾µ)
    X_train_type12 = train_type12[['TIM_THICKNESS', 'TIM_COVERAGE']].values
    y_train_type12 = train_type12[target_col].values
    
    # Type 3
    train_type3 = train_above[train_above['TIM_TYPE'] == 3]
    X_train_type3 = train_type3[['TIM_THICKNESS', 'TIM_COVERAGE']].values
    y_train_type3 = train_type3[target_col].values
    
    print(f"\nè¨“ç·´é›†åˆ†å‰²:")
    print(f"  Type 1 & 2: {len(train_type12)} ç­†")
    print(f"  Type 3:     {len(train_type3)} ç­†")
    
    # ========== åˆ†å‰²æ¸¬è©¦é›† ==========
    
    test_type12 = test_above[test_above['TIM_TYPE'].isin([1, 2])]
    X_test_type12 = test_type12[['TIM_THICKNESS', 'TIM_COVERAGE']].values
    y_test_type12 = test_type12[target_col].values
    
    test_type3 = test_above[test_above['TIM_TYPE'] == 3]
    X_test_type3 = test_type3[['TIM_THICKNESS', 'TIM_COVERAGE']].values
    y_test_type3 = test_type3[target_col].values
    
    print(f"\næ¸¬è©¦é›†åˆ†å‰²:")
    print(f"  Type 1 & 2: {len(test_type12)} ç­†")
    print(f"  Type 3:     {len(test_type3)} ç­†")
    
    # ========== è¨“ç·´æ¨¡å‹ A: Type 1 & 2 ==========
    
    model_type12, likelihood_type12, scaler_x_type12, scaler_y_type12 = train_model(
        X_train_type12, y_train_type12, config, 
        model_name="æ¨¡å‹ A (Type 1 & 2)", 
        verbose=verbose
    )
    
    # è©•ä¼°æ¨¡å‹ A
    results_type12 = evaluate_model(
        model_type12, likelihood_type12,
        X_test_type12, y_test_type12,
        scaler_x_type12, scaler_y_type12,
        model_name="æ¨¡å‹ A (Type 1 & 2)",
        verbose=verbose
    )
    
    save_predictions(X_test_type12, y_test_type12, results_type12,
                     f'phase4b_type12_above_seed{seed}_predictions.csv')
    
    clear_gpu_cache()
    
    # ========== è¨“ç·´æ¨¡å‹ B: Type 3 ==========
    
    model_type3, likelihood_type3, scaler_x_type3, scaler_y_type3 = train_model(
        X_train_type3, y_train_type3, config,
        model_name="æ¨¡å‹ B (Type 3)",
        verbose=verbose
    )
    
    # è©•ä¼°æ¨¡å‹ B
    results_type3 = evaluate_model(
        model_type3, likelihood_type3,
        X_test_type3, y_test_type3,
        scaler_x_type3, scaler_y_type3,
        model_name="æ¨¡å‹ B (Type 3)",
        verbose=verbose
    )
    
    save_predictions(X_test_type3, y_test_type3, results_type3,
                     f'phase4b_type3_above_seed{seed}_predictions.csv')
    
    # ========== åˆä½µçµæœ ==========
    
    # åˆä½µé æ¸¬
    all_predictions = np.zeros(len(test_above))
    all_errors = np.zeros(len(test_above))
    
    type12_indices = test_above[test_above['TIM_TYPE'].isin([1, 2])].index
    type3_indices = test_above[test_above['TIM_TYPE'] == 3].index
    
    all_predictions[type12_indices] = results_type12['predictions']
    all_predictions[type3_indices] = results_type3['predictions']
    
    all_errors[type12_indices] = results_type12['errors']
    all_errors[type3_indices] = results_type3['errors']
    
    # è¨ˆç®—æ•´é«”æŒ‡æ¨™
    overall_mape = np.mean(all_errors)
    overall_outliers_20 = np.sum(all_errors > 20)
    overall_outliers_15 = np.sum(all_errors > 15)
    overall_outliers_10 = np.sum(all_errors > 10)
    
    # Type 3 çš„ç•°å¸¸é»
    type3_outliers = np.sum(all_errors[type3_indices] > 20)
    
    # ä¿å­˜åˆä½µçµæœ
    combined_results = {
        'predictions': all_predictions,
        'errors': all_errors,
        'std': np.concatenate([results_type12['std'], results_type3['std']])
    }
    save_predictions(
        test_above[feature_cols].values,
        test_above[target_col].values,
        combined_results,
        f'phase4b_combined_above_seed{seed}_predictions.csv',
        include_type=True
    )
    
    # ==========================================
    # Below Dataset (ä¿æŒåŸæ¨£ï¼Œä¸åˆ†æ²»)
    # ==========================================
    
    print(f"\n\n{'='*80}")
    print(f"ğŸ”µ Below 50% Coverage (çµ±ä¸€æ¨¡å‹)")
    print(f"{'='*80}\n")
    
    train_below = pd.read_excel('data/train/Below.xlsx')
    test_below = pd.read_excel('data/test/Below.xlsx')
    
    train_below_clean = train_below.groupby(feature_cols, as_index=False).agg({
        target_col: 'mean'
    })
    
    print(f"è¨“ç·´é›†: {len(train_below_clean)} ç­†")
    print(f"æ¸¬è©¦é›†: {len(test_below)} ç­†")
    
    X_train_below = train_below_clean[feature_cols].values
    y_train_below = train_below_clean[target_col].values
    
    X_test_below = test_below[feature_cols].values
    y_test_below = test_below[target_col].values
    
    # è¨“ç·´ (ä½¿ç”¨åŸå§‹æ–¹æ³•ï¼Œå› ç‚º Below å•é¡Œä¸å¤§)
    # æš«æ™‚ç”¨ Type 1&2 çš„æ¬Šé‡ç­–ç•¥ (å‡å‹»æ¬Šé‡)
    config_below = config.copy()
    config_below['type3_weight_factor'] = 3.0  # é™ä½æ¬Šé‡
    
    model_below, likelihood_below, scaler_x_below, scaler_y_below = train_model(
        X_train_below, y_train_below, config_below,
        model_name="æ¨¡å‹ (Below çµ±ä¸€)",
        verbose=verbose
    )
    
    # è©•ä¼°
    results_below = evaluate_model(
        model_below, likelihood_below,
        X_test_below, y_test_below,
        scaler_x_below, scaler_y_below,
        model_name="æ¨¡å‹ (Below çµ±ä¸€)",
        verbose=verbose
    )
    
    save_predictions(X_test_below, y_test_below, results_below,
                     f'phase4b_below_seed{seed}_predictions.csv',
                     include_type=True)
    
    # ==========================================
    # ç¸½çµ
    # ==========================================
    
    print("\n" + "="*80)
    print("Phase 4B æœ€çµ‚çµæœç¸½çµ")
    print("="*80)
    print(f"éš¨æ©Ÿç¨®å­: {seed}")
    
    print(f"\nAbove è³‡æ–™é›† (åˆ†æ²»ç­–ç•¥):")
    print(f"  æ¨¡å‹ A (Type 1 & 2):")
    print(f"    æ¨£æœ¬æ•¸: {len(test_type12)}")
    print(f"    MAPE: {results_type12['mape']:.2f}%")
    print(f"    ç•°å¸¸é» >20%: {results_type12['outliers_20']}/{len(test_type12)} ({results_type12['outliers_20']/len(test_type12)*100:.2f}%)")
    
    print(f"\n  æ¨¡å‹ B (Type 3):")
    print(f"    æ¨£æœ¬æ•¸: {len(test_type3)}")
    print(f"    MAPE: {results_type3['mape']:.2f}%")
    print(f"    ç•°å¸¸é» >20%: {results_type3['outliers_20']}/{len(test_type3)} ({results_type3['outliers_20']/len(test_type3)*100:.2f}%)")
    
    print(f"\n  æ•´é«” (åˆä½µ):")
    print(f"    ç¸½æ¨£æœ¬æ•¸: {len(test_above)}")
    print(f"    æ•´é«” MAPE: {overall_mape:.2f}%")
    print(f"    ç•°å¸¸é» >20%: {overall_outliers_20}/{len(test_above)} ({overall_outliers_20/len(test_above)*100:.2f}%)")
    print(f"    ç•°å¸¸é» >15%: {overall_outliers_15}/{len(test_above)} ({overall_outliers_15/len(test_above)*100:.2f}%)")
    print(f"    ç•°å¸¸é» >10%: {overall_outliers_10}/{len(test_above)} ({overall_outliers_10/len(test_above)*100:.2f}%)")
    print(f"    Type 3 ç•°å¸¸é»: {type3_outliers}/{len(test_type3)}")
    
    print(f"\nBelow è³‡æ–™é›† (çµ±ä¸€æ¨¡å‹):")
    print(f"  ç•°å¸¸é» (>20%): {results_below['outliers_20']}/{len(y_test_below)} ({results_below['outliers_20']/len(y_test_below)*100:.2f}%)")
    print(f"  MAPE: {results_below['mape']:.2f}%")
    
    print("\n" + "="*80)
    print("âœ“ åˆ†æ²»è¨“ç·´å®Œæˆï¼")
    print("="*80)
    
    # èˆ‡ Phase 2B æ¯”è¼ƒ
    print(f"\nğŸ’¡ èˆ‡ Phase 2B æ¯”è¼ƒ:")
    print(f"   Phase 2B (çµ±ä¸€æ¨¡å‹): MAPE ~48%, Type 3 ç•°å¸¸é»è¼ƒå¤š")
    print(f"   Phase 4B (åˆ†æ²»ç­–ç•¥): MAPE {overall_mape:.2f}%, Type 3 ç•°å¸¸é» {type3_outliers}/{len(test_type3)}")
    print(f"   æ”¹å–„: {'âœ“ æœ‰æ”¹å–„' if overall_mape < 48 else 'âš ï¸ éœ€è¦èª¿æ•´'}")
    
    print("\n" + "="*80 + "\n")
    
    return {
        'above_type12': results_type12,
        'above_type3': results_type3,
        'above_overall_mape': overall_mape,
        'above_overall_outliers': overall_outliers_20,
        'below': results_below,
        'seed': seed
    }


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='Phase 4B - åˆ†æ²»è¨“ç·´ç­–ç•¥')
    parser.add_argument('--seed', type=int, default=2024, help='éš¨æ©Ÿç¨®å­ (é è¨­: 2024)')
    parser.add_argument('-v', '--verbose', action='store_true', help='é¡¯ç¤ºè©³ç´°è¨“ç·´éç¨‹')
    
    args = parser.parse_args()
    
    results = main(seed=args.seed, verbose=args.verbose)
    
    print("\nğŸ’¡ ä½¿ç”¨ç¯„ä¾‹:")
    print("  python phase4b_divide_conquer.py --seed 2024 -v")
    print("  python phase4b_divide_conquer.py --seed 42")
    print("\nğŸ¯ å„ªå‹¢:")
    print("  1. Type 3 ç¨ç«‹å»ºæ¨¡ï¼Œä¸å— Type 1&2 å¹²æ“¾")
    print("  2. Type 3 å¯ç”¨ 10x æ¬Šé‡å¼·åŒ–å›°é›£æ¨£æœ¬")
    print("  3. æ¨¡å‹æ›´å°ˆæ³¨ï¼Œå­¸ç¿’æ›´ç²¾æº–")
    print("  4. é æ¸¬æ™‚æ ¹æ“š Type è‡ªå‹•é¸æ“‡æ¨¡å‹\n")